{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23cd435c-ef3e-4b43-b8e2-fcb787783631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a658aa-85b8-4505-a357-a122062be893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.cifar.CIFAR100(\"./data\", train=True, download=False)\n",
    "test = datasets.cifar.CIFAR100(\"./data\", train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7162832-551b-45d8-bf15-9c4aadca6849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d03ef74-e1e4-45dc-8da6-e9668bc4eeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a27c1a-a7a2-4124-8044-e9b2d18126b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171997df-2440-4083-995a-8d398b5abee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.adam_adapter import Learner\n",
    "json = \"./exps/adam_adapter.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97ba423-5f48-412d-90b1-5102ee886499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simplecil import Learner\n",
    "json = \"./exps/simplecil.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9342b9a0-a398-4a43-9120-868427617410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_random(seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ef6495-4280-4943-8069-3fe0bf71308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_device(args):\n",
    "    device_type = args[\"device\"]\n",
    "    gpus = []\n",
    "\n",
    "    for device in device_type:\n",
    "        if device == -1:\n",
    "            device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(device))\n",
    "\n",
    "        gpus.append(device)\n",
    "\n",
    "    args[\"device\"] = gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca34349e-ab4d-46d0-a1bd-3c39a1ec4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.data_manager import DataManager\n",
    "from utils.toolkit import tensor2numpy, accuracy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126ae811-133e-4602-8a77-c3eb2188c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(setting_path):\n",
    "    import json\n",
    "    with open(setting_path) as data_file:\n",
    "        param = json.load(data_file)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827a1705-525d-480f-8987-d7acad448388",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_json(json)\n",
    "args[\"seed\"] = args[\"seed\"][0]\n",
    "#args[\"device\"] = args[\"device\"][0]\n",
    "_set_random(args[\"seed\"])\n",
    "#_set_device(args)\n",
    "args[\"device\"][0] = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57166167-c42c-4d68-9e5b-8ee9c605edf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prefix': ' ',\n",
       " 'dataset': 'cifar224',\n",
       " 'memory_size': 0,\n",
       " 'memory_per_class': 0,\n",
       " 'fixed_memory': False,\n",
       " 'shuffle': True,\n",
       " 'init_cls': 10,\n",
       " 'increment': 10,\n",
       " 'model_name': 'simplecil',\n",
       " 'backbone_type': 'pretrained_vit_b16_224',\n",
       " 'device': ['cuda:0'],\n",
       " 'seed': 1993,\n",
       " 'tuned_epoch': 0,\n",
       " 'init_lr': 0.01,\n",
       " 'batch_size': 256,\n",
       " 'weight_decay': 0.05,\n",
       " 'min_lr': 1e-08,\n",
       " 'optimizer': 'sgd',\n",
       " 'vpt_type': 'shallow',\n",
       " 'prompt_token_num': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb05f3c-6440-44d7-bb78-4c449e82be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./checkpoints/adam_adapter/task_1.pkl\"\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "\n",
    "task = checkpoint['tasks']\n",
    "args.update(checkpoint['model_state_dict'])\n",
    "\n",
    "args[\"device\"][0] = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c9c420-3d5f-4dd3-b4e6-9c06d6eb4e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0312, -0.0190,  0.0140,  ...,  0.0216,  0.0246,  0.0246],\n",
       "        [ 0.0344, -0.0331,  0.0220,  ...,  0.0070, -0.0033, -0.0178],\n",
       "        [-0.0307,  0.0150, -0.0314,  ...,  0.0098,  0.0316,  0.0103],\n",
       "        ...,\n",
       "        [ 0.0143, -0.0060, -0.0096,  ...,  0.0142, -0.0134,  0.0114],\n",
       "        [-0.0041, -0.0273,  0.0061,  ...,  0.0227,  0.0149,  0.0047],\n",
       "        [ 0.0328,  0.0253,  0.0149,  ..., -0.0328,  0.0213, -0.0010]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"blocks.0.adaptmlp.down_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a84ce7f4-dfd7-4116-9fdb-8ba18b382b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.update(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96c0dc42-393e-472e-a075-b95e15f9b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "After BaseNet initialization.\n"
     ]
    }
   ],
   "source": [
    "model = Learner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862a6d66-275e-464b-ae10-34d57c0987b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404506c6-2de6-48d7-89fc-42f1520262cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3afa1ed2-f4ed-4f0c-9392-35c4794e9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n",
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "pretrained_vit_b16_224\n"
     ]
    }
   ],
   "source": [
    "model.construct_dual_branch_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5805b242-56a0-4a6e-a7c4-34ac68dbda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict= checkpoint['model_state_dict']\n",
    "keys = state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8423d36-ac30-4798-84d0-5327ded29918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'backbones.1.blocks.0.adaptmlp.down_proj.weight' -> 'blocks.0.adaptmlp.down_proj.weight'\n",
    "for key in list(keys):\n",
    "    if 'backbones.1.blocks' in key:\n",
    "        entry = state_dict.pop(key)\n",
    "        state_dict[key.replace('backbones.1.blocks', 'blocks')] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c42132-43d4-4d9a-a26d-eccc7acb2d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbones.0.cls_token', 'backbones.0.pos_embed', 'backbones.0.patch_embed.proj.weight', 'backbones.0.patch_embed.proj.bias', 'backbones.0.blocks.0.norm1.weight', 'backbones.0.blocks.0.norm1.bias', 'backbones.0.blocks.0.attn.qkv.weight', 'backbones.0.blocks.0.attn.qkv.bias', 'backbones.0.blocks.0.attn.proj.weight', 'backbones.0.blocks.0.attn.proj.bias', 'backbones.0.blocks.0.norm2.weight', 'backbones.0.blocks.0.norm2.bias', 'backbones.0.blocks.0.mlp.fc1.weight', 'backbones.0.blocks.0.mlp.fc1.bias', 'backbones.0.blocks.0.mlp.fc2.weight', 'backbones.0.blocks.0.mlp.fc2.bias', 'backbones.0.blocks.1.norm1.weight', 'backbones.0.blocks.1.norm1.bias', 'backbones.0.blocks.1.attn.qkv.weight', 'backbones.0.blocks.1.attn.qkv.bias', 'backbones.0.blocks.1.attn.proj.weight', 'backbones.0.blocks.1.attn.proj.bias', 'backbones.0.blocks.1.norm2.weight', 'backbones.0.blocks.1.norm2.bias', 'backbones.0.blocks.1.mlp.fc1.weight', 'backbones.0.blocks.1.mlp.fc1.bias', 'backbones.0.blocks.1.mlp.fc2.weight', 'backbones.0.blocks.1.mlp.fc2.bias', 'backbones.0.blocks.2.norm1.weight', 'backbones.0.blocks.2.norm1.bias', 'backbones.0.blocks.2.attn.qkv.weight', 'backbones.0.blocks.2.attn.qkv.bias', 'backbones.0.blocks.2.attn.proj.weight', 'backbones.0.blocks.2.attn.proj.bias', 'backbones.0.blocks.2.norm2.weight', 'backbones.0.blocks.2.norm2.bias', 'backbones.0.blocks.2.mlp.fc1.weight', 'backbones.0.blocks.2.mlp.fc1.bias', 'backbones.0.blocks.2.mlp.fc2.weight', 'backbones.0.blocks.2.mlp.fc2.bias', 'backbones.0.blocks.3.norm1.weight', 'backbones.0.blocks.3.norm1.bias', 'backbones.0.blocks.3.attn.qkv.weight', 'backbones.0.blocks.3.attn.qkv.bias', 'backbones.0.blocks.3.attn.proj.weight', 'backbones.0.blocks.3.attn.proj.bias', 'backbones.0.blocks.3.norm2.weight', 'backbones.0.blocks.3.norm2.bias', 'backbones.0.blocks.3.mlp.fc1.weight', 'backbones.0.blocks.3.mlp.fc1.bias', 'backbones.0.blocks.3.mlp.fc2.weight', 'backbones.0.blocks.3.mlp.fc2.bias', 'backbones.0.blocks.4.norm1.weight', 'backbones.0.blocks.4.norm1.bias', 'backbones.0.blocks.4.attn.qkv.weight', 'backbones.0.blocks.4.attn.qkv.bias', 'backbones.0.blocks.4.attn.proj.weight', 'backbones.0.blocks.4.attn.proj.bias', 'backbones.0.blocks.4.norm2.weight', 'backbones.0.blocks.4.norm2.bias', 'backbones.0.blocks.4.mlp.fc1.weight', 'backbones.0.blocks.4.mlp.fc1.bias', 'backbones.0.blocks.4.mlp.fc2.weight', 'backbones.0.blocks.4.mlp.fc2.bias', 'backbones.0.blocks.5.norm1.weight', 'backbones.0.blocks.5.norm1.bias', 'backbones.0.blocks.5.attn.qkv.weight', 'backbones.0.blocks.5.attn.qkv.bias', 'backbones.0.blocks.5.attn.proj.weight', 'backbones.0.blocks.5.attn.proj.bias', 'backbones.0.blocks.5.norm2.weight', 'backbones.0.blocks.5.norm2.bias', 'backbones.0.blocks.5.mlp.fc1.weight', 'backbones.0.blocks.5.mlp.fc1.bias', 'backbones.0.blocks.5.mlp.fc2.weight', 'backbones.0.blocks.5.mlp.fc2.bias', 'backbones.0.blocks.6.norm1.weight', 'backbones.0.blocks.6.norm1.bias', 'backbones.0.blocks.6.attn.qkv.weight', 'backbones.0.blocks.6.attn.qkv.bias', 'backbones.0.blocks.6.attn.proj.weight', 'backbones.0.blocks.6.attn.proj.bias', 'backbones.0.blocks.6.norm2.weight', 'backbones.0.blocks.6.norm2.bias', 'backbones.0.blocks.6.mlp.fc1.weight', 'backbones.0.blocks.6.mlp.fc1.bias', 'backbones.0.blocks.6.mlp.fc2.weight', 'backbones.0.blocks.6.mlp.fc2.bias', 'backbones.0.blocks.7.norm1.weight', 'backbones.0.blocks.7.norm1.bias', 'backbones.0.blocks.7.attn.qkv.weight', 'backbones.0.blocks.7.attn.qkv.bias', 'backbones.0.blocks.7.attn.proj.weight', 'backbones.0.blocks.7.attn.proj.bias', 'backbones.0.blocks.7.norm2.weight', 'backbones.0.blocks.7.norm2.bias', 'backbones.0.blocks.7.mlp.fc1.weight', 'backbones.0.blocks.7.mlp.fc1.bias', 'backbones.0.blocks.7.mlp.fc2.weight', 'backbones.0.blocks.7.mlp.fc2.bias', 'backbones.0.blocks.8.norm1.weight', 'backbones.0.blocks.8.norm1.bias', 'backbones.0.blocks.8.attn.qkv.weight', 'backbones.0.blocks.8.attn.qkv.bias', 'backbones.0.blocks.8.attn.proj.weight', 'backbones.0.blocks.8.attn.proj.bias', 'backbones.0.blocks.8.norm2.weight', 'backbones.0.blocks.8.norm2.bias', 'backbones.0.blocks.8.mlp.fc1.weight', 'backbones.0.blocks.8.mlp.fc1.bias', 'backbones.0.blocks.8.mlp.fc2.weight', 'backbones.0.blocks.8.mlp.fc2.bias', 'backbones.0.blocks.9.norm1.weight', 'backbones.0.blocks.9.norm1.bias', 'backbones.0.blocks.9.attn.qkv.weight', 'backbones.0.blocks.9.attn.qkv.bias', 'backbones.0.blocks.9.attn.proj.weight', 'backbones.0.blocks.9.attn.proj.bias', 'backbones.0.blocks.9.norm2.weight', 'backbones.0.blocks.9.norm2.bias', 'backbones.0.blocks.9.mlp.fc1.weight', 'backbones.0.blocks.9.mlp.fc1.bias', 'backbones.0.blocks.9.mlp.fc2.weight', 'backbones.0.blocks.9.mlp.fc2.bias', 'backbones.0.blocks.10.norm1.weight', 'backbones.0.blocks.10.norm1.bias', 'backbones.0.blocks.10.attn.qkv.weight', 'backbones.0.blocks.10.attn.qkv.bias', 'backbones.0.blocks.10.attn.proj.weight', 'backbones.0.blocks.10.attn.proj.bias', 'backbones.0.blocks.10.norm2.weight', 'backbones.0.blocks.10.norm2.bias', 'backbones.0.blocks.10.mlp.fc1.weight', 'backbones.0.blocks.10.mlp.fc1.bias', 'backbones.0.blocks.10.mlp.fc2.weight', 'backbones.0.blocks.10.mlp.fc2.bias', 'backbones.0.blocks.11.norm1.weight', 'backbones.0.blocks.11.norm1.bias', 'backbones.0.blocks.11.attn.qkv.weight', 'backbones.0.blocks.11.attn.qkv.bias', 'backbones.0.blocks.11.attn.proj.weight', 'backbones.0.blocks.11.attn.proj.bias', 'backbones.0.blocks.11.norm2.weight', 'backbones.0.blocks.11.norm2.bias', 'backbones.0.blocks.11.mlp.fc1.weight', 'backbones.0.blocks.11.mlp.fc1.bias', 'backbones.0.blocks.11.mlp.fc2.weight', 'backbones.0.blocks.11.mlp.fc2.bias', 'backbones.0.norm.weight', 'backbones.0.norm.bias', 'backbones.1.cls_token', 'backbones.1.pos_embed', 'backbones.1.patch_embed.proj.weight', 'backbones.1.patch_embed.proj.bias', 'backbones.1.norm.weight', 'backbones.1.norm.bias', 'fc.weight', 'fc.sigma', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.q_proj.weight', 'blocks.0.attn.q_proj.bias', 'blocks.0.attn.v_proj.weight', 'blocks.0.attn.v_proj.bias', 'blocks.0.attn.k_proj.weight', 'blocks.0.attn.k_proj.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.fc1.weight', 'blocks.0.fc1.bias', 'blocks.0.fc2.weight', 'blocks.0.fc2.bias', 'blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.q_proj.weight', 'blocks.1.attn.q_proj.bias', 'blocks.1.attn.v_proj.weight', 'blocks.1.attn.v_proj.bias', 'blocks.1.attn.k_proj.weight', 'blocks.1.attn.k_proj.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.fc1.weight', 'blocks.1.fc1.bias', 'blocks.1.fc2.weight', 'blocks.1.fc2.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.q_proj.weight', 'blocks.2.attn.q_proj.bias', 'blocks.2.attn.v_proj.weight', 'blocks.2.attn.v_proj.bias', 'blocks.2.attn.k_proj.weight', 'blocks.2.attn.k_proj.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.fc1.weight', 'blocks.2.fc1.bias', 'blocks.2.fc2.weight', 'blocks.2.fc2.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.q_proj.weight', 'blocks.3.attn.q_proj.bias', 'blocks.3.attn.v_proj.weight', 'blocks.3.attn.v_proj.bias', 'blocks.3.attn.k_proj.weight', 'blocks.3.attn.k_proj.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.fc1.weight', 'blocks.3.fc1.bias', 'blocks.3.fc2.weight', 'blocks.3.fc2.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.q_proj.weight', 'blocks.4.attn.q_proj.bias', 'blocks.4.attn.v_proj.weight', 'blocks.4.attn.v_proj.bias', 'blocks.4.attn.k_proj.weight', 'blocks.4.attn.k_proj.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.fc1.weight', 'blocks.4.fc1.bias', 'blocks.4.fc2.weight', 'blocks.4.fc2.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.q_proj.weight', 'blocks.5.attn.q_proj.bias', 'blocks.5.attn.v_proj.weight', 'blocks.5.attn.v_proj.bias', 'blocks.5.attn.k_proj.weight', 'blocks.5.attn.k_proj.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.fc1.weight', 'blocks.5.fc1.bias', 'blocks.5.fc2.weight', 'blocks.5.fc2.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.q_proj.weight', 'blocks.6.attn.q_proj.bias', 'blocks.6.attn.v_proj.weight', 'blocks.6.attn.v_proj.bias', 'blocks.6.attn.k_proj.weight', 'blocks.6.attn.k_proj.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.fc1.weight', 'blocks.6.fc1.bias', 'blocks.6.fc2.weight', 'blocks.6.fc2.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.q_proj.weight', 'blocks.7.attn.q_proj.bias', 'blocks.7.attn.v_proj.weight', 'blocks.7.attn.v_proj.bias', 'blocks.7.attn.k_proj.weight', 'blocks.7.attn.k_proj.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.fc1.weight', 'blocks.7.fc1.bias', 'blocks.7.fc2.weight', 'blocks.7.fc2.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.q_proj.weight', 'blocks.8.attn.q_proj.bias', 'blocks.8.attn.v_proj.weight', 'blocks.8.attn.v_proj.bias', 'blocks.8.attn.k_proj.weight', 'blocks.8.attn.k_proj.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.fc1.weight', 'blocks.8.fc1.bias', 'blocks.8.fc2.weight', 'blocks.8.fc2.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.q_proj.weight', 'blocks.9.attn.q_proj.bias', 'blocks.9.attn.v_proj.weight', 'blocks.9.attn.v_proj.bias', 'blocks.9.attn.k_proj.weight', 'blocks.9.attn.k_proj.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.fc1.weight', 'blocks.9.fc1.bias', 'blocks.9.fc2.weight', 'blocks.9.fc2.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.q_proj.weight', 'blocks.10.attn.q_proj.bias', 'blocks.10.attn.v_proj.weight', 'blocks.10.attn.v_proj.bias', 'blocks.10.attn.k_proj.weight', 'blocks.10.attn.k_proj.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.fc1.weight', 'blocks.10.fc1.bias', 'blocks.10.fc2.weight', 'blocks.10.fc2.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.q_proj.weight', 'blocks.11.attn.q_proj.bias', 'blocks.11.attn.v_proj.weight', 'blocks.11.attn.v_proj.bias', 'blocks.11.attn.k_proj.weight', 'blocks.11.attn.k_proj.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.fc1.weight', 'blocks.11.fc1.bias', 'blocks.11.fc2.weight', 'blocks.11.fc2.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21e370d-e89f-4040-8c6d-caaec7ba4f04",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536])."
     ]
    }
   ],
   "source": [
    "model._network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32050cb0-3763-467c-889e-cbbc26e3c56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbones.0.cls_token', 'backbones.0.pos_embed', 'backbones.0.patch_embed.proj.weight', 'backbones.0.patch_embed.proj.bias', 'backbones.0.blocks.0.norm1.weight', 'backbones.0.blocks.0.norm1.bias', 'backbones.0.blocks.0.attn.qkv.weight', 'backbones.0.blocks.0.attn.qkv.bias', 'backbones.0.blocks.0.attn.proj.weight', 'backbones.0.blocks.0.attn.proj.bias', 'backbones.0.blocks.0.norm2.weight', 'backbones.0.blocks.0.norm2.bias', 'backbones.0.blocks.0.mlp.fc1.weight', 'backbones.0.blocks.0.mlp.fc1.bias', 'backbones.0.blocks.0.mlp.fc2.weight', 'backbones.0.blocks.0.mlp.fc2.bias', 'backbones.0.blocks.1.norm1.weight', 'backbones.0.blocks.1.norm1.bias', 'backbones.0.blocks.1.attn.qkv.weight', 'backbones.0.blocks.1.attn.qkv.bias', 'backbones.0.blocks.1.attn.proj.weight', 'backbones.0.blocks.1.attn.proj.bias', 'backbones.0.blocks.1.norm2.weight', 'backbones.0.blocks.1.norm2.bias', 'backbones.0.blocks.1.mlp.fc1.weight', 'backbones.0.blocks.1.mlp.fc1.bias', 'backbones.0.blocks.1.mlp.fc2.weight', 'backbones.0.blocks.1.mlp.fc2.bias', 'backbones.0.blocks.2.norm1.weight', 'backbones.0.blocks.2.norm1.bias', 'backbones.0.blocks.2.attn.qkv.weight', 'backbones.0.blocks.2.attn.qkv.bias', 'backbones.0.blocks.2.attn.proj.weight', 'backbones.0.blocks.2.attn.proj.bias', 'backbones.0.blocks.2.norm2.weight', 'backbones.0.blocks.2.norm2.bias', 'backbones.0.blocks.2.mlp.fc1.weight', 'backbones.0.blocks.2.mlp.fc1.bias', 'backbones.0.blocks.2.mlp.fc2.weight', 'backbones.0.blocks.2.mlp.fc2.bias', 'backbones.0.blocks.3.norm1.weight', 'backbones.0.blocks.3.norm1.bias', 'backbones.0.blocks.3.attn.qkv.weight', 'backbones.0.blocks.3.attn.qkv.bias', 'backbones.0.blocks.3.attn.proj.weight', 'backbones.0.blocks.3.attn.proj.bias', 'backbones.0.blocks.3.norm2.weight', 'backbones.0.blocks.3.norm2.bias', 'backbones.0.blocks.3.mlp.fc1.weight', 'backbones.0.blocks.3.mlp.fc1.bias', 'backbones.0.blocks.3.mlp.fc2.weight', 'backbones.0.blocks.3.mlp.fc2.bias', 'backbones.0.blocks.4.norm1.weight', 'backbones.0.blocks.4.norm1.bias', 'backbones.0.blocks.4.attn.qkv.weight', 'backbones.0.blocks.4.attn.qkv.bias', 'backbones.0.blocks.4.attn.proj.weight', 'backbones.0.blocks.4.attn.proj.bias', 'backbones.0.blocks.4.norm2.weight', 'backbones.0.blocks.4.norm2.bias', 'backbones.0.blocks.4.mlp.fc1.weight', 'backbones.0.blocks.4.mlp.fc1.bias', 'backbones.0.blocks.4.mlp.fc2.weight', 'backbones.0.blocks.4.mlp.fc2.bias', 'backbones.0.blocks.5.norm1.weight', 'backbones.0.blocks.5.norm1.bias', 'backbones.0.blocks.5.attn.qkv.weight', 'backbones.0.blocks.5.attn.qkv.bias', 'backbones.0.blocks.5.attn.proj.weight', 'backbones.0.blocks.5.attn.proj.bias', 'backbones.0.blocks.5.norm2.weight', 'backbones.0.blocks.5.norm2.bias', 'backbones.0.blocks.5.mlp.fc1.weight', 'backbones.0.blocks.5.mlp.fc1.bias', 'backbones.0.blocks.5.mlp.fc2.weight', 'backbones.0.blocks.5.mlp.fc2.bias', 'backbones.0.blocks.6.norm1.weight', 'backbones.0.blocks.6.norm1.bias', 'backbones.0.blocks.6.attn.qkv.weight', 'backbones.0.blocks.6.attn.qkv.bias', 'backbones.0.blocks.6.attn.proj.weight', 'backbones.0.blocks.6.attn.proj.bias', 'backbones.0.blocks.6.norm2.weight', 'backbones.0.blocks.6.norm2.bias', 'backbones.0.blocks.6.mlp.fc1.weight', 'backbones.0.blocks.6.mlp.fc1.bias', 'backbones.0.blocks.6.mlp.fc2.weight', 'backbones.0.blocks.6.mlp.fc2.bias', 'backbones.0.blocks.7.norm1.weight', 'backbones.0.blocks.7.norm1.bias', 'backbones.0.blocks.7.attn.qkv.weight', 'backbones.0.blocks.7.attn.qkv.bias', 'backbones.0.blocks.7.attn.proj.weight', 'backbones.0.blocks.7.attn.proj.bias', 'backbones.0.blocks.7.norm2.weight', 'backbones.0.blocks.7.norm2.bias', 'backbones.0.blocks.7.mlp.fc1.weight', 'backbones.0.blocks.7.mlp.fc1.bias', 'backbones.0.blocks.7.mlp.fc2.weight', 'backbones.0.blocks.7.mlp.fc2.bias', 'backbones.0.blocks.8.norm1.weight', 'backbones.0.blocks.8.norm1.bias', 'backbones.0.blocks.8.attn.qkv.weight', 'backbones.0.blocks.8.attn.qkv.bias', 'backbones.0.blocks.8.attn.proj.weight', 'backbones.0.blocks.8.attn.proj.bias', 'backbones.0.blocks.8.norm2.weight', 'backbones.0.blocks.8.norm2.bias', 'backbones.0.blocks.8.mlp.fc1.weight', 'backbones.0.blocks.8.mlp.fc1.bias', 'backbones.0.blocks.8.mlp.fc2.weight', 'backbones.0.blocks.8.mlp.fc2.bias', 'backbones.0.blocks.9.norm1.weight', 'backbones.0.blocks.9.norm1.bias', 'backbones.0.blocks.9.attn.qkv.weight', 'backbones.0.blocks.9.attn.qkv.bias', 'backbones.0.blocks.9.attn.proj.weight', 'backbones.0.blocks.9.attn.proj.bias', 'backbones.0.blocks.9.norm2.weight', 'backbones.0.blocks.9.norm2.bias', 'backbones.0.blocks.9.mlp.fc1.weight', 'backbones.0.blocks.9.mlp.fc1.bias', 'backbones.0.blocks.9.mlp.fc2.weight', 'backbones.0.blocks.9.mlp.fc2.bias', 'backbones.0.blocks.10.norm1.weight', 'backbones.0.blocks.10.norm1.bias', 'backbones.0.blocks.10.attn.qkv.weight', 'backbones.0.blocks.10.attn.qkv.bias', 'backbones.0.blocks.10.attn.proj.weight', 'backbones.0.blocks.10.attn.proj.bias', 'backbones.0.blocks.10.norm2.weight', 'backbones.0.blocks.10.norm2.bias', 'backbones.0.blocks.10.mlp.fc1.weight', 'backbones.0.blocks.10.mlp.fc1.bias', 'backbones.0.blocks.10.mlp.fc2.weight', 'backbones.0.blocks.10.mlp.fc2.bias', 'backbones.0.blocks.11.norm1.weight', 'backbones.0.blocks.11.norm1.bias', 'backbones.0.blocks.11.attn.qkv.weight', 'backbones.0.blocks.11.attn.qkv.bias', 'backbones.0.blocks.11.attn.proj.weight', 'backbones.0.blocks.11.attn.proj.bias', 'backbones.0.blocks.11.norm2.weight', 'backbones.0.blocks.11.norm2.bias', 'backbones.0.blocks.11.mlp.fc1.weight', 'backbones.0.blocks.11.mlp.fc1.bias', 'backbones.0.blocks.11.mlp.fc2.weight', 'backbones.0.blocks.11.mlp.fc2.bias', 'backbones.0.norm.weight', 'backbones.0.norm.bias', 'backbones.1.cls_token', 'backbones.1.pos_embed', 'backbones.1.patch_embed.proj.weight', 'backbones.1.patch_embed.proj.bias', 'backbones.1.blocks.0.norm1.weight', 'backbones.1.blocks.0.norm1.bias', 'backbones.1.blocks.0.attn.q_proj.weight', 'backbones.1.blocks.0.attn.q_proj.bias', 'backbones.1.blocks.0.attn.v_proj.weight', 'backbones.1.blocks.0.attn.v_proj.bias', 'backbones.1.blocks.0.attn.k_proj.weight', 'backbones.1.blocks.0.attn.k_proj.bias', 'backbones.1.blocks.0.attn.proj.weight', 'backbones.1.blocks.0.attn.proj.bias', 'backbones.1.blocks.0.norm2.weight', 'backbones.1.blocks.0.norm2.bias', 'backbones.1.blocks.0.fc1.weight', 'backbones.1.blocks.0.fc1.bias', 'backbones.1.blocks.0.fc2.weight', 'backbones.1.blocks.0.fc2.bias', 'backbones.1.blocks.0.adaptmlp.down_proj.weight', 'backbones.1.blocks.0.adaptmlp.down_proj.bias', 'backbones.1.blocks.0.adaptmlp.up_proj.weight', 'backbones.1.blocks.0.adaptmlp.up_proj.bias', 'backbones.1.blocks.1.norm1.weight', 'backbones.1.blocks.1.norm1.bias', 'backbones.1.blocks.1.attn.q_proj.weight', 'backbones.1.blocks.1.attn.q_proj.bias', 'backbones.1.blocks.1.attn.v_proj.weight', 'backbones.1.blocks.1.attn.v_proj.bias', 'backbones.1.blocks.1.attn.k_proj.weight', 'backbones.1.blocks.1.attn.k_proj.bias', 'backbones.1.blocks.1.attn.proj.weight', 'backbones.1.blocks.1.attn.proj.bias', 'backbones.1.blocks.1.norm2.weight', 'backbones.1.blocks.1.norm2.bias', 'backbones.1.blocks.1.fc1.weight', 'backbones.1.blocks.1.fc1.bias', 'backbones.1.blocks.1.fc2.weight', 'backbones.1.blocks.1.fc2.bias', 'backbones.1.blocks.1.adaptmlp.down_proj.weight', 'backbones.1.blocks.1.adaptmlp.down_proj.bias', 'backbones.1.blocks.1.adaptmlp.up_proj.weight', 'backbones.1.blocks.1.adaptmlp.up_proj.bias', 'backbones.1.blocks.2.norm1.weight', 'backbones.1.blocks.2.norm1.bias', 'backbones.1.blocks.2.attn.q_proj.weight', 'backbones.1.blocks.2.attn.q_proj.bias', 'backbones.1.blocks.2.attn.v_proj.weight', 'backbones.1.blocks.2.attn.v_proj.bias', 'backbones.1.blocks.2.attn.k_proj.weight', 'backbones.1.blocks.2.attn.k_proj.bias', 'backbones.1.blocks.2.attn.proj.weight', 'backbones.1.blocks.2.attn.proj.bias', 'backbones.1.blocks.2.norm2.weight', 'backbones.1.blocks.2.norm2.bias', 'backbones.1.blocks.2.fc1.weight', 'backbones.1.blocks.2.fc1.bias', 'backbones.1.blocks.2.fc2.weight', 'backbones.1.blocks.2.fc2.bias', 'backbones.1.blocks.2.adaptmlp.down_proj.weight', 'backbones.1.blocks.2.adaptmlp.down_proj.bias', 'backbones.1.blocks.2.adaptmlp.up_proj.weight', 'backbones.1.blocks.2.adaptmlp.up_proj.bias', 'backbones.1.blocks.3.norm1.weight', 'backbones.1.blocks.3.norm1.bias', 'backbones.1.blocks.3.attn.q_proj.weight', 'backbones.1.blocks.3.attn.q_proj.bias', 'backbones.1.blocks.3.attn.v_proj.weight', 'backbones.1.blocks.3.attn.v_proj.bias', 'backbones.1.blocks.3.attn.k_proj.weight', 'backbones.1.blocks.3.attn.k_proj.bias', 'backbones.1.blocks.3.attn.proj.weight', 'backbones.1.blocks.3.attn.proj.bias', 'backbones.1.blocks.3.norm2.weight', 'backbones.1.blocks.3.norm2.bias', 'backbones.1.blocks.3.fc1.weight', 'backbones.1.blocks.3.fc1.bias', 'backbones.1.blocks.3.fc2.weight', 'backbones.1.blocks.3.fc2.bias', 'backbones.1.blocks.3.adaptmlp.down_proj.weight', 'backbones.1.blocks.3.adaptmlp.down_proj.bias', 'backbones.1.blocks.3.adaptmlp.up_proj.weight', 'backbones.1.blocks.3.adaptmlp.up_proj.bias', 'backbones.1.blocks.4.norm1.weight', 'backbones.1.blocks.4.norm1.bias', 'backbones.1.blocks.4.attn.q_proj.weight', 'backbones.1.blocks.4.attn.q_proj.bias', 'backbones.1.blocks.4.attn.v_proj.weight', 'backbones.1.blocks.4.attn.v_proj.bias', 'backbones.1.blocks.4.attn.k_proj.weight', 'backbones.1.blocks.4.attn.k_proj.bias', 'backbones.1.blocks.4.attn.proj.weight', 'backbones.1.blocks.4.attn.proj.bias', 'backbones.1.blocks.4.norm2.weight', 'backbones.1.blocks.4.norm2.bias', 'backbones.1.blocks.4.fc1.weight', 'backbones.1.blocks.4.fc1.bias', 'backbones.1.blocks.4.fc2.weight', 'backbones.1.blocks.4.fc2.bias', 'backbones.1.blocks.4.adaptmlp.down_proj.weight', 'backbones.1.blocks.4.adaptmlp.down_proj.bias', 'backbones.1.blocks.4.adaptmlp.up_proj.weight', 'backbones.1.blocks.4.adaptmlp.up_proj.bias', 'backbones.1.blocks.5.norm1.weight', 'backbones.1.blocks.5.norm1.bias', 'backbones.1.blocks.5.attn.q_proj.weight', 'backbones.1.blocks.5.attn.q_proj.bias', 'backbones.1.blocks.5.attn.v_proj.weight', 'backbones.1.blocks.5.attn.v_proj.bias', 'backbones.1.blocks.5.attn.k_proj.weight', 'backbones.1.blocks.5.attn.k_proj.bias', 'backbones.1.blocks.5.attn.proj.weight', 'backbones.1.blocks.5.attn.proj.bias', 'backbones.1.blocks.5.norm2.weight', 'backbones.1.blocks.5.norm2.bias', 'backbones.1.blocks.5.fc1.weight', 'backbones.1.blocks.5.fc1.bias', 'backbones.1.blocks.5.fc2.weight', 'backbones.1.blocks.5.fc2.bias', 'backbones.1.blocks.5.adaptmlp.down_proj.weight', 'backbones.1.blocks.5.adaptmlp.down_proj.bias', 'backbones.1.blocks.5.adaptmlp.up_proj.weight', 'backbones.1.blocks.5.adaptmlp.up_proj.bias', 'backbones.1.blocks.6.norm1.weight', 'backbones.1.blocks.6.norm1.bias', 'backbones.1.blocks.6.attn.q_proj.weight', 'backbones.1.blocks.6.attn.q_proj.bias', 'backbones.1.blocks.6.attn.v_proj.weight', 'backbones.1.blocks.6.attn.v_proj.bias', 'backbones.1.blocks.6.attn.k_proj.weight', 'backbones.1.blocks.6.attn.k_proj.bias', 'backbones.1.blocks.6.attn.proj.weight', 'backbones.1.blocks.6.attn.proj.bias', 'backbones.1.blocks.6.norm2.weight', 'backbones.1.blocks.6.norm2.bias', 'backbones.1.blocks.6.fc1.weight', 'backbones.1.blocks.6.fc1.bias', 'backbones.1.blocks.6.fc2.weight', 'backbones.1.blocks.6.fc2.bias', 'backbones.1.blocks.6.adaptmlp.down_proj.weight', 'backbones.1.blocks.6.adaptmlp.down_proj.bias', 'backbones.1.blocks.6.adaptmlp.up_proj.weight', 'backbones.1.blocks.6.adaptmlp.up_proj.bias', 'backbones.1.blocks.7.norm1.weight', 'backbones.1.blocks.7.norm1.bias', 'backbones.1.blocks.7.attn.q_proj.weight', 'backbones.1.blocks.7.attn.q_proj.bias', 'backbones.1.blocks.7.attn.v_proj.weight', 'backbones.1.blocks.7.attn.v_proj.bias', 'backbones.1.blocks.7.attn.k_proj.weight', 'backbones.1.blocks.7.attn.k_proj.bias', 'backbones.1.blocks.7.attn.proj.weight', 'backbones.1.blocks.7.attn.proj.bias', 'backbones.1.blocks.7.norm2.weight', 'backbones.1.blocks.7.norm2.bias', 'backbones.1.blocks.7.fc1.weight', 'backbones.1.blocks.7.fc1.bias', 'backbones.1.blocks.7.fc2.weight', 'backbones.1.blocks.7.fc2.bias', 'backbones.1.blocks.7.adaptmlp.down_proj.weight', 'backbones.1.blocks.7.adaptmlp.down_proj.bias', 'backbones.1.blocks.7.adaptmlp.up_proj.weight', 'backbones.1.blocks.7.adaptmlp.up_proj.bias', 'backbones.1.blocks.8.norm1.weight', 'backbones.1.blocks.8.norm1.bias', 'backbones.1.blocks.8.attn.q_proj.weight', 'backbones.1.blocks.8.attn.q_proj.bias', 'backbones.1.blocks.8.attn.v_proj.weight', 'backbones.1.blocks.8.attn.v_proj.bias', 'backbones.1.blocks.8.attn.k_proj.weight', 'backbones.1.blocks.8.attn.k_proj.bias', 'backbones.1.blocks.8.attn.proj.weight', 'backbones.1.blocks.8.attn.proj.bias', 'backbones.1.blocks.8.norm2.weight', 'backbones.1.blocks.8.norm2.bias', 'backbones.1.blocks.8.fc1.weight', 'backbones.1.blocks.8.fc1.bias', 'backbones.1.blocks.8.fc2.weight', 'backbones.1.blocks.8.fc2.bias', 'backbones.1.blocks.8.adaptmlp.down_proj.weight', 'backbones.1.blocks.8.adaptmlp.down_proj.bias', 'backbones.1.blocks.8.adaptmlp.up_proj.weight', 'backbones.1.blocks.8.adaptmlp.up_proj.bias', 'backbones.1.blocks.9.norm1.weight', 'backbones.1.blocks.9.norm1.bias', 'backbones.1.blocks.9.attn.q_proj.weight', 'backbones.1.blocks.9.attn.q_proj.bias', 'backbones.1.blocks.9.attn.v_proj.weight', 'backbones.1.blocks.9.attn.v_proj.bias', 'backbones.1.blocks.9.attn.k_proj.weight', 'backbones.1.blocks.9.attn.k_proj.bias', 'backbones.1.blocks.9.attn.proj.weight', 'backbones.1.blocks.9.attn.proj.bias', 'backbones.1.blocks.9.norm2.weight', 'backbones.1.blocks.9.norm2.bias', 'backbones.1.blocks.9.fc1.weight', 'backbones.1.blocks.9.fc1.bias', 'backbones.1.blocks.9.fc2.weight', 'backbones.1.blocks.9.fc2.bias', 'backbones.1.blocks.9.adaptmlp.down_proj.weight', 'backbones.1.blocks.9.adaptmlp.down_proj.bias', 'backbones.1.blocks.9.adaptmlp.up_proj.weight', 'backbones.1.blocks.9.adaptmlp.up_proj.bias', 'backbones.1.blocks.10.norm1.weight', 'backbones.1.blocks.10.norm1.bias', 'backbones.1.blocks.10.attn.q_proj.weight', 'backbones.1.blocks.10.attn.q_proj.bias', 'backbones.1.blocks.10.attn.v_proj.weight', 'backbones.1.blocks.10.attn.v_proj.bias', 'backbones.1.blocks.10.attn.k_proj.weight', 'backbones.1.blocks.10.attn.k_proj.bias', 'backbones.1.blocks.10.attn.proj.weight', 'backbones.1.blocks.10.attn.proj.bias', 'backbones.1.blocks.10.norm2.weight', 'backbones.1.blocks.10.norm2.bias', 'backbones.1.blocks.10.fc1.weight', 'backbones.1.blocks.10.fc1.bias', 'backbones.1.blocks.10.fc2.weight', 'backbones.1.blocks.10.fc2.bias', 'backbones.1.blocks.10.adaptmlp.down_proj.weight', 'backbones.1.blocks.10.adaptmlp.down_proj.bias', 'backbones.1.blocks.10.adaptmlp.up_proj.weight', 'backbones.1.blocks.10.adaptmlp.up_proj.bias', 'backbones.1.blocks.11.norm1.weight', 'backbones.1.blocks.11.norm1.bias', 'backbones.1.blocks.11.attn.q_proj.weight', 'backbones.1.blocks.11.attn.q_proj.bias', 'backbones.1.blocks.11.attn.v_proj.weight', 'backbones.1.blocks.11.attn.v_proj.bias', 'backbones.1.blocks.11.attn.k_proj.weight', 'backbones.1.blocks.11.attn.k_proj.bias', 'backbones.1.blocks.11.attn.proj.weight', 'backbones.1.blocks.11.attn.proj.bias', 'backbones.1.blocks.11.norm2.weight', 'backbones.1.blocks.11.norm2.bias', 'backbones.1.blocks.11.fc1.weight', 'backbones.1.blocks.11.fc1.bias', 'backbones.1.blocks.11.fc2.weight', 'backbones.1.blocks.11.fc2.bias', 'backbones.1.blocks.11.adaptmlp.down_proj.weight', 'backbones.1.blocks.11.adaptmlp.down_proj.bias', 'backbones.1.blocks.11.adaptmlp.up_proj.weight', 'backbones.1.blocks.11.adaptmlp.up_proj.bias', 'backbones.1.norm.weight', 'backbones.1.norm.bias', 'fc.weight', 'fc.sigma'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8011bb12-92d4-46b1-89fd-f12abd6508f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.backbone.cls_token\", \"module.backbone.pos_embed\", \"module.backbone.patch_embed.proj.weight\", \"module.backbone.patch_embed.proj.bias\", \"module.backbone.blocks.0.norm1.weight\", \"module.backbone.blocks.0.norm1.bias\", \"module.backbone.blocks.0.attn.q_proj.weight\", \"module.backbone.blocks.0.attn.q_proj.bias\", \"module.backbone.blocks.0.attn.v_proj.weight\", \"module.backbone.blocks.0.attn.v_proj.bias\", \"module.backbone.blocks.0.attn.k_proj.weight\", \"module.backbone.blocks.0.attn.k_proj.bias\", \"module.backbone.blocks.0.attn.proj.weight\", \"module.backbone.blocks.0.attn.proj.bias\", \"module.backbone.blocks.0.norm2.weight\", \"module.backbone.blocks.0.norm2.bias\", \"module.backbone.blocks.0.fc1.weight\", \"module.backbone.blocks.0.fc1.bias\", \"module.backbone.blocks.0.fc2.weight\", \"module.backbone.blocks.0.fc2.bias\", \"module.backbone.blocks.0.adaptmlp.down_proj.weight\", \"module.backbone.blocks.0.adaptmlp.down_proj.bias\", \"module.backbone.blocks.0.adaptmlp.up_proj.weight\", \"module.backbone.blocks.0.adaptmlp.up_proj.bias\", \"module.backbone.blocks.1.norm1.weight\", \"module.backbone.blocks.1.norm1.bias\", \"module.backbone.blocks.1.attn.q_proj.weight\", \"module.backbone.blocks.1.attn.q_proj.bias\", \"module.backbone.blocks.1.attn.v_proj.weight\", \"module.backbone.blocks.1.attn.v_proj.bias\", \"module.backbone.blocks.1.attn.k_proj.weight\", \"module.backbone.blocks.1.attn.k_proj.bias\", \"module.backbone.blocks.1.attn.proj.weight\", \"module.backbone.blocks.1.attn.proj.bias\", \"module.backbone.blocks.1.norm2.weight\", \"module.backbone.blocks.1.norm2.bias\", \"module.backbone.blocks.1.fc1.weight\", \"module.backbone.blocks.1.fc1.bias\", \"module.backbone.blocks.1.fc2.weight\", \"module.backbone.blocks.1.fc2.bias\", \"module.backbone.blocks.1.adaptmlp.down_proj.weight\", \"module.backbone.blocks.1.adaptmlp.down_proj.bias\", \"module.backbone.blocks.1.adaptmlp.up_proj.weight\", \"module.backbone.blocks.1.adaptmlp.up_proj.bias\", \"module.backbone.blocks.2.norm1.weight\", \"module.backbone.blocks.2.norm1.bias\", \"module.backbone.blocks.2.attn.q_proj.weight\", \"module.backbone.blocks.2.attn.q_proj.bias\", \"module.backbone.blocks.2.attn.v_proj.weight\", \"module.backbone.blocks.2.attn.v_proj.bias\", \"module.backbone.blocks.2.attn.k_proj.weight\", \"module.backbone.blocks.2.attn.k_proj.bias\", \"module.backbone.blocks.2.attn.proj.weight\", \"module.backbone.blocks.2.attn.proj.bias\", \"module.backbone.blocks.2.norm2.weight\", \"module.backbone.blocks.2.norm2.bias\", \"module.backbone.blocks.2.fc1.weight\", \"module.backbone.blocks.2.fc1.bias\", \"module.backbone.blocks.2.fc2.weight\", \"module.backbone.blocks.2.fc2.bias\", \"module.backbone.blocks.2.adaptmlp.down_proj.weight\", \"module.backbone.blocks.2.adaptmlp.down_proj.bias\", \"module.backbone.blocks.2.adaptmlp.up_proj.weight\", \"module.backbone.blocks.2.adaptmlp.up_proj.bias\", \"module.backbone.blocks.3.norm1.weight\", \"module.backbone.blocks.3.norm1.bias\", \"module.backbone.blocks.3.attn.q_proj.weight\", \"module.backbone.blocks.3.attn.q_proj.bias\", \"module.backbone.blocks.3.attn.v_proj.weight\", \"module.backbone.blocks.3.attn.v_proj.bias\", \"module.backbone.blocks.3.attn.k_proj.weight\", \"module.backbone.blocks.3.attn.k_proj.bias\", \"module.backbone.blocks.3.attn.proj.weight\", \"module.backbone.blocks.3.attn.proj.bias\", \"module.backbone.blocks.3.norm2.weight\", \"module.backbone.blocks.3.norm2.bias\", \"module.backbone.blocks.3.fc1.weight\", \"module.backbone.blocks.3.fc1.bias\", \"module.backbone.blocks.3.fc2.weight\", \"module.backbone.blocks.3.fc2.bias\", \"module.backbone.blocks.3.adaptmlp.down_proj.weight\", \"module.backbone.blocks.3.adaptmlp.down_proj.bias\", \"module.backbone.blocks.3.adaptmlp.up_proj.weight\", \"module.backbone.blocks.3.adaptmlp.up_proj.bias\", \"module.backbone.blocks.4.norm1.weight\", \"module.backbone.blocks.4.norm1.bias\", \"module.backbone.blocks.4.attn.q_proj.weight\", \"module.backbone.blocks.4.attn.q_proj.bias\", \"module.backbone.blocks.4.attn.v_proj.weight\", \"module.backbone.blocks.4.attn.v_proj.bias\", \"module.backbone.blocks.4.attn.k_proj.weight\", \"module.backbone.blocks.4.attn.k_proj.bias\", \"module.backbone.blocks.4.attn.proj.weight\", \"module.backbone.blocks.4.attn.proj.bias\", \"module.backbone.blocks.4.norm2.weight\", \"module.backbone.blocks.4.norm2.bias\", \"module.backbone.blocks.4.fc1.weight\", \"module.backbone.blocks.4.fc1.bias\", \"module.backbone.blocks.4.fc2.weight\", \"module.backbone.blocks.4.fc2.bias\", \"module.backbone.blocks.4.adaptmlp.down_proj.weight\", \"module.backbone.blocks.4.adaptmlp.down_proj.bias\", \"module.backbone.blocks.4.adaptmlp.up_proj.weight\", \"module.backbone.blocks.4.adaptmlp.up_proj.bias\", \"module.backbone.blocks.5.norm1.weight\", \"module.backbone.blocks.5.norm1.bias\", \"module.backbone.blocks.5.attn.q_proj.weight\", \"module.backbone.blocks.5.attn.q_proj.bias\", \"module.backbone.blocks.5.attn.v_proj.weight\", \"module.backbone.blocks.5.attn.v_proj.bias\", \"module.backbone.blocks.5.attn.k_proj.weight\", \"module.backbone.blocks.5.attn.k_proj.bias\", \"module.backbone.blocks.5.attn.proj.weight\", \"module.backbone.blocks.5.attn.proj.bias\", \"module.backbone.blocks.5.norm2.weight\", \"module.backbone.blocks.5.norm2.bias\", \"module.backbone.blocks.5.fc1.weight\", \"module.backbone.blocks.5.fc1.bias\", \"module.backbone.blocks.5.fc2.weight\", \"module.backbone.blocks.5.fc2.bias\", \"module.backbone.blocks.5.adaptmlp.down_proj.weight\", \"module.backbone.blocks.5.adaptmlp.down_proj.bias\", \"module.backbone.blocks.5.adaptmlp.up_proj.weight\", \"module.backbone.blocks.5.adaptmlp.up_proj.bias\", \"module.backbone.blocks.6.norm1.weight\", \"module.backbone.blocks.6.norm1.bias\", \"module.backbone.blocks.6.attn.q_proj.weight\", \"module.backbone.blocks.6.attn.q_proj.bias\", \"module.backbone.blocks.6.attn.v_proj.weight\", \"module.backbone.blocks.6.attn.v_proj.bias\", \"module.backbone.blocks.6.attn.k_proj.weight\", \"module.backbone.blocks.6.attn.k_proj.bias\", \"module.backbone.blocks.6.attn.proj.weight\", \"module.backbone.blocks.6.attn.proj.bias\", \"module.backbone.blocks.6.norm2.weight\", \"module.backbone.blocks.6.norm2.bias\", \"module.backbone.blocks.6.fc1.weight\", \"module.backbone.blocks.6.fc1.bias\", \"module.backbone.blocks.6.fc2.weight\", \"module.backbone.blocks.6.fc2.bias\", \"module.backbone.blocks.6.adaptmlp.down_proj.weight\", \"module.backbone.blocks.6.adaptmlp.down_proj.bias\", \"module.backbone.blocks.6.adaptmlp.up_proj.weight\", \"module.backbone.blocks.6.adaptmlp.up_proj.bias\", \"module.backbone.blocks.7.norm1.weight\", \"module.backbone.blocks.7.norm1.bias\", \"module.backbone.blocks.7.attn.q_proj.weight\", \"module.backbone.blocks.7.attn.q_proj.bias\", \"module.backbone.blocks.7.attn.v_proj.weight\", \"module.backbone.blocks.7.attn.v_proj.bias\", \"module.backbone.blocks.7.attn.k_proj.weight\", \"module.backbone.blocks.7.attn.k_proj.bias\", \"module.backbone.blocks.7.attn.proj.weight\", \"module.backbone.blocks.7.attn.proj.bias\", \"module.backbone.blocks.7.norm2.weight\", \"module.backbone.blocks.7.norm2.bias\", \"module.backbone.blocks.7.fc1.weight\", \"module.backbone.blocks.7.fc1.bias\", \"module.backbone.blocks.7.fc2.weight\", \"module.backbone.blocks.7.fc2.bias\", \"module.backbone.blocks.7.adaptmlp.down_proj.weight\", \"module.backbone.blocks.7.adaptmlp.down_proj.bias\", \"module.backbone.blocks.7.adaptmlp.up_proj.weight\", \"module.backbone.blocks.7.adaptmlp.up_proj.bias\", \"module.backbone.blocks.8.norm1.weight\", \"module.backbone.blocks.8.norm1.bias\", \"module.backbone.blocks.8.attn.q_proj.weight\", \"module.backbone.blocks.8.attn.q_proj.bias\", \"module.backbone.blocks.8.attn.v_proj.weight\", \"module.backbone.blocks.8.attn.v_proj.bias\", \"module.backbone.blocks.8.attn.k_proj.weight\", \"module.backbone.blocks.8.attn.k_proj.bias\", \"module.backbone.blocks.8.attn.proj.weight\", \"module.backbone.blocks.8.attn.proj.bias\", \"module.backbone.blocks.8.norm2.weight\", \"module.backbone.blocks.8.norm2.bias\", \"module.backbone.blocks.8.fc1.weight\", \"module.backbone.blocks.8.fc1.bias\", \"module.backbone.blocks.8.fc2.weight\", \"module.backbone.blocks.8.fc2.bias\", \"module.backbone.blocks.8.adaptmlp.down_proj.weight\", \"module.backbone.blocks.8.adaptmlp.down_proj.bias\", \"module.backbone.blocks.8.adaptmlp.up_proj.weight\", \"module.backbone.blocks.8.adaptmlp.up_proj.bias\", \"module.backbone.blocks.9.norm1.weight\", \"module.backbone.blocks.9.norm1.bias\", \"module.backbone.blocks.9.attn.q_proj.weight\", \"module.backbone.blocks.9.attn.q_proj.bias\", \"module.backbone.blocks.9.attn.v_proj.weight\", \"module.backbone.blocks.9.attn.v_proj.bias\", \"module.backbone.blocks.9.attn.k_proj.weight\", \"module.backbone.blocks.9.attn.k_proj.bias\", \"module.backbone.blocks.9.attn.proj.weight\", \"module.backbone.blocks.9.attn.proj.bias\", \"module.backbone.blocks.9.norm2.weight\", \"module.backbone.blocks.9.norm2.bias\", \"module.backbone.blocks.9.fc1.weight\", \"module.backbone.blocks.9.fc1.bias\", \"module.backbone.blocks.9.fc2.weight\", \"module.backbone.blocks.9.fc2.bias\", \"module.backbone.blocks.9.adaptmlp.down_proj.weight\", \"module.backbone.blocks.9.adaptmlp.down_proj.bias\", \"module.backbone.blocks.9.adaptmlp.up_proj.weight\", \"module.backbone.blocks.9.adaptmlp.up_proj.bias\", \"module.backbone.blocks.10.norm1.weight\", \"module.backbone.blocks.10.norm1.bias\", \"module.backbone.blocks.10.attn.q_proj.weight\", \"module.backbone.blocks.10.attn.q_proj.bias\", \"module.backbone.blocks.10.attn.v_proj.weight\", \"module.backbone.blocks.10.attn.v_proj.bias\", \"module.backbone.blocks.10.attn.k_proj.weight\", \"module.backbone.blocks.10.attn.k_proj.bias\", \"module.backbone.blocks.10.attn.proj.weight\", \"module.backbone.blocks.10.attn.proj.bias\", \"module.backbone.blocks.10.norm2.weight\", \"module.backbone.blocks.10.norm2.bias\", \"module.backbone.blocks.10.fc1.weight\", \"module.backbone.blocks.10.fc1.bias\", \"module.backbone.blocks.10.fc2.weight\", \"module.backbone.blocks.10.fc2.bias\", \"module.backbone.blocks.10.adaptmlp.down_proj.weight\", \"module.backbone.blocks.10.adaptmlp.down_proj.bias\", \"module.backbone.blocks.10.adaptmlp.up_proj.weight\", \"module.backbone.blocks.10.adaptmlp.up_proj.bias\", \"module.backbone.blocks.11.norm1.weight\", \"module.backbone.blocks.11.norm1.bias\", \"module.backbone.blocks.11.attn.q_proj.weight\", \"module.backbone.blocks.11.attn.q_proj.bias\", \"module.backbone.blocks.11.attn.v_proj.weight\", \"module.backbone.blocks.11.attn.v_proj.bias\", \"module.backbone.blocks.11.attn.k_proj.weight\", \"module.backbone.blocks.11.attn.k_proj.bias\", \"module.backbone.blocks.11.attn.proj.weight\", \"module.backbone.blocks.11.attn.proj.bias\", \"module.backbone.blocks.11.norm2.weight\", \"module.backbone.blocks.11.norm2.bias\", \"module.backbone.blocks.11.fc1.weight\", \"module.backbone.blocks.11.fc1.bias\", \"module.backbone.blocks.11.fc2.weight\", \"module.backbone.blocks.11.fc2.bias\", \"module.backbone.blocks.11.adaptmlp.down_proj.weight\", \"module.backbone.blocks.11.adaptmlp.down_proj.bias\", \"module.backbone.blocks.11.adaptmlp.up_proj.weight\", \"module.backbone.blocks.11.adaptmlp.up_proj.bias\", \"module.backbone.norm.weight\", \"module.backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model\u001b[38;5;241m.\u001b[39m_network)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.backbone.cls_token\", \"module.backbone.pos_embed\", \"module.backbone.patch_embed.proj.weight\", \"module.backbone.patch_embed.proj.bias\", \"module.backbone.blocks.0.norm1.weight\", \"module.backbone.blocks.0.norm1.bias\", \"module.backbone.blocks.0.attn.q_proj.weight\", \"module.backbone.blocks.0.attn.q_proj.bias\", \"module.backbone.blocks.0.attn.v_proj.weight\", \"module.backbone.blocks.0.attn.v_proj.bias\", \"module.backbone.blocks.0.attn.k_proj.weight\", \"module.backbone.blocks.0.attn.k_proj.bias\", \"module.backbone.blocks.0.attn.proj.weight\", \"module.backbone.blocks.0.attn.proj.bias\", \"module.backbone.blocks.0.norm2.weight\", \"module.backbone.blocks.0.norm2.bias\", \"module.backbone.blocks.0.fc1.weight\", \"module.backbone.blocks.0.fc1.bias\", \"module.backbone.blocks.0.fc2.weight\", \"module.backbone.blocks.0.fc2.bias\", \"module.backbone.blocks.0.adaptmlp.down_proj.weight\", \"module.backbone.blocks.0.adaptmlp.down_proj.bias\", \"module.backbone.blocks.0.adaptmlp.up_proj.weight\", \"module.backbone.blocks.0.adaptmlp.up_proj.bias\", \"module.backbone.blocks.1.norm1.weight\", \"module.backbone.blocks.1.norm1.bias\", \"module.backbone.blocks.1.attn.q_proj.weight\", \"module.backbone.blocks.1.attn.q_proj.bias\", \"module.backbone.blocks.1.attn.v_proj.weight\", \"module.backbone.blocks.1.attn.v_proj.bias\", \"module.backbone.blocks.1.attn.k_proj.weight\", \"module.backbone.blocks.1.attn.k_proj.bias\", \"module.backbone.blocks.1.attn.proj.weight\", \"module.backbone.blocks.1.attn.proj.bias\", \"module.backbone.blocks.1.norm2.weight\", \"module.backbone.blocks.1.norm2.bias\", \"module.backbone.blocks.1.fc1.weight\", \"module.backbone.blocks.1.fc1.bias\", \"module.backbone.blocks.1.fc2.weight\", \"module.backbone.blocks.1.fc2.bias\", \"module.backbone.blocks.1.adaptmlp.down_proj.weight\", \"module.backbone.blocks.1.adaptmlp.down_proj.bias\", \"module.backbone.blocks.1.adaptmlp.up_proj.weight\", \"module.backbone.blocks.1.adaptmlp.up_proj.bias\", \"module.backbone.blocks.2.norm1.weight\", \"module.backbone.blocks.2.norm1.bias\", \"module.backbone.blocks.2.attn.q_proj.weight\", \"module.backbone.blocks.2.attn.q_proj.bias\", \"module.backbone.blocks.2.attn.v_proj.weight\", \"module.backbone.blocks.2.attn.v_proj.bias\", \"module.backbone.blocks.2.attn.k_proj.weight\", \"module.backbone.blocks.2.attn.k_proj.bias\", \"module.backbone.blocks.2.attn.proj.weight\", \"module.backbone.blocks.2.attn.proj.bias\", \"module.backbone.blocks.2.norm2.weight\", \"module.backbone.blocks.2.norm2.bias\", \"module.backbone.blocks.2.fc1.weight\", \"module.backbone.blocks.2.fc1.bias\", \"module.backbone.blocks.2.fc2.weight\", \"module.backbone.blocks.2.fc2.bias\", \"module.backbone.blocks.2.adaptmlp.down_proj.weight\", \"module.backbone.blocks.2.adaptmlp.down_proj.bias\", \"module.backbone.blocks.2.adaptmlp.up_proj.weight\", \"module.backbone.blocks.2.adaptmlp.up_proj.bias\", \"module.backbone.blocks.3.norm1.weight\", \"module.backbone.blocks.3.norm1.bias\", \"module.backbone.blocks.3.attn.q_proj.weight\", \"module.backbone.blocks.3.attn.q_proj.bias\", \"module.backbone.blocks.3.attn.v_proj.weight\", \"module.backbone.blocks.3.attn.v_proj.bias\", \"module.backbone.blocks.3.attn.k_proj.weight\", \"module.backbone.blocks.3.attn.k_proj.bias\", \"module.backbone.blocks.3.attn.proj.weight\", \"module.backbone.blocks.3.attn.proj.bias\", \"module.backbone.blocks.3.norm2.weight\", \"module.backbone.blocks.3.norm2.bias\", \"module.backbone.blocks.3.fc1.weight\", \"module.backbone.blocks.3.fc1.bias\", \"module.backbone.blocks.3.fc2.weight\", \"module.backbone.blocks.3.fc2.bias\", \"module.backbone.blocks.3.adaptmlp.down_proj.weight\", \"module.backbone.blocks.3.adaptmlp.down_proj.bias\", \"module.backbone.blocks.3.adaptmlp.up_proj.weight\", \"module.backbone.blocks.3.adaptmlp.up_proj.bias\", \"module.backbone.blocks.4.norm1.weight\", \"module.backbone.blocks.4.norm1.bias\", \"module.backbone.blocks.4.attn.q_proj.weight\", \"module.backbone.blocks.4.attn.q_proj.bias\", \"module.backbone.blocks.4.attn.v_proj.weight\", \"module.backbone.blocks.4.attn.v_proj.bias\", \"module.backbone.blocks.4.attn.k_proj.weight\", \"module.backbone.blocks.4.attn.k_proj.bias\", \"module.backbone.blocks.4.attn.proj.weight\", \"module.backbone.blocks.4.attn.proj.bias\", \"module.backbone.blocks.4.norm2.weight\", \"module.backbone.blocks.4.norm2.bias\", \"module.backbone.blocks.4.fc1.weight\", \"module.backbone.blocks.4.fc1.bias\", \"module.backbone.blocks.4.fc2.weight\", \"module.backbone.blocks.4.fc2.bias\", \"module.backbone.blocks.4.adaptmlp.down_proj.weight\", \"module.backbone.blocks.4.adaptmlp.down_proj.bias\", \"module.backbone.blocks.4.adaptmlp.up_proj.weight\", \"module.backbone.blocks.4.adaptmlp.up_proj.bias\", \"module.backbone.blocks.5.norm1.weight\", \"module.backbone.blocks.5.norm1.bias\", \"module.backbone.blocks.5.attn.q_proj.weight\", \"module.backbone.blocks.5.attn.q_proj.bias\", \"module.backbone.blocks.5.attn.v_proj.weight\", \"module.backbone.blocks.5.attn.v_proj.bias\", \"module.backbone.blocks.5.attn.k_proj.weight\", \"module.backbone.blocks.5.attn.k_proj.bias\", \"module.backbone.blocks.5.attn.proj.weight\", \"module.backbone.blocks.5.attn.proj.bias\", \"module.backbone.blocks.5.norm2.weight\", \"module.backbone.blocks.5.norm2.bias\", \"module.backbone.blocks.5.fc1.weight\", \"module.backbone.blocks.5.fc1.bias\", \"module.backbone.blocks.5.fc2.weight\", \"module.backbone.blocks.5.fc2.bias\", \"module.backbone.blocks.5.adaptmlp.down_proj.weight\", \"module.backbone.blocks.5.adaptmlp.down_proj.bias\", \"module.backbone.blocks.5.adaptmlp.up_proj.weight\", \"module.backbone.blocks.5.adaptmlp.up_proj.bias\", \"module.backbone.blocks.6.norm1.weight\", \"module.backbone.blocks.6.norm1.bias\", \"module.backbone.blocks.6.attn.q_proj.weight\", \"module.backbone.blocks.6.attn.q_proj.bias\", \"module.backbone.blocks.6.attn.v_proj.weight\", \"module.backbone.blocks.6.attn.v_proj.bias\", \"module.backbone.blocks.6.attn.k_proj.weight\", \"module.backbone.blocks.6.attn.k_proj.bias\", \"module.backbone.blocks.6.attn.proj.weight\", \"module.backbone.blocks.6.attn.proj.bias\", \"module.backbone.blocks.6.norm2.weight\", \"module.backbone.blocks.6.norm2.bias\", \"module.backbone.blocks.6.fc1.weight\", \"module.backbone.blocks.6.fc1.bias\", \"module.backbone.blocks.6.fc2.weight\", \"module.backbone.blocks.6.fc2.bias\", \"module.backbone.blocks.6.adaptmlp.down_proj.weight\", \"module.backbone.blocks.6.adaptmlp.down_proj.bias\", \"module.backbone.blocks.6.adaptmlp.up_proj.weight\", \"module.backbone.blocks.6.adaptmlp.up_proj.bias\", \"module.backbone.blocks.7.norm1.weight\", \"module.backbone.blocks.7.norm1.bias\", \"module.backbone.blocks.7.attn.q_proj.weight\", \"module.backbone.blocks.7.attn.q_proj.bias\", \"module.backbone.blocks.7.attn.v_proj.weight\", \"module.backbone.blocks.7.attn.v_proj.bias\", \"module.backbone.blocks.7.attn.k_proj.weight\", \"module.backbone.blocks.7.attn.k_proj.bias\", \"module.backbone.blocks.7.attn.proj.weight\", \"module.backbone.blocks.7.attn.proj.bias\", \"module.backbone.blocks.7.norm2.weight\", \"module.backbone.blocks.7.norm2.bias\", \"module.backbone.blocks.7.fc1.weight\", \"module.backbone.blocks.7.fc1.bias\", \"module.backbone.blocks.7.fc2.weight\", \"module.backbone.blocks.7.fc2.bias\", \"module.backbone.blocks.7.adaptmlp.down_proj.weight\", \"module.backbone.blocks.7.adaptmlp.down_proj.bias\", \"module.backbone.blocks.7.adaptmlp.up_proj.weight\", \"module.backbone.blocks.7.adaptmlp.up_proj.bias\", \"module.backbone.blocks.8.norm1.weight\", \"module.backbone.blocks.8.norm1.bias\", \"module.backbone.blocks.8.attn.q_proj.weight\", \"module.backbone.blocks.8.attn.q_proj.bias\", \"module.backbone.blocks.8.attn.v_proj.weight\", \"module.backbone.blocks.8.attn.v_proj.bias\", \"module.backbone.blocks.8.attn.k_proj.weight\", \"module.backbone.blocks.8.attn.k_proj.bias\", \"module.backbone.blocks.8.attn.proj.weight\", \"module.backbone.blocks.8.attn.proj.bias\", \"module.backbone.blocks.8.norm2.weight\", \"module.backbone.blocks.8.norm2.bias\", \"module.backbone.blocks.8.fc1.weight\", \"module.backbone.blocks.8.fc1.bias\", \"module.backbone.blocks.8.fc2.weight\", \"module.backbone.blocks.8.fc2.bias\", \"module.backbone.blocks.8.adaptmlp.down_proj.weight\", \"module.backbone.blocks.8.adaptmlp.down_proj.bias\", \"module.backbone.blocks.8.adaptmlp.up_proj.weight\", \"module.backbone.blocks.8.adaptmlp.up_proj.bias\", \"module.backbone.blocks.9.norm1.weight\", \"module.backbone.blocks.9.norm1.bias\", \"module.backbone.blocks.9.attn.q_proj.weight\", \"module.backbone.blocks.9.attn.q_proj.bias\", \"module.backbone.blocks.9.attn.v_proj.weight\", \"module.backbone.blocks.9.attn.v_proj.bias\", \"module.backbone.blocks.9.attn.k_proj.weight\", \"module.backbone.blocks.9.attn.k_proj.bias\", \"module.backbone.blocks.9.attn.proj.weight\", \"module.backbone.blocks.9.attn.proj.bias\", \"module.backbone.blocks.9.norm2.weight\", \"module.backbone.blocks.9.norm2.bias\", \"module.backbone.blocks.9.fc1.weight\", \"module.backbone.blocks.9.fc1.bias\", \"module.backbone.blocks.9.fc2.weight\", \"module.backbone.blocks.9.fc2.bias\", \"module.backbone.blocks.9.adaptmlp.down_proj.weight\", \"module.backbone.blocks.9.adaptmlp.down_proj.bias\", \"module.backbone.blocks.9.adaptmlp.up_proj.weight\", \"module.backbone.blocks.9.adaptmlp.up_proj.bias\", \"module.backbone.blocks.10.norm1.weight\", \"module.backbone.blocks.10.norm1.bias\", \"module.backbone.blocks.10.attn.q_proj.weight\", \"module.backbone.blocks.10.attn.q_proj.bias\", \"module.backbone.blocks.10.attn.v_proj.weight\", \"module.backbone.blocks.10.attn.v_proj.bias\", \"module.backbone.blocks.10.attn.k_proj.weight\", \"module.backbone.blocks.10.attn.k_proj.bias\", \"module.backbone.blocks.10.attn.proj.weight\", \"module.backbone.blocks.10.attn.proj.bias\", \"module.backbone.blocks.10.norm2.weight\", \"module.backbone.blocks.10.norm2.bias\", \"module.backbone.blocks.10.fc1.weight\", \"module.backbone.blocks.10.fc1.bias\", \"module.backbone.blocks.10.fc2.weight\", \"module.backbone.blocks.10.fc2.bias\", \"module.backbone.blocks.10.adaptmlp.down_proj.weight\", \"module.backbone.blocks.10.adaptmlp.down_proj.bias\", \"module.backbone.blocks.10.adaptmlp.up_proj.weight\", \"module.backbone.blocks.10.adaptmlp.up_proj.bias\", \"module.backbone.blocks.11.norm1.weight\", \"module.backbone.blocks.11.norm1.bias\", \"module.backbone.blocks.11.attn.q_proj.weight\", \"module.backbone.blocks.11.attn.q_proj.bias\", \"module.backbone.blocks.11.attn.v_proj.weight\", \"module.backbone.blocks.11.attn.v_proj.bias\", \"module.backbone.blocks.11.attn.k_proj.weight\", \"module.backbone.blocks.11.attn.k_proj.bias\", \"module.backbone.blocks.11.attn.proj.weight\", \"module.backbone.blocks.11.attn.proj.bias\", \"module.backbone.blocks.11.norm2.weight\", \"module.backbone.blocks.11.norm2.bias\", \"module.backbone.blocks.11.fc1.weight\", \"module.backbone.blocks.11.fc1.bias\", \"module.backbone.blocks.11.fc2.weight\", \"module.backbone.blocks.11.fc2.bias\", \"module.backbone.blocks.11.adaptmlp.down_proj.weight\", \"module.backbone.blocks.11.adaptmlp.down_proj.bias\", \"module.backbone.blocks.11.adaptmlp.up_proj.weight\", \"module.backbone.blocks.11.adaptmlp.up_proj.bias\", \"module.backbone.norm.weight\", \"module.backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". "
     ]
    }
   ],
   "source": [
    "m = nn.DataParallel(model._network)\n",
    "m.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfb20f6e-90cb-42ac-bcff-12a4a2931b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SimpleVitNet:\n\tMissing key(s) in state_dict: \"backbone.cls_token\", \"backbone.pos_embed\", \"backbone.patch_embed.proj.weight\", \"backbone.patch_embed.proj.bias\", \"backbone.blocks.0.norm1.weight\", \"backbone.blocks.0.norm1.bias\", \"backbone.blocks.0.attn.q_proj.weight\", \"backbone.blocks.0.attn.q_proj.bias\", \"backbone.blocks.0.attn.v_proj.weight\", \"backbone.blocks.0.attn.v_proj.bias\", \"backbone.blocks.0.attn.k_proj.weight\", \"backbone.blocks.0.attn.k_proj.bias\", \"backbone.blocks.0.attn.proj.weight\", \"backbone.blocks.0.attn.proj.bias\", \"backbone.blocks.0.norm2.weight\", \"backbone.blocks.0.norm2.bias\", \"backbone.blocks.0.fc1.weight\", \"backbone.blocks.0.fc1.bias\", \"backbone.blocks.0.fc2.weight\", \"backbone.blocks.0.fc2.bias\", \"backbone.blocks.0.adaptmlp.down_proj.weight\", \"backbone.blocks.0.adaptmlp.down_proj.bias\", \"backbone.blocks.0.adaptmlp.up_proj.weight\", \"backbone.blocks.0.adaptmlp.up_proj.bias\", \"backbone.blocks.1.norm1.weight\", \"backbone.blocks.1.norm1.bias\", \"backbone.blocks.1.attn.q_proj.weight\", \"backbone.blocks.1.attn.q_proj.bias\", \"backbone.blocks.1.attn.v_proj.weight\", \"backbone.blocks.1.attn.v_proj.bias\", \"backbone.blocks.1.attn.k_proj.weight\", \"backbone.blocks.1.attn.k_proj.bias\", \"backbone.blocks.1.attn.proj.weight\", \"backbone.blocks.1.attn.proj.bias\", \"backbone.blocks.1.norm2.weight\", \"backbone.blocks.1.norm2.bias\", \"backbone.blocks.1.fc1.weight\", \"backbone.blocks.1.fc1.bias\", \"backbone.blocks.1.fc2.weight\", \"backbone.blocks.1.fc2.bias\", \"backbone.blocks.1.adaptmlp.down_proj.weight\", \"backbone.blocks.1.adaptmlp.down_proj.bias\", \"backbone.blocks.1.adaptmlp.up_proj.weight\", \"backbone.blocks.1.adaptmlp.up_proj.bias\", \"backbone.blocks.2.norm1.weight\", \"backbone.blocks.2.norm1.bias\", \"backbone.blocks.2.attn.q_proj.weight\", \"backbone.blocks.2.attn.q_proj.bias\", \"backbone.blocks.2.attn.v_proj.weight\", \"backbone.blocks.2.attn.v_proj.bias\", \"backbone.blocks.2.attn.k_proj.weight\", \"backbone.blocks.2.attn.k_proj.bias\", \"backbone.blocks.2.attn.proj.weight\", \"backbone.blocks.2.attn.proj.bias\", \"backbone.blocks.2.norm2.weight\", \"backbone.blocks.2.norm2.bias\", \"backbone.blocks.2.fc1.weight\", \"backbone.blocks.2.fc1.bias\", \"backbone.blocks.2.fc2.weight\", \"backbone.blocks.2.fc2.bias\", \"backbone.blocks.2.adaptmlp.down_proj.weight\", \"backbone.blocks.2.adaptmlp.down_proj.bias\", \"backbone.blocks.2.adaptmlp.up_proj.weight\", \"backbone.blocks.2.adaptmlp.up_proj.bias\", \"backbone.blocks.3.norm1.weight\", \"backbone.blocks.3.norm1.bias\", \"backbone.blocks.3.attn.q_proj.weight\", \"backbone.blocks.3.attn.q_proj.bias\", \"backbone.blocks.3.attn.v_proj.weight\", \"backbone.blocks.3.attn.v_proj.bias\", \"backbone.blocks.3.attn.k_proj.weight\", \"backbone.blocks.3.attn.k_proj.bias\", \"backbone.blocks.3.attn.proj.weight\", \"backbone.blocks.3.attn.proj.bias\", \"backbone.blocks.3.norm2.weight\", \"backbone.blocks.3.norm2.bias\", \"backbone.blocks.3.fc1.weight\", \"backbone.blocks.3.fc1.bias\", \"backbone.blocks.3.fc2.weight\", \"backbone.blocks.3.fc2.bias\", \"backbone.blocks.3.adaptmlp.down_proj.weight\", \"backbone.blocks.3.adaptmlp.down_proj.bias\", \"backbone.blocks.3.adaptmlp.up_proj.weight\", \"backbone.blocks.3.adaptmlp.up_proj.bias\", \"backbone.blocks.4.norm1.weight\", \"backbone.blocks.4.norm1.bias\", \"backbone.blocks.4.attn.q_proj.weight\", \"backbone.blocks.4.attn.q_proj.bias\", \"backbone.blocks.4.attn.v_proj.weight\", \"backbone.blocks.4.attn.v_proj.bias\", \"backbone.blocks.4.attn.k_proj.weight\", \"backbone.blocks.4.attn.k_proj.bias\", \"backbone.blocks.4.attn.proj.weight\", \"backbone.blocks.4.attn.proj.bias\", \"backbone.blocks.4.norm2.weight\", \"backbone.blocks.4.norm2.bias\", \"backbone.blocks.4.fc1.weight\", \"backbone.blocks.4.fc1.bias\", \"backbone.blocks.4.fc2.weight\", \"backbone.blocks.4.fc2.bias\", \"backbone.blocks.4.adaptmlp.down_proj.weight\", \"backbone.blocks.4.adaptmlp.down_proj.bias\", \"backbone.blocks.4.adaptmlp.up_proj.weight\", \"backbone.blocks.4.adaptmlp.up_proj.bias\", \"backbone.blocks.5.norm1.weight\", \"backbone.blocks.5.norm1.bias\", \"backbone.blocks.5.attn.q_proj.weight\", \"backbone.blocks.5.attn.q_proj.bias\", \"backbone.blocks.5.attn.v_proj.weight\", \"backbone.blocks.5.attn.v_proj.bias\", \"backbone.blocks.5.attn.k_proj.weight\", \"backbone.blocks.5.attn.k_proj.bias\", \"backbone.blocks.5.attn.proj.weight\", \"backbone.blocks.5.attn.proj.bias\", \"backbone.blocks.5.norm2.weight\", \"backbone.blocks.5.norm2.bias\", \"backbone.blocks.5.fc1.weight\", \"backbone.blocks.5.fc1.bias\", \"backbone.blocks.5.fc2.weight\", \"backbone.blocks.5.fc2.bias\", \"backbone.blocks.5.adaptmlp.down_proj.weight\", \"backbone.blocks.5.adaptmlp.down_proj.bias\", \"backbone.blocks.5.adaptmlp.up_proj.weight\", \"backbone.blocks.5.adaptmlp.up_proj.bias\", \"backbone.blocks.6.norm1.weight\", \"backbone.blocks.6.norm1.bias\", \"backbone.blocks.6.attn.q_proj.weight\", \"backbone.blocks.6.attn.q_proj.bias\", \"backbone.blocks.6.attn.v_proj.weight\", \"backbone.blocks.6.attn.v_proj.bias\", \"backbone.blocks.6.attn.k_proj.weight\", \"backbone.blocks.6.attn.k_proj.bias\", \"backbone.blocks.6.attn.proj.weight\", \"backbone.blocks.6.attn.proj.bias\", \"backbone.blocks.6.norm2.weight\", \"backbone.blocks.6.norm2.bias\", \"backbone.blocks.6.fc1.weight\", \"backbone.blocks.6.fc1.bias\", \"backbone.blocks.6.fc2.weight\", \"backbone.blocks.6.fc2.bias\", \"backbone.blocks.6.adaptmlp.down_proj.weight\", \"backbone.blocks.6.adaptmlp.down_proj.bias\", \"backbone.blocks.6.adaptmlp.up_proj.weight\", \"backbone.blocks.6.adaptmlp.up_proj.bias\", \"backbone.blocks.7.norm1.weight\", \"backbone.blocks.7.norm1.bias\", \"backbone.blocks.7.attn.q_proj.weight\", \"backbone.blocks.7.attn.q_proj.bias\", \"backbone.blocks.7.attn.v_proj.weight\", \"backbone.blocks.7.attn.v_proj.bias\", \"backbone.blocks.7.attn.k_proj.weight\", \"backbone.blocks.7.attn.k_proj.bias\", \"backbone.blocks.7.attn.proj.weight\", \"backbone.blocks.7.attn.proj.bias\", \"backbone.blocks.7.norm2.weight\", \"backbone.blocks.7.norm2.bias\", \"backbone.blocks.7.fc1.weight\", \"backbone.blocks.7.fc1.bias\", \"backbone.blocks.7.fc2.weight\", \"backbone.blocks.7.fc2.bias\", \"backbone.blocks.7.adaptmlp.down_proj.weight\", \"backbone.blocks.7.adaptmlp.down_proj.bias\", \"backbone.blocks.7.adaptmlp.up_proj.weight\", \"backbone.blocks.7.adaptmlp.up_proj.bias\", \"backbone.blocks.8.norm1.weight\", \"backbone.blocks.8.norm1.bias\", \"backbone.blocks.8.attn.q_proj.weight\", \"backbone.blocks.8.attn.q_proj.bias\", \"backbone.blocks.8.attn.v_proj.weight\", \"backbone.blocks.8.attn.v_proj.bias\", \"backbone.blocks.8.attn.k_proj.weight\", \"backbone.blocks.8.attn.k_proj.bias\", \"backbone.blocks.8.attn.proj.weight\", \"backbone.blocks.8.attn.proj.bias\", \"backbone.blocks.8.norm2.weight\", \"backbone.blocks.8.norm2.bias\", \"backbone.blocks.8.fc1.weight\", \"backbone.blocks.8.fc1.bias\", \"backbone.blocks.8.fc2.weight\", \"backbone.blocks.8.fc2.bias\", \"backbone.blocks.8.adaptmlp.down_proj.weight\", \"backbone.blocks.8.adaptmlp.down_proj.bias\", \"backbone.blocks.8.adaptmlp.up_proj.weight\", \"backbone.blocks.8.adaptmlp.up_proj.bias\", \"backbone.blocks.9.norm1.weight\", \"backbone.blocks.9.norm1.bias\", \"backbone.blocks.9.attn.q_proj.weight\", \"backbone.blocks.9.attn.q_proj.bias\", \"backbone.blocks.9.attn.v_proj.weight\", \"backbone.blocks.9.attn.v_proj.bias\", \"backbone.blocks.9.attn.k_proj.weight\", \"backbone.blocks.9.attn.k_proj.bias\", \"backbone.blocks.9.attn.proj.weight\", \"backbone.blocks.9.attn.proj.bias\", \"backbone.blocks.9.norm2.weight\", \"backbone.blocks.9.norm2.bias\", \"backbone.blocks.9.fc1.weight\", \"backbone.blocks.9.fc1.bias\", \"backbone.blocks.9.fc2.weight\", \"backbone.blocks.9.fc2.bias\", \"backbone.blocks.9.adaptmlp.down_proj.weight\", \"backbone.blocks.9.adaptmlp.down_proj.bias\", \"backbone.blocks.9.adaptmlp.up_proj.weight\", \"backbone.blocks.9.adaptmlp.up_proj.bias\", \"backbone.blocks.10.norm1.weight\", \"backbone.blocks.10.norm1.bias\", \"backbone.blocks.10.attn.q_proj.weight\", \"backbone.blocks.10.attn.q_proj.bias\", \"backbone.blocks.10.attn.v_proj.weight\", \"backbone.blocks.10.attn.v_proj.bias\", \"backbone.blocks.10.attn.k_proj.weight\", \"backbone.blocks.10.attn.k_proj.bias\", \"backbone.blocks.10.attn.proj.weight\", \"backbone.blocks.10.attn.proj.bias\", \"backbone.blocks.10.norm2.weight\", \"backbone.blocks.10.norm2.bias\", \"backbone.blocks.10.fc1.weight\", \"backbone.blocks.10.fc1.bias\", \"backbone.blocks.10.fc2.weight\", \"backbone.blocks.10.fc2.bias\", \"backbone.blocks.10.adaptmlp.down_proj.weight\", \"backbone.blocks.10.adaptmlp.down_proj.bias\", \"backbone.blocks.10.adaptmlp.up_proj.weight\", \"backbone.blocks.10.adaptmlp.up_proj.bias\", \"backbone.blocks.11.norm1.weight\", \"backbone.blocks.11.norm1.bias\", \"backbone.blocks.11.attn.q_proj.weight\", \"backbone.blocks.11.attn.q_proj.bias\", \"backbone.blocks.11.attn.v_proj.weight\", \"backbone.blocks.11.attn.v_proj.bias\", \"backbone.blocks.11.attn.k_proj.weight\", \"backbone.blocks.11.attn.k_proj.bias\", \"backbone.blocks.11.attn.proj.weight\", \"backbone.blocks.11.attn.proj.bias\", \"backbone.blocks.11.norm2.weight\", \"backbone.blocks.11.norm2.bias\", \"backbone.blocks.11.fc1.weight\", \"backbone.blocks.11.fc1.bias\", \"backbone.blocks.11.fc2.weight\", \"backbone.blocks.11.fc2.bias\", \"backbone.blocks.11.adaptmlp.down_proj.weight\", \"backbone.blocks.11.adaptmlp.down_proj.bias\", \"backbone.blocks.11.adaptmlp.up_proj.weight\", \"backbone.blocks.11.adaptmlp.up_proj.bias\", \"backbone.norm.weight\", \"backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SimpleVitNet:\n\tMissing key(s) in state_dict: \"backbone.cls_token\", \"backbone.pos_embed\", \"backbone.patch_embed.proj.weight\", \"backbone.patch_embed.proj.bias\", \"backbone.blocks.0.norm1.weight\", \"backbone.blocks.0.norm1.bias\", \"backbone.blocks.0.attn.q_proj.weight\", \"backbone.blocks.0.attn.q_proj.bias\", \"backbone.blocks.0.attn.v_proj.weight\", \"backbone.blocks.0.attn.v_proj.bias\", \"backbone.blocks.0.attn.k_proj.weight\", \"backbone.blocks.0.attn.k_proj.bias\", \"backbone.blocks.0.attn.proj.weight\", \"backbone.blocks.0.attn.proj.bias\", \"backbone.blocks.0.norm2.weight\", \"backbone.blocks.0.norm2.bias\", \"backbone.blocks.0.fc1.weight\", \"backbone.blocks.0.fc1.bias\", \"backbone.blocks.0.fc2.weight\", \"backbone.blocks.0.fc2.bias\", \"backbone.blocks.0.adaptmlp.down_proj.weight\", \"backbone.blocks.0.adaptmlp.down_proj.bias\", \"backbone.blocks.0.adaptmlp.up_proj.weight\", \"backbone.blocks.0.adaptmlp.up_proj.bias\", \"backbone.blocks.1.norm1.weight\", \"backbone.blocks.1.norm1.bias\", \"backbone.blocks.1.attn.q_proj.weight\", \"backbone.blocks.1.attn.q_proj.bias\", \"backbone.blocks.1.attn.v_proj.weight\", \"backbone.blocks.1.attn.v_proj.bias\", \"backbone.blocks.1.attn.k_proj.weight\", \"backbone.blocks.1.attn.k_proj.bias\", \"backbone.blocks.1.attn.proj.weight\", \"backbone.blocks.1.attn.proj.bias\", \"backbone.blocks.1.norm2.weight\", \"backbone.blocks.1.norm2.bias\", \"backbone.blocks.1.fc1.weight\", \"backbone.blocks.1.fc1.bias\", \"backbone.blocks.1.fc2.weight\", \"backbone.blocks.1.fc2.bias\", \"backbone.blocks.1.adaptmlp.down_proj.weight\", \"backbone.blocks.1.adaptmlp.down_proj.bias\", \"backbone.blocks.1.adaptmlp.up_proj.weight\", \"backbone.blocks.1.adaptmlp.up_proj.bias\", \"backbone.blocks.2.norm1.weight\", \"backbone.blocks.2.norm1.bias\", \"backbone.blocks.2.attn.q_proj.weight\", \"backbone.blocks.2.attn.q_proj.bias\", \"backbone.blocks.2.attn.v_proj.weight\", \"backbone.blocks.2.attn.v_proj.bias\", \"backbone.blocks.2.attn.k_proj.weight\", \"backbone.blocks.2.attn.k_proj.bias\", \"backbone.blocks.2.attn.proj.weight\", \"backbone.blocks.2.attn.proj.bias\", \"backbone.blocks.2.norm2.weight\", \"backbone.blocks.2.norm2.bias\", \"backbone.blocks.2.fc1.weight\", \"backbone.blocks.2.fc1.bias\", \"backbone.blocks.2.fc2.weight\", \"backbone.blocks.2.fc2.bias\", \"backbone.blocks.2.adaptmlp.down_proj.weight\", \"backbone.blocks.2.adaptmlp.down_proj.bias\", \"backbone.blocks.2.adaptmlp.up_proj.weight\", \"backbone.blocks.2.adaptmlp.up_proj.bias\", \"backbone.blocks.3.norm1.weight\", \"backbone.blocks.3.norm1.bias\", \"backbone.blocks.3.attn.q_proj.weight\", \"backbone.blocks.3.attn.q_proj.bias\", \"backbone.blocks.3.attn.v_proj.weight\", \"backbone.blocks.3.attn.v_proj.bias\", \"backbone.blocks.3.attn.k_proj.weight\", \"backbone.blocks.3.attn.k_proj.bias\", \"backbone.blocks.3.attn.proj.weight\", \"backbone.blocks.3.attn.proj.bias\", \"backbone.blocks.3.norm2.weight\", \"backbone.blocks.3.norm2.bias\", \"backbone.blocks.3.fc1.weight\", \"backbone.blocks.3.fc1.bias\", \"backbone.blocks.3.fc2.weight\", \"backbone.blocks.3.fc2.bias\", \"backbone.blocks.3.adaptmlp.down_proj.weight\", \"backbone.blocks.3.adaptmlp.down_proj.bias\", \"backbone.blocks.3.adaptmlp.up_proj.weight\", \"backbone.blocks.3.adaptmlp.up_proj.bias\", \"backbone.blocks.4.norm1.weight\", \"backbone.blocks.4.norm1.bias\", \"backbone.blocks.4.attn.q_proj.weight\", \"backbone.blocks.4.attn.q_proj.bias\", \"backbone.blocks.4.attn.v_proj.weight\", \"backbone.blocks.4.attn.v_proj.bias\", \"backbone.blocks.4.attn.k_proj.weight\", \"backbone.blocks.4.attn.k_proj.bias\", \"backbone.blocks.4.attn.proj.weight\", \"backbone.blocks.4.attn.proj.bias\", \"backbone.blocks.4.norm2.weight\", \"backbone.blocks.4.norm2.bias\", \"backbone.blocks.4.fc1.weight\", \"backbone.blocks.4.fc1.bias\", \"backbone.blocks.4.fc2.weight\", \"backbone.blocks.4.fc2.bias\", \"backbone.blocks.4.adaptmlp.down_proj.weight\", \"backbone.blocks.4.adaptmlp.down_proj.bias\", \"backbone.blocks.4.adaptmlp.up_proj.weight\", \"backbone.blocks.4.adaptmlp.up_proj.bias\", \"backbone.blocks.5.norm1.weight\", \"backbone.blocks.5.norm1.bias\", \"backbone.blocks.5.attn.q_proj.weight\", \"backbone.blocks.5.attn.q_proj.bias\", \"backbone.blocks.5.attn.v_proj.weight\", \"backbone.blocks.5.attn.v_proj.bias\", \"backbone.blocks.5.attn.k_proj.weight\", \"backbone.blocks.5.attn.k_proj.bias\", \"backbone.blocks.5.attn.proj.weight\", \"backbone.blocks.5.attn.proj.bias\", \"backbone.blocks.5.norm2.weight\", \"backbone.blocks.5.norm2.bias\", \"backbone.blocks.5.fc1.weight\", \"backbone.blocks.5.fc1.bias\", \"backbone.blocks.5.fc2.weight\", \"backbone.blocks.5.fc2.bias\", \"backbone.blocks.5.adaptmlp.down_proj.weight\", \"backbone.blocks.5.adaptmlp.down_proj.bias\", \"backbone.blocks.5.adaptmlp.up_proj.weight\", \"backbone.blocks.5.adaptmlp.up_proj.bias\", \"backbone.blocks.6.norm1.weight\", \"backbone.blocks.6.norm1.bias\", \"backbone.blocks.6.attn.q_proj.weight\", \"backbone.blocks.6.attn.q_proj.bias\", \"backbone.blocks.6.attn.v_proj.weight\", \"backbone.blocks.6.attn.v_proj.bias\", \"backbone.blocks.6.attn.k_proj.weight\", \"backbone.blocks.6.attn.k_proj.bias\", \"backbone.blocks.6.attn.proj.weight\", \"backbone.blocks.6.attn.proj.bias\", \"backbone.blocks.6.norm2.weight\", \"backbone.blocks.6.norm2.bias\", \"backbone.blocks.6.fc1.weight\", \"backbone.blocks.6.fc1.bias\", \"backbone.blocks.6.fc2.weight\", \"backbone.blocks.6.fc2.bias\", \"backbone.blocks.6.adaptmlp.down_proj.weight\", \"backbone.blocks.6.adaptmlp.down_proj.bias\", \"backbone.blocks.6.adaptmlp.up_proj.weight\", \"backbone.blocks.6.adaptmlp.up_proj.bias\", \"backbone.blocks.7.norm1.weight\", \"backbone.blocks.7.norm1.bias\", \"backbone.blocks.7.attn.q_proj.weight\", \"backbone.blocks.7.attn.q_proj.bias\", \"backbone.blocks.7.attn.v_proj.weight\", \"backbone.blocks.7.attn.v_proj.bias\", \"backbone.blocks.7.attn.k_proj.weight\", \"backbone.blocks.7.attn.k_proj.bias\", \"backbone.blocks.7.attn.proj.weight\", \"backbone.blocks.7.attn.proj.bias\", \"backbone.blocks.7.norm2.weight\", \"backbone.blocks.7.norm2.bias\", \"backbone.blocks.7.fc1.weight\", \"backbone.blocks.7.fc1.bias\", \"backbone.blocks.7.fc2.weight\", \"backbone.blocks.7.fc2.bias\", \"backbone.blocks.7.adaptmlp.down_proj.weight\", \"backbone.blocks.7.adaptmlp.down_proj.bias\", \"backbone.blocks.7.adaptmlp.up_proj.weight\", \"backbone.blocks.7.adaptmlp.up_proj.bias\", \"backbone.blocks.8.norm1.weight\", \"backbone.blocks.8.norm1.bias\", \"backbone.blocks.8.attn.q_proj.weight\", \"backbone.blocks.8.attn.q_proj.bias\", \"backbone.blocks.8.attn.v_proj.weight\", \"backbone.blocks.8.attn.v_proj.bias\", \"backbone.blocks.8.attn.k_proj.weight\", \"backbone.blocks.8.attn.k_proj.bias\", \"backbone.blocks.8.attn.proj.weight\", \"backbone.blocks.8.attn.proj.bias\", \"backbone.blocks.8.norm2.weight\", \"backbone.blocks.8.norm2.bias\", \"backbone.blocks.8.fc1.weight\", \"backbone.blocks.8.fc1.bias\", \"backbone.blocks.8.fc2.weight\", \"backbone.blocks.8.fc2.bias\", \"backbone.blocks.8.adaptmlp.down_proj.weight\", \"backbone.blocks.8.adaptmlp.down_proj.bias\", \"backbone.blocks.8.adaptmlp.up_proj.weight\", \"backbone.blocks.8.adaptmlp.up_proj.bias\", \"backbone.blocks.9.norm1.weight\", \"backbone.blocks.9.norm1.bias\", \"backbone.blocks.9.attn.q_proj.weight\", \"backbone.blocks.9.attn.q_proj.bias\", \"backbone.blocks.9.attn.v_proj.weight\", \"backbone.blocks.9.attn.v_proj.bias\", \"backbone.blocks.9.attn.k_proj.weight\", \"backbone.blocks.9.attn.k_proj.bias\", \"backbone.blocks.9.attn.proj.weight\", \"backbone.blocks.9.attn.proj.bias\", \"backbone.blocks.9.norm2.weight\", \"backbone.blocks.9.norm2.bias\", \"backbone.blocks.9.fc1.weight\", \"backbone.blocks.9.fc1.bias\", \"backbone.blocks.9.fc2.weight\", \"backbone.blocks.9.fc2.bias\", \"backbone.blocks.9.adaptmlp.down_proj.weight\", \"backbone.blocks.9.adaptmlp.down_proj.bias\", \"backbone.blocks.9.adaptmlp.up_proj.weight\", \"backbone.blocks.9.adaptmlp.up_proj.bias\", \"backbone.blocks.10.norm1.weight\", \"backbone.blocks.10.norm1.bias\", \"backbone.blocks.10.attn.q_proj.weight\", \"backbone.blocks.10.attn.q_proj.bias\", \"backbone.blocks.10.attn.v_proj.weight\", \"backbone.blocks.10.attn.v_proj.bias\", \"backbone.blocks.10.attn.k_proj.weight\", \"backbone.blocks.10.attn.k_proj.bias\", \"backbone.blocks.10.attn.proj.weight\", \"backbone.blocks.10.attn.proj.bias\", \"backbone.blocks.10.norm2.weight\", \"backbone.blocks.10.norm2.bias\", \"backbone.blocks.10.fc1.weight\", \"backbone.blocks.10.fc1.bias\", \"backbone.blocks.10.fc2.weight\", \"backbone.blocks.10.fc2.bias\", \"backbone.blocks.10.adaptmlp.down_proj.weight\", \"backbone.blocks.10.adaptmlp.down_proj.bias\", \"backbone.blocks.10.adaptmlp.up_proj.weight\", \"backbone.blocks.10.adaptmlp.up_proj.bias\", \"backbone.blocks.11.norm1.weight\", \"backbone.blocks.11.norm1.bias\", \"backbone.blocks.11.attn.q_proj.weight\", \"backbone.blocks.11.attn.q_proj.bias\", \"backbone.blocks.11.attn.v_proj.weight\", \"backbone.blocks.11.attn.v_proj.bias\", \"backbone.blocks.11.attn.k_proj.weight\", \"backbone.blocks.11.attn.k_proj.bias\", \"backbone.blocks.11.attn.proj.weight\", \"backbone.blocks.11.attn.proj.bias\", \"backbone.blocks.11.norm2.weight\", \"backbone.blocks.11.norm2.bias\", \"backbone.blocks.11.fc1.weight\", \"backbone.blocks.11.fc1.bias\", \"backbone.blocks.11.fc2.weight\", \"backbone.blocks.11.fc2.bias\", \"backbone.blocks.11.adaptmlp.down_proj.weight\", \"backbone.blocks.11.adaptmlp.down_proj.bias\", \"backbone.blocks.11.adaptmlp.up_proj.weight\", \"backbone.blocks.11.adaptmlp.up_proj.bias\", \"backbone.norm.weight\", \"backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f31b4fee-74e3-47ba-956f-eb9c4a93f90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2844ca58-212c-4fe8-9410-9692ffffc972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(\n",
    "        args[\"dataset\"],\n",
    "        args[\"shuffle\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"init_cls\"],\n",
    "        args[\"increment\"],\n",
    "        args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33b1e98e-9e92-4fc7-b76d-9f6d5b46dbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network.to(\"cuda:1\")#args[\"device\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17575af8-0623-4638-a00b-bbcff677515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._cur_task += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "993a71d4-28b4-4b28-a916-5dd71800fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e63fb169-b17f-4afb-be44-d410eefc6da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._cur_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bed53522-8cff-4039-b4e8-6de1afc4b1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c319bf69-55b4-49a5-8e6a-a1f3e1b4c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "#total_classes = args[\"init_cls\"] + args[\"increment\"] * task\n",
    "test_dataset = data_manager.get_dataset(np.arange(0, model._total_classes), source=\"test\", mode=\"test\" )\n",
    "model.test_loader = DataLoader(test_dataset, batch_size=args[\"batch_size\"], shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "train_dataset = data_manager.get_dataset(np.arange(0, model._total_classes),source=\"train\", mode=\"train\")\n",
    "model.train_dataset=train_dataset\n",
    "model.data_manager=data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "621e46b9-5b68-460a-8e52-798eace0b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_for_protonet=data_manager.get_dataset(np.arange(0,model._total_classes),source=\"train\", mode=\"test\")\n",
    "model.train_loader_for_protonet = DataLoader(train_dataset_for_protonet, batch_size=args[\"batch_size\"], shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd337d02-bf12-4666-b8e7-1f738f12b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._known_classes = model._total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7650314b-e953-45a0-b3a6-e0b227480aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n",
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "pretrained_vit_b16_224\n"
     ]
    }
   ],
   "source": [
    "model.construct_dual_branch_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3703d121-24e1-411f-bebe-d7b4d62f79b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBranchCosineIncrementalNet(\n",
       "  (backbone): Identity()\n",
       "  (backbones): ModuleList(\n",
       "    (0): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (1): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (fc): CosineLinear()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replace_fc(model.train_loader_for_protonet, model._network, None)\n",
    "# sind die weights die gleichen -> dann fishy \n",
    "#wenn nicht dann sollte es passen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbeae1de-6f8f-479d-9ec5-d51e3475c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n",
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "pretrained_vit_b16_224\n"
     ]
    }
   ],
   "source": [
    "optimizer=optim.AdamW(model._network.parameters(), lr=model.init_lr, weight_decay=model.weight_decay)\n",
    "scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['tuned_epoch'], eta_min=model.min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75d6c8d-4347-4abb-9fb1-313377957c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._network.update_fc(model._total_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9893a92b-b2f7-4bfa-adcd-c7e567a6d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (_, inputs, targets) in enumerate(loader):\n",
    "        inputs = inputs.to(\"cuda:1\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)[\"logits\"]\n",
    "        predicts = torch.max(outputs, dim=1)[1]\n",
    "        correct += (predicts.cpu() == targets).sum()\n",
    "        total += len(targets)\n",
    "\n",
    "    return np.around(tensor2numpy(correct) * 100 / total, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fb4dd80-ec79-41de-9925-0dd3c8066eaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536])."
     ]
    }
   ],
   "source": [
    "model._network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "701790c1-d17f-43da-8d97-89706bda8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'top1': [98.2, 95.75, 94.17, 92.32, 90.5, 88.87, 88.59, 86.16, 85.64, 85.16], \n",
    "#'top5': [100.0, 99.35, 99.1, 98.95, 98.94, 98.5, 98.51, 98.21, 97.94, 97.62]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233c2e92-c1ed-48e2-8ca8-16fd881a6414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_compute_accuracy(model._network, model.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1abea2d8-1423-48d6-a49b-59d537efd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._cur_task = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa274928-d775-4cb9-bdee-0529eac6f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'grouped': {'total': 96.6, '00-09': 96.6, 'old': 0, 'new': 96.6},\n",
       "  'top1': 96.6,\n",
       "  'top5': 99.7},\n",
       " None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.test_loader = test_loader\n",
    "model.eval_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d55ed22a-a913-4ec5-ae81-49db4223b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import AutoAttack \n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from foolbox.attacks import LinfPGD\n",
    "import eagerpy as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3da15eb-7235-457d-8e10-917ffae3fef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "EstimatorError",
     "evalue": "AutoAttack requires an estimator derived from <class 'art.estimators.estimator.BaseEstimator'> and <class 'art.estimators.classification.classifier.ClassifierMixin'>, the provided classifier is an instance of <class 'utils.inc_net.MultiBranchCosineIncrementalNet'> and is derived from (<class 'utils.inc_net.BaseNet'>,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEstimatorError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m  \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(epsilons)):\n\u001b[1;32m     19\u001b[0m      \u001b[38;5;66;03m#attack = FastGradientMethod(estimator=model, eps=epsilons[j])\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m      attack \u001b[38;5;241m=\u001b[39m \u001b[43mAutoAttack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLinf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPDG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m      \u001b[38;5;66;03m#_images, _labels = ep.astensors(images, labels)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m      \u001b[38;5;66;03m#raw_advs, clipped_advs, success = attack(model._network, _images, _labels, epsilons=epsilons)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m      x_test_adv \u001b[38;5;241m=\u001b[39m attack\u001b[38;5;241m.\u001b[39mgenerate(x\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/attacks/evasion/auto_attack.py:96\u001b[0m, in \u001b[0;36mAutoAttack.__init__\u001b[0;34m(self, estimator, norm, eps, eps_step, attacks, batch_size, estimator_orig, targeted, parallel)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     estimator: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLASSIFIER_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     parallel: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    Create a :class:`.AutoAttack` instance.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    :param parallel: If True run attacks in parallel.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attacks:\n\u001b[1;32m     99\u001b[0m         attacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/attacks/attack.py:212\u001b[0m, in \u001b[0;36mEvasionAttack.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targeted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/attacks/attack.py:125\u001b[0m, in \u001b[0;36mAttack.__init__\u001b[0;34m(self, estimator, summary_writer)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator requirements have not been defined in `_estimator_requirements`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_estimator_valid(estimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator_requirements):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EstimatorError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_requirements, estimator)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator \u001b[38;5;241m=\u001b[39m estimator\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_writer_arg \u001b[38;5;241m=\u001b[39m summary_writer\n",
      "\u001b[0;31mEstimatorError\u001b[0m: AutoAttack requires an estimator derived from <class 'art.estimators.estimator.BaseEstimator'> and <class 'art.estimators.classification.classifier.ClassifierMixin'>, the provided classifier is an instance of <class 'utils.inc_net.MultiBranchCosineIncrementalNet'> and is derived from (<class 'utils.inc_net.BaseNet'>,)."
     ]
    }
   ],
   "source": [
    "epsilons = [0.01]#[0.001, 0.003, 0.005, 0.008, 0.01, 0.1]\n",
    "clean_acc = 0.0\n",
    "robust_acc = [0.0] * len(epsilons)\n",
    "attack_epochs = 5\n",
    "steps = [1, 5, 10, 30, 40, 50]\n",
    "attack = LinfPGD(steps=steps[0])\n",
    "\n",
    "for i, data in enumerate(model.test_loader, 0):\n",
    "\n",
    "    # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "    if i >= attack_epochs:\n",
    "        break\n",
    "\n",
    "    images, labels = data[0].to(args[\"device\"][0]), data[1].to(args[\"device\"][0])\n",
    "\n",
    "   # clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\n",
    "\n",
    "    for j in range(len(epsilons)):\n",
    "        #attack = FastGradientMethod(estimator=model._network, eps=epsilons[j])\n",
    "        attack = AutoAttack(model._network, norm='Linf', eps=epsilons[j], attacks=[\"PDG\"])\n",
    "        #_images, _labels = ep.astensors(images, labels)\n",
    "        #raw_advs, clipped_advs, success = attack(model._network, _images, _labels, epsilons=epsilons)\n",
    "    \n",
    "        x_test_adv = attack.generate(x=images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x_test_adv)[\"logits\"]\n",
    "        predicts = torch.max(outputs, dim=1)[1]\n",
    "        \n",
    "        \n",
    "        accuracy = np.sum(np.argmax(predicts.cpu(), axis=1) == np.argmax(labels, axis=1)) / len(labels)\n",
    "        \n",
    "        robust_acc[j] += accuracy / args.attack_epochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d924e1d-8e64-401b-a131-c7ad2f7a91bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiBranchCosineIncrementalNet' object has no attribute 'bounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\u001b[39;00m\n\u001b[1;32m     19\u001b[0m  _images, _labels \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensors(images, labels)\n\u001b[0;32m---> 20\u001b[0m  raw_advs, clipped_advs, success \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m  \u001b[38;5;66;03m#x_test_adv = attack.generate(x=images)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m  \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:257\u001b[0m, in \u001b[0;36mFixedEpsilonAttack.__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    254\u001b[0m x, restore_type \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensor_(inputs)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m--> 257\u001b[0m \u001b[43mverify_input_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m criterion \u001b[38;5;241m=\u001b[39m get_criterion(criterion)\n\u001b[1;32m    260\u001b[0m is_adversarial \u001b[38;5;241m=\u001b[39m get_is_adversarial(criterion, model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:498\u001b[0m, in \u001b[0;36mverify_input_bounds\u001b[0;34m(input, model)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_input_bounds\u001b[39m(\u001b[38;5;28minput\u001b[39m: ep\u001b[38;5;241m.\u001b[39mTensor, model: Model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# verify that input to the attack lies within model's input bounds\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[38;5;241m.\u001b[39mlower\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mupper\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiBranchCosineIncrementalNet' object has no attribute 'bounds'"
     ]
    }
   ],
   "source": [
    "epsilons = [0.01]#[0.001, 0.003, 0.005, 0.008, 0.01, 0.1]\n",
    "clean_acc = 0.0\n",
    "robust_acc = [0.0] * len(epsilons)\n",
    "attack_epochs = 5\n",
    "steps = [1, 5, 10, 30, 40, 50]\n",
    "attack = LinfPGD(steps=steps[0])\n",
    "\n",
    "for i, data in enumerate(model.test_loader, 0):\n",
    "\n",
    "    # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "    if i >= attack_epochs:\n",
    "        break\n",
    "\n",
    "    images, labels = data[0].to(args[\"device\"][0]), data[1].to(args[\"device\"][0])\n",
    "\n",
    "   # clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\n",
    "\n",
    "    \n",
    "    _images, _labels = ep.astensors(images, labels)\n",
    "    raw_advs, clipped_advs, success = attack(model._network, _images, _labels, epsilons=epsilons)\n",
    "\n",
    "    #x_test_adv = attack.generate(x=images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(raw_advs)[\"logits\"]\n",
    "    predicts = torch.max(outputs, dim=1)[1]\n",
    "    \n",
    "    \n",
    "    accuracy = np.sum(np.argmax(predicts.cpu(), axis=1) == np.argmax(labels, axis=1)) / len(labels)\n",
    "    \n",
    "    robust_acc[j] += accuracy / args.attack_epochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab5af9-8ca0-4fe8-80de-0af5025cc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "345df6a2-f4fe-458c-93ea-94f73f43ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"device\"][0] = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc8493e-33d2-440f-8fea-f54890efffd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"device\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596b5c27-e9e1-4e43-8c64-4f0ef58e5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.estimators.estimator import BaseEstimator\n",
    "from art.estimators.classification.classifier import ClassifierMixin\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "from foolbox.attacks import LinfPGD, FGSM, L2CarliniWagnerAttack\n",
    "from autoattack import AutoAttack\n",
    "import eagerpy as ep\n",
    "from timm.models import load_checkpoint, create_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98a7df3d-487c-4367-a10a-6526b0c37793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "After BaseNet initialization.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model = Learner(args)\n",
    "model._network.eval().to(args[\"device\"][0])\n",
    "data_manager = DataManager(\n",
    "    args[\"dataset\"],\n",
    "    args[\"shuffle\"],\n",
    "    args[\"seed\"],\n",
    "    args[\"init_cls\"],\n",
    "    args[\"increment\"],\n",
    "    args,\n",
    ")\n",
    "args[\"nb_classes\"] = data_manager.nb_classes # update args\n",
    "args[\"nb_tasks\"] = data_manager.nb_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68462730-686e-4b19-a03f-5a50af0be013",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "472e5ffc-39ae-4c9f-aae1-e0d0373aed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._cur_task += 1\n",
    "model._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)\n",
    "model._network.update_fc(model._total_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b34ed7f-b18b-40fd-853a-3ebab152f80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._cur_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71f965ba-80c7-493e-8924-44c23a966d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0f8ea-ce5b-42bb-8d93-f0fde43fdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.after_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c25ea0b-ef85-488c-9401-5f187976e8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): CosineLinear()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = data_manager.get_dataset(np.arange(model._known_classes, model._total_classes),source=\"train\", mode=\"train\", )\n",
    "model.train_dataset = train_dataset\n",
    "model.data_manager = data_manager\n",
    "model.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataset = data_manager.get_dataset(np.arange(0, model._total_classes), source=\"test\", mode=\"test\" )\n",
    "model.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "train_dataset_for_protonet = data_manager.get_dataset(np.arange(model._known_classes, model._total_classes),source=\"train\", mode=\"test\", )\n",
    "model.train_loader_for_protonet = DataLoader(train_dataset_for_protonet, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "#model._network.to(model._device)\n",
    "model.replace_fc(model.train_loader_for_protonet, model._network, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f080ee25-bb1d-483d-9f9a-590afd5fdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = model._eval_cnn(model.test_loader)\n",
    "cnn_accy = model._evaluate(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce18abed-a171-4e3a-bcd2-72f53b2290a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grouped': {'total': 93.8, '00-09': 93.8, 'old': 0, 'new': 93.8},\n",
       " 'top1': 93.8,\n",
       " 'top5': 99.8}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_accy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23f44e8f-2f38-4fd2-891d-bd88a6079a46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 35\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 3\u001b[0m, in \u001b[0;36mPretrainedClassifier.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_network\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Ensure the model is in evaluation mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_total_classes\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1718\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "\n",
    "class PretrainedClassifier(torch.nn.Module, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model):\n",
    "        self._model = model._network\n",
    "        self._model.eval()  # Ensure the model is in evaluation mode\n",
    "        self.num_classes = model._total_classes\n",
    "        self.input_size = (3, 224, 224)\n",
    "        self._model.to(args[\"device\"][0])\n",
    "        self._model.eval()\n",
    "    \n",
    "    def model(self, model):\n",
    "        self._model = model._network\n",
    "        \n",
    "    def input_shape(self):\n",
    "        return self.input_size\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        raise NotImplementedError(\"fit method not supported for pretrained models\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        x_tensor = torch.tensor(x)\n",
    "        inputs = x_tensor.to(model._device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)[\"logits\"]\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        x_tensor = torch.tensor(x)\n",
    "        inputs = x_tensor.to(model._device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)[\"logits\"]\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        return probabilities.detach().numpy()\n",
    "\n",
    "classifier = PretrainedClassifier(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d53921e-26ff-47b4-a335-cde301e8b759",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The input model must inherit from `nn.Module`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Wrap the classifier in a PyTorchClassifier\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This step is necessary to use ART's attack functionalities\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pytorch_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assuming input shape for your model\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnb_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_total_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust according to your model's input range\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/estimators/classification/pytorch.py:145\u001b[0m, in \u001b[0;36mPyTorchClassifier.__init__\u001b[0;34m(self, model, loss, input_shape, nb_classes, optimizer, use_amp, opt_level, loss_scale, channels_first, clip_values, preprocessing_defences, postprocessing_defences, preprocessing, device_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_rnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m((\u001b[38;5;28misinstance\u001b[39m(m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mRNNBase) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodules()))\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Get the internal layers\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_names: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layers\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Index of layer at which the class gradients should be calculated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/estimators/classification/pytorch.py:1202\u001b[0m, in \u001b[0;36mPyTorchClassifier._make_model_wrapper.<locals>.ModelWrapper.get_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m-> 1202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input model must inherit from `nn.Module`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1203\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferred \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m hidden layers on PyTorch classifier.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28mlen\u001b[39m(result),\n\u001b[1;32m   1206\u001b[0m )\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: The input model must inherit from `nn.Module`."
     ]
    }
   ],
   "source": [
    "# Wrap the classifier in a PyTorchClassifier\n",
    "# This step is necessary to use ART's attack functionalities\n",
    "pytorch_classifier = PyTorchClassifier(\n",
    "    model=classifier,\n",
    "    input_shape=(3, 224, 224),  # Assuming input shape for your model\n",
    "    nb_classes=model._total_classes,\n",
    "    clip_values=(0, 1),  # Adjust according to your model's input range\n",
    "    loss = nn.CrossEntropyLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51dddde3-bb1d-49ac-833a-a1c941472443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.inc_net import IncrementalNet,SimpleCosineIncrementalNet,SimpleVitNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2afcf7a6-f51b-4375-bdf8-3a45fc4caa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel(SimpleVitNet):\n",
    "    def __init__(self,args):\n",
    "        super().__init__(args,True)\n",
    "        \n",
    "        #self._cur_task += 1\n",
    "        #self._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)\n",
    "        self.update_fc(10)\n",
    "    def __call__(self, inputs):\n",
    "        return super().__call__(inputs)[\"logits\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19fd3cea-043d-46b0-bb23-3355dfa0f9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "After BaseNet initialization.\n"
     ]
    }
   ],
   "source": [
    "model = WrapperModel(args)\n",
    "torch.device(\"cuda:1\")\n",
    "model.to(\"cuda:1\")\n",
    "model.eval()\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "278f933e-ab70-4f53-a56b-4841de2cd5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13780"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.device(\"cuda:1\")\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3e35aaa-d8d8-4757-854e-f8530b7b56c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#labels = label.to(\"cuda:1\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m, in \u001b[0;36mWrapperModel.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/srv/storage/han22002/LAMDA-PILOT/utils/inc_net.py:515\u001b[0m, in \u001b[0;36mSimpleVitNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 515\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_rand \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(x \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_rand)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/timm/models/vision_transformer.py:704\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 704\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/timm/models/vision_transformer.py:681\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 681\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_embed(x)\n\u001b[1;32m    683\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_drop(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/timm/layers/patch_embed.py:87\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m     pad_w \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m W \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;241m0\u001b[39m, pad_w, \u001b[38;5;241m0\u001b[39m, pad_h))\n\u001b[0;32m---> 87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten:\n\u001b[1;32m     89\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NCHW -> NLC\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "train_dataset = data_manager.get_dataset(np.arange(0, 10),source=\"train\", mode=\"train\", )\n",
    "train_dataset = train_dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "for i, batch in enumerate(train_loader):\n",
    "    (_,data, label) = batch\n",
    "    images = data.to(\"cuda:1\")\n",
    "    #labels = label.to(\"cuda:1\")\n",
    "    print(model(images))\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5921393e-aed7-4c91-939d-e07c43b5a9f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown type: <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 26\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#images, labels = data[0].to(device), data[1].to(device)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#if step == steps[0]:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#    clean_acc += (get_acc(fmodel, images, labels)) / args.attack_epochs  # accumulate for attack epochs.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m _images, _labels \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensors(images, labels)\n\u001b[0;32m---> 26\u001b[0m raw_advs, clipped_advs, success \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m robust_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m success\u001b[38;5;241m.\u001b[39mfloat32()\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(robust_accuracy)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:283\u001b[0m, in \u001b[0;36mFixedEpsilonAttack.__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    281\u001b[0m success \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon \u001b[38;5;129;01min\u001b[39;00m real_epsilons:\n\u001b[0;32m--> 283\u001b[0m     xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# clip to epsilon because we don't really know what the attack returns;\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# alternatively, we could check if the perturbation is at most epsilon,\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# but then we would need to handle numerical violations;\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     xpc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mclip_perturbation(x, xp, epsilon)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/gradient_descent_base.py:155\u001b[0m, in \u001b[0;36mBaseGradientDescent.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    152\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m--> 155\u001b[0m     _, gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(gradients, x\u001b[38;5;241m=\u001b[39mx, bounds\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbounds)\n\u001b[1;32m    157\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m gradient_step_sign \u001b[38;5;241m*\u001b[39m optimizer(gradients)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/gradient_descent_base.py:111\u001b[0m, in \u001b[0;36mBaseGradientDescent.value_and_grad\u001b[0;34m(self, loss_fn, x)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_and_grad\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# can be overridden by users\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     loss_fn: Callable[[ep\u001b[38;5;241m.\u001b[39mTensor], ep\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    109\u001b[0m     x: ep\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ep\u001b[38;5;241m.\u001b[39mTensor, ep\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/eagerpy/framework.py:360\u001b[0m, in \u001b[0;36mvalue_and_grad\u001b[0;34m(f, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_and_grad\u001b[39m(\n\u001b[1;32m    358\u001b[0m     f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, TensorType], t: TensorType, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    359\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/eagerpy/tensor/tensor.py:553\u001b[0m, in \u001b[0;36mTensor.value_and_grad\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_and_grad\u001b[39m(\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mself\u001b[39m: TensorType, f: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, TensorType], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    552\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorType, TensorType]:\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value_and_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/eagerpy/tensor/pytorch.py:505\u001b[0m, in \u001b[0;36mPyTorchTensor._value_and_grad_fn.<locals>.value_and_grad\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     loss, aux \u001b[38;5;241m=\u001b[39m f(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mraw\n\u001b[1;32m    507\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/gradient_descent_base.py:96\u001b[0m, in \u001b[0;36mBaseGradientDescent.get_loss_fn.<locals>.loss_fn\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(inputs: ep\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ep\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 96\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ep\u001b[38;5;241m.\u001b[39mcrossentropy(logits, labels)\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/models/base.py:102\u001b[0m, in \u001b[0;36mModelWithPreprocessing.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    100\u001b[0m x, restore_type \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensor_(inputs)\n\u001b[1;32m    101\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(x)\n\u001b[0;32m--> 102\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restore_type(z)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/eagerpy/astensor.py:55\u001b[0m, in \u001b[0;36mastensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, m[name]\u001b[38;5;241m.\u001b[39mndarray):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NumPyTensor(x)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown type: <class 'dict'>"
     ]
    }
   ],
   "source": [
    "\n",
    "epsilons = [0.01]\n",
    "steps = [1]#, 5, 10, 30, 40, 50]\n",
    "attack_epochs = 1\n",
    "for step in steps:\n",
    "    # Adversarial attack.\n",
    "    attack = LinfPGD(steps=step)\n",
    "\n",
    "    clean_acc = 0.0\n",
    "    robust_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(model.train_loader):\n",
    "            (_,data, label) = batch\n",
    "            images = data.to(args[\"device\"][0])\n",
    "            labels = label.to(args[\"device\"][0])\n",
    "            \n",
    "            # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "            if i >= attack_epochs:\n",
    "                break\n",
    "    \n",
    "            #images, labels = data[0].to(device), data[1].to(device)\n",
    "            #if step == steps[0]:\n",
    "            #    clean_acc += (get_acc(fmodel, images, labels)) / args.attack_epochs  # accumulate for attack epochs.\n",
    "    \n",
    "            \n",
    "            _images, _labels = ep.astensors(images, labels)\n",
    "            raw_advs, clipped_advs, success = attack(fmodel, _images, _labels, epsilons=epsilons)\n",
    "    \n",
    "            robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "            print(robust_accuracy)\n",
    "            robust_acc += robust_accuracy / attack_epochs\n",
    "    \n",
    "            for eps, acc in zip(epsilons, robust_acc):\n",
    "                print(f\"  Step {step}, Linf norm  {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
    "            print('  -------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d4fa6-5a9b-46ca-98b1-d789a720a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
