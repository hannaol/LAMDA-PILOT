{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23cd435c-ef3e-4b43-b8e2-fcb787783631",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a658aa-85b8-4505-a357-a122062be893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.cifar.CIFAR100(\"./data\", train=True, download=False)\n",
    "test = datasets.cifar.CIFAR100(\"./data\", train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7162832-551b-45d8-bf15-9c4aadca6849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d03ef74-e1e4-45dc-8da6-e9668bc4eeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR100\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a27c1a-a7a2-4124-8044-e9b2d18126b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171997df-2440-4083-995a-8d398b5abee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.adam_adapter import Learner\n",
    "json = \"./exps/adam_adapter.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97ba423-5f48-412d-90b1-5102ee886499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simplecil import Learner\n",
    "json = \"./exps/simplecil.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9342b9a0-a398-4a43-9120-868427617410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_random(seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ef6495-4280-4943-8069-3fe0bf71308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_device(args):\n",
    "    device_type = args[\"device\"]\n",
    "    gpus = []\n",
    "\n",
    "    for device in device_type:\n",
    "        if device == -1:\n",
    "            device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(device))\n",
    "\n",
    "        gpus.append(device)\n",
    "\n",
    "    args[\"device\"] = gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca34349e-ab4d-46d0-a1bd-3c39a1ec4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.data_manager import DataManager\n",
    "from utils.toolkit import tensor2numpy, accuracy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126ae811-133e-4602-8a77-c3eb2188c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(setting_path):\n",
    "    import json\n",
    "    with open(setting_path) as data_file:\n",
    "        param = json.load(data_file)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "827a1705-525d-480f-8987-d7acad448388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = load_json(json)\n",
    "args[\"seed\"] = args[\"seed\"][0]\n",
    "#args[\"device\"] = args[\"device\"][0]\n",
    "_set_random(args[\"seed\"])\n",
    "#_set_device(args)\n",
    "args[\"device\"][0] = \"cuda:7\"\n",
    "torch.device(args[\"device\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57166167-c42c-4d68-9e5b-8ee9c605edf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prefix': ' ',\n",
       " 'dataset': 'cifar224',\n",
       " 'memory_size': 0,\n",
       " 'memory_per_class': 0,\n",
       " 'fixed_memory': False,\n",
       " 'shuffle': True,\n",
       " 'init_cls': 10,\n",
       " 'increment': 10,\n",
       " 'model_name': 'simplecil',\n",
       " 'backbone_type': 'pretrained_vit_b16_224',\n",
       " 'device': ['cuda:7'],\n",
       " 'seed': 1993,\n",
       " 'tuned_epoch': 0,\n",
       " 'init_lr': 0.01,\n",
       " 'batch_size': 256,\n",
       " 'weight_decay': 0.05,\n",
       " 'min_lr': 1e-08,\n",
       " 'optimizer': 'sgd',\n",
       " 'vpt_type': 'shallow',\n",
       " 'prompt_token_num': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb05f3c-6440-44d7-bb78-4c449e82be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./checkpoints/adam_adapter/task_1.pkl\"\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "\n",
    "task = checkpoint['tasks']\n",
    "args.update(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c9c420-3d5f-4dd3-b4e6-9c06d6eb4e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0312, -0.0190,  0.0140,  ...,  0.0216,  0.0246,  0.0246],\n",
       "        [ 0.0344, -0.0331,  0.0220,  ...,  0.0070, -0.0033, -0.0178],\n",
       "        [-0.0307,  0.0150, -0.0314,  ...,  0.0098,  0.0316,  0.0103],\n",
       "        ...,\n",
       "        [ 0.0143, -0.0060, -0.0096,  ...,  0.0142, -0.0134,  0.0114],\n",
       "        [-0.0041, -0.0273,  0.0061,  ...,  0.0227,  0.0149,  0.0047],\n",
       "        [ 0.0328,  0.0253,  0.0149,  ..., -0.0328,  0.0213, -0.0010]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"blocks.0.adaptmlp.down_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a84ce7f4-dfd7-4116-9fdb-8ba18b382b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.update(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96c0dc42-393e-472e-a075-b95e15f9b391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "After BaseNet initialization.\n"
     ]
    }
   ],
   "source": [
    "model = Learner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862a6d66-275e-464b-ae10-34d57c0987b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404506c6-2de6-48d7-89fc-42f1520262cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3afa1ed2-f4ed-4f0c-9392-35c4794e9c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n",
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "pretrained_vit_b16_224\n"
     ]
    }
   ],
   "source": [
    "model.construct_dual_branch_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5805b242-56a0-4a6e-a7c4-34ac68dbda2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict= checkpoint['model_state_dict']\n",
    "keys = state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8423d36-ac30-4798-84d0-5327ded29918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'backbones.1.blocks.0.adaptmlp.down_proj.weight' -> 'blocks.0.adaptmlp.down_proj.weight'\n",
    "for key in list(keys):\n",
    "    if 'backbones.1.blocks' in key:\n",
    "        entry = state_dict.pop(key)\n",
    "        state_dict[key.replace('backbones.1.blocks', 'blocks')] = entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c42132-43d4-4d9a-a26d-eccc7acb2d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbones.0.cls_token', 'backbones.0.pos_embed', 'backbones.0.patch_embed.proj.weight', 'backbones.0.patch_embed.proj.bias', 'backbones.0.blocks.0.norm1.weight', 'backbones.0.blocks.0.norm1.bias', 'backbones.0.blocks.0.attn.qkv.weight', 'backbones.0.blocks.0.attn.qkv.bias', 'backbones.0.blocks.0.attn.proj.weight', 'backbones.0.blocks.0.attn.proj.bias', 'backbones.0.blocks.0.norm2.weight', 'backbones.0.blocks.0.norm2.bias', 'backbones.0.blocks.0.mlp.fc1.weight', 'backbones.0.blocks.0.mlp.fc1.bias', 'backbones.0.blocks.0.mlp.fc2.weight', 'backbones.0.blocks.0.mlp.fc2.bias', 'backbones.0.blocks.1.norm1.weight', 'backbones.0.blocks.1.norm1.bias', 'backbones.0.blocks.1.attn.qkv.weight', 'backbones.0.blocks.1.attn.qkv.bias', 'backbones.0.blocks.1.attn.proj.weight', 'backbones.0.blocks.1.attn.proj.bias', 'backbones.0.blocks.1.norm2.weight', 'backbones.0.blocks.1.norm2.bias', 'backbones.0.blocks.1.mlp.fc1.weight', 'backbones.0.blocks.1.mlp.fc1.bias', 'backbones.0.blocks.1.mlp.fc2.weight', 'backbones.0.blocks.1.mlp.fc2.bias', 'backbones.0.blocks.2.norm1.weight', 'backbones.0.blocks.2.norm1.bias', 'backbones.0.blocks.2.attn.qkv.weight', 'backbones.0.blocks.2.attn.qkv.bias', 'backbones.0.blocks.2.attn.proj.weight', 'backbones.0.blocks.2.attn.proj.bias', 'backbones.0.blocks.2.norm2.weight', 'backbones.0.blocks.2.norm2.bias', 'backbones.0.blocks.2.mlp.fc1.weight', 'backbones.0.blocks.2.mlp.fc1.bias', 'backbones.0.blocks.2.mlp.fc2.weight', 'backbones.0.blocks.2.mlp.fc2.bias', 'backbones.0.blocks.3.norm1.weight', 'backbones.0.blocks.3.norm1.bias', 'backbones.0.blocks.3.attn.qkv.weight', 'backbones.0.blocks.3.attn.qkv.bias', 'backbones.0.blocks.3.attn.proj.weight', 'backbones.0.blocks.3.attn.proj.bias', 'backbones.0.blocks.3.norm2.weight', 'backbones.0.blocks.3.norm2.bias', 'backbones.0.blocks.3.mlp.fc1.weight', 'backbones.0.blocks.3.mlp.fc1.bias', 'backbones.0.blocks.3.mlp.fc2.weight', 'backbones.0.blocks.3.mlp.fc2.bias', 'backbones.0.blocks.4.norm1.weight', 'backbones.0.blocks.4.norm1.bias', 'backbones.0.blocks.4.attn.qkv.weight', 'backbones.0.blocks.4.attn.qkv.bias', 'backbones.0.blocks.4.attn.proj.weight', 'backbones.0.blocks.4.attn.proj.bias', 'backbones.0.blocks.4.norm2.weight', 'backbones.0.blocks.4.norm2.bias', 'backbones.0.blocks.4.mlp.fc1.weight', 'backbones.0.blocks.4.mlp.fc1.bias', 'backbones.0.blocks.4.mlp.fc2.weight', 'backbones.0.blocks.4.mlp.fc2.bias', 'backbones.0.blocks.5.norm1.weight', 'backbones.0.blocks.5.norm1.bias', 'backbones.0.blocks.5.attn.qkv.weight', 'backbones.0.blocks.5.attn.qkv.bias', 'backbones.0.blocks.5.attn.proj.weight', 'backbones.0.blocks.5.attn.proj.bias', 'backbones.0.blocks.5.norm2.weight', 'backbones.0.blocks.5.norm2.bias', 'backbones.0.blocks.5.mlp.fc1.weight', 'backbones.0.blocks.5.mlp.fc1.bias', 'backbones.0.blocks.5.mlp.fc2.weight', 'backbones.0.blocks.5.mlp.fc2.bias', 'backbones.0.blocks.6.norm1.weight', 'backbones.0.blocks.6.norm1.bias', 'backbones.0.blocks.6.attn.qkv.weight', 'backbones.0.blocks.6.attn.qkv.bias', 'backbones.0.blocks.6.attn.proj.weight', 'backbones.0.blocks.6.attn.proj.bias', 'backbones.0.blocks.6.norm2.weight', 'backbones.0.blocks.6.norm2.bias', 'backbones.0.blocks.6.mlp.fc1.weight', 'backbones.0.blocks.6.mlp.fc1.bias', 'backbones.0.blocks.6.mlp.fc2.weight', 'backbones.0.blocks.6.mlp.fc2.bias', 'backbones.0.blocks.7.norm1.weight', 'backbones.0.blocks.7.norm1.bias', 'backbones.0.blocks.7.attn.qkv.weight', 'backbones.0.blocks.7.attn.qkv.bias', 'backbones.0.blocks.7.attn.proj.weight', 'backbones.0.blocks.7.attn.proj.bias', 'backbones.0.blocks.7.norm2.weight', 'backbones.0.blocks.7.norm2.bias', 'backbones.0.blocks.7.mlp.fc1.weight', 'backbones.0.blocks.7.mlp.fc1.bias', 'backbones.0.blocks.7.mlp.fc2.weight', 'backbones.0.blocks.7.mlp.fc2.bias', 'backbones.0.blocks.8.norm1.weight', 'backbones.0.blocks.8.norm1.bias', 'backbones.0.blocks.8.attn.qkv.weight', 'backbones.0.blocks.8.attn.qkv.bias', 'backbones.0.blocks.8.attn.proj.weight', 'backbones.0.blocks.8.attn.proj.bias', 'backbones.0.blocks.8.norm2.weight', 'backbones.0.blocks.8.norm2.bias', 'backbones.0.blocks.8.mlp.fc1.weight', 'backbones.0.blocks.8.mlp.fc1.bias', 'backbones.0.blocks.8.mlp.fc2.weight', 'backbones.0.blocks.8.mlp.fc2.bias', 'backbones.0.blocks.9.norm1.weight', 'backbones.0.blocks.9.norm1.bias', 'backbones.0.blocks.9.attn.qkv.weight', 'backbones.0.blocks.9.attn.qkv.bias', 'backbones.0.blocks.9.attn.proj.weight', 'backbones.0.blocks.9.attn.proj.bias', 'backbones.0.blocks.9.norm2.weight', 'backbones.0.blocks.9.norm2.bias', 'backbones.0.blocks.9.mlp.fc1.weight', 'backbones.0.blocks.9.mlp.fc1.bias', 'backbones.0.blocks.9.mlp.fc2.weight', 'backbones.0.blocks.9.mlp.fc2.bias', 'backbones.0.blocks.10.norm1.weight', 'backbones.0.blocks.10.norm1.bias', 'backbones.0.blocks.10.attn.qkv.weight', 'backbones.0.blocks.10.attn.qkv.bias', 'backbones.0.blocks.10.attn.proj.weight', 'backbones.0.blocks.10.attn.proj.bias', 'backbones.0.blocks.10.norm2.weight', 'backbones.0.blocks.10.norm2.bias', 'backbones.0.blocks.10.mlp.fc1.weight', 'backbones.0.blocks.10.mlp.fc1.bias', 'backbones.0.blocks.10.mlp.fc2.weight', 'backbones.0.blocks.10.mlp.fc2.bias', 'backbones.0.blocks.11.norm1.weight', 'backbones.0.blocks.11.norm1.bias', 'backbones.0.blocks.11.attn.qkv.weight', 'backbones.0.blocks.11.attn.qkv.bias', 'backbones.0.blocks.11.attn.proj.weight', 'backbones.0.blocks.11.attn.proj.bias', 'backbones.0.blocks.11.norm2.weight', 'backbones.0.blocks.11.norm2.bias', 'backbones.0.blocks.11.mlp.fc1.weight', 'backbones.0.blocks.11.mlp.fc1.bias', 'backbones.0.blocks.11.mlp.fc2.weight', 'backbones.0.blocks.11.mlp.fc2.bias', 'backbones.0.norm.weight', 'backbones.0.norm.bias', 'backbones.1.cls_token', 'backbones.1.pos_embed', 'backbones.1.patch_embed.proj.weight', 'backbones.1.patch_embed.proj.bias', 'backbones.1.norm.weight', 'backbones.1.norm.bias', 'fc.weight', 'fc.sigma', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.q_proj.weight', 'blocks.0.attn.q_proj.bias', 'blocks.0.attn.v_proj.weight', 'blocks.0.attn.v_proj.bias', 'blocks.0.attn.k_proj.weight', 'blocks.0.attn.k_proj.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.fc1.weight', 'blocks.0.fc1.bias', 'blocks.0.fc2.weight', 'blocks.0.fc2.bias', 'blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.q_proj.weight', 'blocks.1.attn.q_proj.bias', 'blocks.1.attn.v_proj.weight', 'blocks.1.attn.v_proj.bias', 'blocks.1.attn.k_proj.weight', 'blocks.1.attn.k_proj.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.fc1.weight', 'blocks.1.fc1.bias', 'blocks.1.fc2.weight', 'blocks.1.fc2.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.q_proj.weight', 'blocks.2.attn.q_proj.bias', 'blocks.2.attn.v_proj.weight', 'blocks.2.attn.v_proj.bias', 'blocks.2.attn.k_proj.weight', 'blocks.2.attn.k_proj.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.fc1.weight', 'blocks.2.fc1.bias', 'blocks.2.fc2.weight', 'blocks.2.fc2.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.q_proj.weight', 'blocks.3.attn.q_proj.bias', 'blocks.3.attn.v_proj.weight', 'blocks.3.attn.v_proj.bias', 'blocks.3.attn.k_proj.weight', 'blocks.3.attn.k_proj.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.fc1.weight', 'blocks.3.fc1.bias', 'blocks.3.fc2.weight', 'blocks.3.fc2.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.q_proj.weight', 'blocks.4.attn.q_proj.bias', 'blocks.4.attn.v_proj.weight', 'blocks.4.attn.v_proj.bias', 'blocks.4.attn.k_proj.weight', 'blocks.4.attn.k_proj.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.fc1.weight', 'blocks.4.fc1.bias', 'blocks.4.fc2.weight', 'blocks.4.fc2.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.q_proj.weight', 'blocks.5.attn.q_proj.bias', 'blocks.5.attn.v_proj.weight', 'blocks.5.attn.v_proj.bias', 'blocks.5.attn.k_proj.weight', 'blocks.5.attn.k_proj.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.fc1.weight', 'blocks.5.fc1.bias', 'blocks.5.fc2.weight', 'blocks.5.fc2.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.q_proj.weight', 'blocks.6.attn.q_proj.bias', 'blocks.6.attn.v_proj.weight', 'blocks.6.attn.v_proj.bias', 'blocks.6.attn.k_proj.weight', 'blocks.6.attn.k_proj.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.fc1.weight', 'blocks.6.fc1.bias', 'blocks.6.fc2.weight', 'blocks.6.fc2.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.q_proj.weight', 'blocks.7.attn.q_proj.bias', 'blocks.7.attn.v_proj.weight', 'blocks.7.attn.v_proj.bias', 'blocks.7.attn.k_proj.weight', 'blocks.7.attn.k_proj.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.fc1.weight', 'blocks.7.fc1.bias', 'blocks.7.fc2.weight', 'blocks.7.fc2.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.q_proj.weight', 'blocks.8.attn.q_proj.bias', 'blocks.8.attn.v_proj.weight', 'blocks.8.attn.v_proj.bias', 'blocks.8.attn.k_proj.weight', 'blocks.8.attn.k_proj.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.fc1.weight', 'blocks.8.fc1.bias', 'blocks.8.fc2.weight', 'blocks.8.fc2.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.q_proj.weight', 'blocks.9.attn.q_proj.bias', 'blocks.9.attn.v_proj.weight', 'blocks.9.attn.v_proj.bias', 'blocks.9.attn.k_proj.weight', 'blocks.9.attn.k_proj.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.fc1.weight', 'blocks.9.fc1.bias', 'blocks.9.fc2.weight', 'blocks.9.fc2.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.q_proj.weight', 'blocks.10.attn.q_proj.bias', 'blocks.10.attn.v_proj.weight', 'blocks.10.attn.v_proj.bias', 'blocks.10.attn.k_proj.weight', 'blocks.10.attn.k_proj.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.fc1.weight', 'blocks.10.fc1.bias', 'blocks.10.fc2.weight', 'blocks.10.fc2.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.q_proj.weight', 'blocks.11.attn.q_proj.bias', 'blocks.11.attn.v_proj.weight', 'blocks.11.attn.v_proj.bias', 'blocks.11.attn.k_proj.weight', 'blocks.11.attn.k_proj.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.fc1.weight', 'blocks.11.fc1.bias', 'blocks.11.fc2.weight', 'blocks.11.fc2.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f21e370d-e89f-4040-8c6d-caaec7ba4f04",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536])."
     ]
    }
   ],
   "source": [
    "model._network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32050cb0-3763-467c-889e-cbbc26e3c56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbones.0.cls_token', 'backbones.0.pos_embed', 'backbones.0.patch_embed.proj.weight', 'backbones.0.patch_embed.proj.bias', 'backbones.0.blocks.0.norm1.weight', 'backbones.0.blocks.0.norm1.bias', 'backbones.0.blocks.0.attn.qkv.weight', 'backbones.0.blocks.0.attn.qkv.bias', 'backbones.0.blocks.0.attn.proj.weight', 'backbones.0.blocks.0.attn.proj.bias', 'backbones.0.blocks.0.norm2.weight', 'backbones.0.blocks.0.norm2.bias', 'backbones.0.blocks.0.mlp.fc1.weight', 'backbones.0.blocks.0.mlp.fc1.bias', 'backbones.0.blocks.0.mlp.fc2.weight', 'backbones.0.blocks.0.mlp.fc2.bias', 'backbones.0.blocks.1.norm1.weight', 'backbones.0.blocks.1.norm1.bias', 'backbones.0.blocks.1.attn.qkv.weight', 'backbones.0.blocks.1.attn.qkv.bias', 'backbones.0.blocks.1.attn.proj.weight', 'backbones.0.blocks.1.attn.proj.bias', 'backbones.0.blocks.1.norm2.weight', 'backbones.0.blocks.1.norm2.bias', 'backbones.0.blocks.1.mlp.fc1.weight', 'backbones.0.blocks.1.mlp.fc1.bias', 'backbones.0.blocks.1.mlp.fc2.weight', 'backbones.0.blocks.1.mlp.fc2.bias', 'backbones.0.blocks.2.norm1.weight', 'backbones.0.blocks.2.norm1.bias', 'backbones.0.blocks.2.attn.qkv.weight', 'backbones.0.blocks.2.attn.qkv.bias', 'backbones.0.blocks.2.attn.proj.weight', 'backbones.0.blocks.2.attn.proj.bias', 'backbones.0.blocks.2.norm2.weight', 'backbones.0.blocks.2.norm2.bias', 'backbones.0.blocks.2.mlp.fc1.weight', 'backbones.0.blocks.2.mlp.fc1.bias', 'backbones.0.blocks.2.mlp.fc2.weight', 'backbones.0.blocks.2.mlp.fc2.bias', 'backbones.0.blocks.3.norm1.weight', 'backbones.0.blocks.3.norm1.bias', 'backbones.0.blocks.3.attn.qkv.weight', 'backbones.0.blocks.3.attn.qkv.bias', 'backbones.0.blocks.3.attn.proj.weight', 'backbones.0.blocks.3.attn.proj.bias', 'backbones.0.blocks.3.norm2.weight', 'backbones.0.blocks.3.norm2.bias', 'backbones.0.blocks.3.mlp.fc1.weight', 'backbones.0.blocks.3.mlp.fc1.bias', 'backbones.0.blocks.3.mlp.fc2.weight', 'backbones.0.blocks.3.mlp.fc2.bias', 'backbones.0.blocks.4.norm1.weight', 'backbones.0.blocks.4.norm1.bias', 'backbones.0.blocks.4.attn.qkv.weight', 'backbones.0.blocks.4.attn.qkv.bias', 'backbones.0.blocks.4.attn.proj.weight', 'backbones.0.blocks.4.attn.proj.bias', 'backbones.0.blocks.4.norm2.weight', 'backbones.0.blocks.4.norm2.bias', 'backbones.0.blocks.4.mlp.fc1.weight', 'backbones.0.blocks.4.mlp.fc1.bias', 'backbones.0.blocks.4.mlp.fc2.weight', 'backbones.0.blocks.4.mlp.fc2.bias', 'backbones.0.blocks.5.norm1.weight', 'backbones.0.blocks.5.norm1.bias', 'backbones.0.blocks.5.attn.qkv.weight', 'backbones.0.blocks.5.attn.qkv.bias', 'backbones.0.blocks.5.attn.proj.weight', 'backbones.0.blocks.5.attn.proj.bias', 'backbones.0.blocks.5.norm2.weight', 'backbones.0.blocks.5.norm2.bias', 'backbones.0.blocks.5.mlp.fc1.weight', 'backbones.0.blocks.5.mlp.fc1.bias', 'backbones.0.blocks.5.mlp.fc2.weight', 'backbones.0.blocks.5.mlp.fc2.bias', 'backbones.0.blocks.6.norm1.weight', 'backbones.0.blocks.6.norm1.bias', 'backbones.0.blocks.6.attn.qkv.weight', 'backbones.0.blocks.6.attn.qkv.bias', 'backbones.0.blocks.6.attn.proj.weight', 'backbones.0.blocks.6.attn.proj.bias', 'backbones.0.blocks.6.norm2.weight', 'backbones.0.blocks.6.norm2.bias', 'backbones.0.blocks.6.mlp.fc1.weight', 'backbones.0.blocks.6.mlp.fc1.bias', 'backbones.0.blocks.6.mlp.fc2.weight', 'backbones.0.blocks.6.mlp.fc2.bias', 'backbones.0.blocks.7.norm1.weight', 'backbones.0.blocks.7.norm1.bias', 'backbones.0.blocks.7.attn.qkv.weight', 'backbones.0.blocks.7.attn.qkv.bias', 'backbones.0.blocks.7.attn.proj.weight', 'backbones.0.blocks.7.attn.proj.bias', 'backbones.0.blocks.7.norm2.weight', 'backbones.0.blocks.7.norm2.bias', 'backbones.0.blocks.7.mlp.fc1.weight', 'backbones.0.blocks.7.mlp.fc1.bias', 'backbones.0.blocks.7.mlp.fc2.weight', 'backbones.0.blocks.7.mlp.fc2.bias', 'backbones.0.blocks.8.norm1.weight', 'backbones.0.blocks.8.norm1.bias', 'backbones.0.blocks.8.attn.qkv.weight', 'backbones.0.blocks.8.attn.qkv.bias', 'backbones.0.blocks.8.attn.proj.weight', 'backbones.0.blocks.8.attn.proj.bias', 'backbones.0.blocks.8.norm2.weight', 'backbones.0.blocks.8.norm2.bias', 'backbones.0.blocks.8.mlp.fc1.weight', 'backbones.0.blocks.8.mlp.fc1.bias', 'backbones.0.blocks.8.mlp.fc2.weight', 'backbones.0.blocks.8.mlp.fc2.bias', 'backbones.0.blocks.9.norm1.weight', 'backbones.0.blocks.9.norm1.bias', 'backbones.0.blocks.9.attn.qkv.weight', 'backbones.0.blocks.9.attn.qkv.bias', 'backbones.0.blocks.9.attn.proj.weight', 'backbones.0.blocks.9.attn.proj.bias', 'backbones.0.blocks.9.norm2.weight', 'backbones.0.blocks.9.norm2.bias', 'backbones.0.blocks.9.mlp.fc1.weight', 'backbones.0.blocks.9.mlp.fc1.bias', 'backbones.0.blocks.9.mlp.fc2.weight', 'backbones.0.blocks.9.mlp.fc2.bias', 'backbones.0.blocks.10.norm1.weight', 'backbones.0.blocks.10.norm1.bias', 'backbones.0.blocks.10.attn.qkv.weight', 'backbones.0.blocks.10.attn.qkv.bias', 'backbones.0.blocks.10.attn.proj.weight', 'backbones.0.blocks.10.attn.proj.bias', 'backbones.0.blocks.10.norm2.weight', 'backbones.0.blocks.10.norm2.bias', 'backbones.0.blocks.10.mlp.fc1.weight', 'backbones.0.blocks.10.mlp.fc1.bias', 'backbones.0.blocks.10.mlp.fc2.weight', 'backbones.0.blocks.10.mlp.fc2.bias', 'backbones.0.blocks.11.norm1.weight', 'backbones.0.blocks.11.norm1.bias', 'backbones.0.blocks.11.attn.qkv.weight', 'backbones.0.blocks.11.attn.qkv.bias', 'backbones.0.blocks.11.attn.proj.weight', 'backbones.0.blocks.11.attn.proj.bias', 'backbones.0.blocks.11.norm2.weight', 'backbones.0.blocks.11.norm2.bias', 'backbones.0.blocks.11.mlp.fc1.weight', 'backbones.0.blocks.11.mlp.fc1.bias', 'backbones.0.blocks.11.mlp.fc2.weight', 'backbones.0.blocks.11.mlp.fc2.bias', 'backbones.0.norm.weight', 'backbones.0.norm.bias', 'backbones.1.cls_token', 'backbones.1.pos_embed', 'backbones.1.patch_embed.proj.weight', 'backbones.1.patch_embed.proj.bias', 'backbones.1.blocks.0.norm1.weight', 'backbones.1.blocks.0.norm1.bias', 'backbones.1.blocks.0.attn.q_proj.weight', 'backbones.1.blocks.0.attn.q_proj.bias', 'backbones.1.blocks.0.attn.v_proj.weight', 'backbones.1.blocks.0.attn.v_proj.bias', 'backbones.1.blocks.0.attn.k_proj.weight', 'backbones.1.blocks.0.attn.k_proj.bias', 'backbones.1.blocks.0.attn.proj.weight', 'backbones.1.blocks.0.attn.proj.bias', 'backbones.1.blocks.0.norm2.weight', 'backbones.1.blocks.0.norm2.bias', 'backbones.1.blocks.0.fc1.weight', 'backbones.1.blocks.0.fc1.bias', 'backbones.1.blocks.0.fc2.weight', 'backbones.1.blocks.0.fc2.bias', 'backbones.1.blocks.0.adaptmlp.down_proj.weight', 'backbones.1.blocks.0.adaptmlp.down_proj.bias', 'backbones.1.blocks.0.adaptmlp.up_proj.weight', 'backbones.1.blocks.0.adaptmlp.up_proj.bias', 'backbones.1.blocks.1.norm1.weight', 'backbones.1.blocks.1.norm1.bias', 'backbones.1.blocks.1.attn.q_proj.weight', 'backbones.1.blocks.1.attn.q_proj.bias', 'backbones.1.blocks.1.attn.v_proj.weight', 'backbones.1.blocks.1.attn.v_proj.bias', 'backbones.1.blocks.1.attn.k_proj.weight', 'backbones.1.blocks.1.attn.k_proj.bias', 'backbones.1.blocks.1.attn.proj.weight', 'backbones.1.blocks.1.attn.proj.bias', 'backbones.1.blocks.1.norm2.weight', 'backbones.1.blocks.1.norm2.bias', 'backbones.1.blocks.1.fc1.weight', 'backbones.1.blocks.1.fc1.bias', 'backbones.1.blocks.1.fc2.weight', 'backbones.1.blocks.1.fc2.bias', 'backbones.1.blocks.1.adaptmlp.down_proj.weight', 'backbones.1.blocks.1.adaptmlp.down_proj.bias', 'backbones.1.blocks.1.adaptmlp.up_proj.weight', 'backbones.1.blocks.1.adaptmlp.up_proj.bias', 'backbones.1.blocks.2.norm1.weight', 'backbones.1.blocks.2.norm1.bias', 'backbones.1.blocks.2.attn.q_proj.weight', 'backbones.1.blocks.2.attn.q_proj.bias', 'backbones.1.blocks.2.attn.v_proj.weight', 'backbones.1.blocks.2.attn.v_proj.bias', 'backbones.1.blocks.2.attn.k_proj.weight', 'backbones.1.blocks.2.attn.k_proj.bias', 'backbones.1.blocks.2.attn.proj.weight', 'backbones.1.blocks.2.attn.proj.bias', 'backbones.1.blocks.2.norm2.weight', 'backbones.1.blocks.2.norm2.bias', 'backbones.1.blocks.2.fc1.weight', 'backbones.1.blocks.2.fc1.bias', 'backbones.1.blocks.2.fc2.weight', 'backbones.1.blocks.2.fc2.bias', 'backbones.1.blocks.2.adaptmlp.down_proj.weight', 'backbones.1.blocks.2.adaptmlp.down_proj.bias', 'backbones.1.blocks.2.adaptmlp.up_proj.weight', 'backbones.1.blocks.2.adaptmlp.up_proj.bias', 'backbones.1.blocks.3.norm1.weight', 'backbones.1.blocks.3.norm1.bias', 'backbones.1.blocks.3.attn.q_proj.weight', 'backbones.1.blocks.3.attn.q_proj.bias', 'backbones.1.blocks.3.attn.v_proj.weight', 'backbones.1.blocks.3.attn.v_proj.bias', 'backbones.1.blocks.3.attn.k_proj.weight', 'backbones.1.blocks.3.attn.k_proj.bias', 'backbones.1.blocks.3.attn.proj.weight', 'backbones.1.blocks.3.attn.proj.bias', 'backbones.1.blocks.3.norm2.weight', 'backbones.1.blocks.3.norm2.bias', 'backbones.1.blocks.3.fc1.weight', 'backbones.1.blocks.3.fc1.bias', 'backbones.1.blocks.3.fc2.weight', 'backbones.1.blocks.3.fc2.bias', 'backbones.1.blocks.3.adaptmlp.down_proj.weight', 'backbones.1.blocks.3.adaptmlp.down_proj.bias', 'backbones.1.blocks.3.adaptmlp.up_proj.weight', 'backbones.1.blocks.3.adaptmlp.up_proj.bias', 'backbones.1.blocks.4.norm1.weight', 'backbones.1.blocks.4.norm1.bias', 'backbones.1.blocks.4.attn.q_proj.weight', 'backbones.1.blocks.4.attn.q_proj.bias', 'backbones.1.blocks.4.attn.v_proj.weight', 'backbones.1.blocks.4.attn.v_proj.bias', 'backbones.1.blocks.4.attn.k_proj.weight', 'backbones.1.blocks.4.attn.k_proj.bias', 'backbones.1.blocks.4.attn.proj.weight', 'backbones.1.blocks.4.attn.proj.bias', 'backbones.1.blocks.4.norm2.weight', 'backbones.1.blocks.4.norm2.bias', 'backbones.1.blocks.4.fc1.weight', 'backbones.1.blocks.4.fc1.bias', 'backbones.1.blocks.4.fc2.weight', 'backbones.1.blocks.4.fc2.bias', 'backbones.1.blocks.4.adaptmlp.down_proj.weight', 'backbones.1.blocks.4.adaptmlp.down_proj.bias', 'backbones.1.blocks.4.adaptmlp.up_proj.weight', 'backbones.1.blocks.4.adaptmlp.up_proj.bias', 'backbones.1.blocks.5.norm1.weight', 'backbones.1.blocks.5.norm1.bias', 'backbones.1.blocks.5.attn.q_proj.weight', 'backbones.1.blocks.5.attn.q_proj.bias', 'backbones.1.blocks.5.attn.v_proj.weight', 'backbones.1.blocks.5.attn.v_proj.bias', 'backbones.1.blocks.5.attn.k_proj.weight', 'backbones.1.blocks.5.attn.k_proj.bias', 'backbones.1.blocks.5.attn.proj.weight', 'backbones.1.blocks.5.attn.proj.bias', 'backbones.1.blocks.5.norm2.weight', 'backbones.1.blocks.5.norm2.bias', 'backbones.1.blocks.5.fc1.weight', 'backbones.1.blocks.5.fc1.bias', 'backbones.1.blocks.5.fc2.weight', 'backbones.1.blocks.5.fc2.bias', 'backbones.1.blocks.5.adaptmlp.down_proj.weight', 'backbones.1.blocks.5.adaptmlp.down_proj.bias', 'backbones.1.blocks.5.adaptmlp.up_proj.weight', 'backbones.1.blocks.5.adaptmlp.up_proj.bias', 'backbones.1.blocks.6.norm1.weight', 'backbones.1.blocks.6.norm1.bias', 'backbones.1.blocks.6.attn.q_proj.weight', 'backbones.1.blocks.6.attn.q_proj.bias', 'backbones.1.blocks.6.attn.v_proj.weight', 'backbones.1.blocks.6.attn.v_proj.bias', 'backbones.1.blocks.6.attn.k_proj.weight', 'backbones.1.blocks.6.attn.k_proj.bias', 'backbones.1.blocks.6.attn.proj.weight', 'backbones.1.blocks.6.attn.proj.bias', 'backbones.1.blocks.6.norm2.weight', 'backbones.1.blocks.6.norm2.bias', 'backbones.1.blocks.6.fc1.weight', 'backbones.1.blocks.6.fc1.bias', 'backbones.1.blocks.6.fc2.weight', 'backbones.1.blocks.6.fc2.bias', 'backbones.1.blocks.6.adaptmlp.down_proj.weight', 'backbones.1.blocks.6.adaptmlp.down_proj.bias', 'backbones.1.blocks.6.adaptmlp.up_proj.weight', 'backbones.1.blocks.6.adaptmlp.up_proj.bias', 'backbones.1.blocks.7.norm1.weight', 'backbones.1.blocks.7.norm1.bias', 'backbones.1.blocks.7.attn.q_proj.weight', 'backbones.1.blocks.7.attn.q_proj.bias', 'backbones.1.blocks.7.attn.v_proj.weight', 'backbones.1.blocks.7.attn.v_proj.bias', 'backbones.1.blocks.7.attn.k_proj.weight', 'backbones.1.blocks.7.attn.k_proj.bias', 'backbones.1.blocks.7.attn.proj.weight', 'backbones.1.blocks.7.attn.proj.bias', 'backbones.1.blocks.7.norm2.weight', 'backbones.1.blocks.7.norm2.bias', 'backbones.1.blocks.7.fc1.weight', 'backbones.1.blocks.7.fc1.bias', 'backbones.1.blocks.7.fc2.weight', 'backbones.1.blocks.7.fc2.bias', 'backbones.1.blocks.7.adaptmlp.down_proj.weight', 'backbones.1.blocks.7.adaptmlp.down_proj.bias', 'backbones.1.blocks.7.adaptmlp.up_proj.weight', 'backbones.1.blocks.7.adaptmlp.up_proj.bias', 'backbones.1.blocks.8.norm1.weight', 'backbones.1.blocks.8.norm1.bias', 'backbones.1.blocks.8.attn.q_proj.weight', 'backbones.1.blocks.8.attn.q_proj.bias', 'backbones.1.blocks.8.attn.v_proj.weight', 'backbones.1.blocks.8.attn.v_proj.bias', 'backbones.1.blocks.8.attn.k_proj.weight', 'backbones.1.blocks.8.attn.k_proj.bias', 'backbones.1.blocks.8.attn.proj.weight', 'backbones.1.blocks.8.attn.proj.bias', 'backbones.1.blocks.8.norm2.weight', 'backbones.1.blocks.8.norm2.bias', 'backbones.1.blocks.8.fc1.weight', 'backbones.1.blocks.8.fc1.bias', 'backbones.1.blocks.8.fc2.weight', 'backbones.1.blocks.8.fc2.bias', 'backbones.1.blocks.8.adaptmlp.down_proj.weight', 'backbones.1.blocks.8.adaptmlp.down_proj.bias', 'backbones.1.blocks.8.adaptmlp.up_proj.weight', 'backbones.1.blocks.8.adaptmlp.up_proj.bias', 'backbones.1.blocks.9.norm1.weight', 'backbones.1.blocks.9.norm1.bias', 'backbones.1.blocks.9.attn.q_proj.weight', 'backbones.1.blocks.9.attn.q_proj.bias', 'backbones.1.blocks.9.attn.v_proj.weight', 'backbones.1.blocks.9.attn.v_proj.bias', 'backbones.1.blocks.9.attn.k_proj.weight', 'backbones.1.blocks.9.attn.k_proj.bias', 'backbones.1.blocks.9.attn.proj.weight', 'backbones.1.blocks.9.attn.proj.bias', 'backbones.1.blocks.9.norm2.weight', 'backbones.1.blocks.9.norm2.bias', 'backbones.1.blocks.9.fc1.weight', 'backbones.1.blocks.9.fc1.bias', 'backbones.1.blocks.9.fc2.weight', 'backbones.1.blocks.9.fc2.bias', 'backbones.1.blocks.9.adaptmlp.down_proj.weight', 'backbones.1.blocks.9.adaptmlp.down_proj.bias', 'backbones.1.blocks.9.adaptmlp.up_proj.weight', 'backbones.1.blocks.9.adaptmlp.up_proj.bias', 'backbones.1.blocks.10.norm1.weight', 'backbones.1.blocks.10.norm1.bias', 'backbones.1.blocks.10.attn.q_proj.weight', 'backbones.1.blocks.10.attn.q_proj.bias', 'backbones.1.blocks.10.attn.v_proj.weight', 'backbones.1.blocks.10.attn.v_proj.bias', 'backbones.1.blocks.10.attn.k_proj.weight', 'backbones.1.blocks.10.attn.k_proj.bias', 'backbones.1.blocks.10.attn.proj.weight', 'backbones.1.blocks.10.attn.proj.bias', 'backbones.1.blocks.10.norm2.weight', 'backbones.1.blocks.10.norm2.bias', 'backbones.1.blocks.10.fc1.weight', 'backbones.1.blocks.10.fc1.bias', 'backbones.1.blocks.10.fc2.weight', 'backbones.1.blocks.10.fc2.bias', 'backbones.1.blocks.10.adaptmlp.down_proj.weight', 'backbones.1.blocks.10.adaptmlp.down_proj.bias', 'backbones.1.blocks.10.adaptmlp.up_proj.weight', 'backbones.1.blocks.10.adaptmlp.up_proj.bias', 'backbones.1.blocks.11.norm1.weight', 'backbones.1.blocks.11.norm1.bias', 'backbones.1.blocks.11.attn.q_proj.weight', 'backbones.1.blocks.11.attn.q_proj.bias', 'backbones.1.blocks.11.attn.v_proj.weight', 'backbones.1.blocks.11.attn.v_proj.bias', 'backbones.1.blocks.11.attn.k_proj.weight', 'backbones.1.blocks.11.attn.k_proj.bias', 'backbones.1.blocks.11.attn.proj.weight', 'backbones.1.blocks.11.attn.proj.bias', 'backbones.1.blocks.11.norm2.weight', 'backbones.1.blocks.11.norm2.bias', 'backbones.1.blocks.11.fc1.weight', 'backbones.1.blocks.11.fc1.bias', 'backbones.1.blocks.11.fc2.weight', 'backbones.1.blocks.11.fc2.bias', 'backbones.1.blocks.11.adaptmlp.down_proj.weight', 'backbones.1.blocks.11.adaptmlp.down_proj.bias', 'backbones.1.blocks.11.adaptmlp.up_proj.weight', 'backbones.1.blocks.11.adaptmlp.up_proj.bias', 'backbones.1.norm.weight', 'backbones.1.norm.bias', 'fc.weight', 'fc.sigma'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8011bb12-92d4-46b1-89fd-f12abd6508f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.backbone.cls_token\", \"module.backbone.pos_embed\", \"module.backbone.patch_embed.proj.weight\", \"module.backbone.patch_embed.proj.bias\", \"module.backbone.blocks.0.norm1.weight\", \"module.backbone.blocks.0.norm1.bias\", \"module.backbone.blocks.0.attn.q_proj.weight\", \"module.backbone.blocks.0.attn.q_proj.bias\", \"module.backbone.blocks.0.attn.v_proj.weight\", \"module.backbone.blocks.0.attn.v_proj.bias\", \"module.backbone.blocks.0.attn.k_proj.weight\", \"module.backbone.blocks.0.attn.k_proj.bias\", \"module.backbone.blocks.0.attn.proj.weight\", \"module.backbone.blocks.0.attn.proj.bias\", \"module.backbone.blocks.0.norm2.weight\", \"module.backbone.blocks.0.norm2.bias\", \"module.backbone.blocks.0.fc1.weight\", \"module.backbone.blocks.0.fc1.bias\", \"module.backbone.blocks.0.fc2.weight\", \"module.backbone.blocks.0.fc2.bias\", \"module.backbone.blocks.0.adaptmlp.down_proj.weight\", \"module.backbone.blocks.0.adaptmlp.down_proj.bias\", \"module.backbone.blocks.0.adaptmlp.up_proj.weight\", \"module.backbone.blocks.0.adaptmlp.up_proj.bias\", \"module.backbone.blocks.1.norm1.weight\", \"module.backbone.blocks.1.norm1.bias\", \"module.backbone.blocks.1.attn.q_proj.weight\", \"module.backbone.blocks.1.attn.q_proj.bias\", \"module.backbone.blocks.1.attn.v_proj.weight\", \"module.backbone.blocks.1.attn.v_proj.bias\", \"module.backbone.blocks.1.attn.k_proj.weight\", \"module.backbone.blocks.1.attn.k_proj.bias\", \"module.backbone.blocks.1.attn.proj.weight\", \"module.backbone.blocks.1.attn.proj.bias\", \"module.backbone.blocks.1.norm2.weight\", \"module.backbone.blocks.1.norm2.bias\", \"module.backbone.blocks.1.fc1.weight\", \"module.backbone.blocks.1.fc1.bias\", \"module.backbone.blocks.1.fc2.weight\", \"module.backbone.blocks.1.fc2.bias\", \"module.backbone.blocks.1.adaptmlp.down_proj.weight\", \"module.backbone.blocks.1.adaptmlp.down_proj.bias\", \"module.backbone.blocks.1.adaptmlp.up_proj.weight\", \"module.backbone.blocks.1.adaptmlp.up_proj.bias\", \"module.backbone.blocks.2.norm1.weight\", \"module.backbone.blocks.2.norm1.bias\", \"module.backbone.blocks.2.attn.q_proj.weight\", \"module.backbone.blocks.2.attn.q_proj.bias\", \"module.backbone.blocks.2.attn.v_proj.weight\", \"module.backbone.blocks.2.attn.v_proj.bias\", \"module.backbone.blocks.2.attn.k_proj.weight\", \"module.backbone.blocks.2.attn.k_proj.bias\", \"module.backbone.blocks.2.attn.proj.weight\", \"module.backbone.blocks.2.attn.proj.bias\", \"module.backbone.blocks.2.norm2.weight\", \"module.backbone.blocks.2.norm2.bias\", \"module.backbone.blocks.2.fc1.weight\", \"module.backbone.blocks.2.fc1.bias\", \"module.backbone.blocks.2.fc2.weight\", \"module.backbone.blocks.2.fc2.bias\", \"module.backbone.blocks.2.adaptmlp.down_proj.weight\", \"module.backbone.blocks.2.adaptmlp.down_proj.bias\", \"module.backbone.blocks.2.adaptmlp.up_proj.weight\", \"module.backbone.blocks.2.adaptmlp.up_proj.bias\", \"module.backbone.blocks.3.norm1.weight\", \"module.backbone.blocks.3.norm1.bias\", \"module.backbone.blocks.3.attn.q_proj.weight\", \"module.backbone.blocks.3.attn.q_proj.bias\", \"module.backbone.blocks.3.attn.v_proj.weight\", \"module.backbone.blocks.3.attn.v_proj.bias\", \"module.backbone.blocks.3.attn.k_proj.weight\", \"module.backbone.blocks.3.attn.k_proj.bias\", \"module.backbone.blocks.3.attn.proj.weight\", \"module.backbone.blocks.3.attn.proj.bias\", \"module.backbone.blocks.3.norm2.weight\", \"module.backbone.blocks.3.norm2.bias\", \"module.backbone.blocks.3.fc1.weight\", \"module.backbone.blocks.3.fc1.bias\", \"module.backbone.blocks.3.fc2.weight\", \"module.backbone.blocks.3.fc2.bias\", \"module.backbone.blocks.3.adaptmlp.down_proj.weight\", \"module.backbone.blocks.3.adaptmlp.down_proj.bias\", \"module.backbone.blocks.3.adaptmlp.up_proj.weight\", \"module.backbone.blocks.3.adaptmlp.up_proj.bias\", \"module.backbone.blocks.4.norm1.weight\", \"module.backbone.blocks.4.norm1.bias\", \"module.backbone.blocks.4.attn.q_proj.weight\", \"module.backbone.blocks.4.attn.q_proj.bias\", \"module.backbone.blocks.4.attn.v_proj.weight\", \"module.backbone.blocks.4.attn.v_proj.bias\", \"module.backbone.blocks.4.attn.k_proj.weight\", \"module.backbone.blocks.4.attn.k_proj.bias\", \"module.backbone.blocks.4.attn.proj.weight\", \"module.backbone.blocks.4.attn.proj.bias\", \"module.backbone.blocks.4.norm2.weight\", \"module.backbone.blocks.4.norm2.bias\", \"module.backbone.blocks.4.fc1.weight\", \"module.backbone.blocks.4.fc1.bias\", \"module.backbone.blocks.4.fc2.weight\", \"module.backbone.blocks.4.fc2.bias\", \"module.backbone.blocks.4.adaptmlp.down_proj.weight\", \"module.backbone.blocks.4.adaptmlp.down_proj.bias\", \"module.backbone.blocks.4.adaptmlp.up_proj.weight\", \"module.backbone.blocks.4.adaptmlp.up_proj.bias\", \"module.backbone.blocks.5.norm1.weight\", \"module.backbone.blocks.5.norm1.bias\", \"module.backbone.blocks.5.attn.q_proj.weight\", \"module.backbone.blocks.5.attn.q_proj.bias\", \"module.backbone.blocks.5.attn.v_proj.weight\", \"module.backbone.blocks.5.attn.v_proj.bias\", \"module.backbone.blocks.5.attn.k_proj.weight\", \"module.backbone.blocks.5.attn.k_proj.bias\", \"module.backbone.blocks.5.attn.proj.weight\", \"module.backbone.blocks.5.attn.proj.bias\", \"module.backbone.blocks.5.norm2.weight\", \"module.backbone.blocks.5.norm2.bias\", \"module.backbone.blocks.5.fc1.weight\", \"module.backbone.blocks.5.fc1.bias\", \"module.backbone.blocks.5.fc2.weight\", \"module.backbone.blocks.5.fc2.bias\", \"module.backbone.blocks.5.adaptmlp.down_proj.weight\", \"module.backbone.blocks.5.adaptmlp.down_proj.bias\", \"module.backbone.blocks.5.adaptmlp.up_proj.weight\", \"module.backbone.blocks.5.adaptmlp.up_proj.bias\", \"module.backbone.blocks.6.norm1.weight\", \"module.backbone.blocks.6.norm1.bias\", \"module.backbone.blocks.6.attn.q_proj.weight\", \"module.backbone.blocks.6.attn.q_proj.bias\", \"module.backbone.blocks.6.attn.v_proj.weight\", \"module.backbone.blocks.6.attn.v_proj.bias\", \"module.backbone.blocks.6.attn.k_proj.weight\", \"module.backbone.blocks.6.attn.k_proj.bias\", \"module.backbone.blocks.6.attn.proj.weight\", \"module.backbone.blocks.6.attn.proj.bias\", \"module.backbone.blocks.6.norm2.weight\", \"module.backbone.blocks.6.norm2.bias\", \"module.backbone.blocks.6.fc1.weight\", \"module.backbone.blocks.6.fc1.bias\", \"module.backbone.blocks.6.fc2.weight\", \"module.backbone.blocks.6.fc2.bias\", \"module.backbone.blocks.6.adaptmlp.down_proj.weight\", \"module.backbone.blocks.6.adaptmlp.down_proj.bias\", \"module.backbone.blocks.6.adaptmlp.up_proj.weight\", \"module.backbone.blocks.6.adaptmlp.up_proj.bias\", \"module.backbone.blocks.7.norm1.weight\", \"module.backbone.blocks.7.norm1.bias\", \"module.backbone.blocks.7.attn.q_proj.weight\", \"module.backbone.blocks.7.attn.q_proj.bias\", \"module.backbone.blocks.7.attn.v_proj.weight\", \"module.backbone.blocks.7.attn.v_proj.bias\", \"module.backbone.blocks.7.attn.k_proj.weight\", \"module.backbone.blocks.7.attn.k_proj.bias\", \"module.backbone.blocks.7.attn.proj.weight\", \"module.backbone.blocks.7.attn.proj.bias\", \"module.backbone.blocks.7.norm2.weight\", \"module.backbone.blocks.7.norm2.bias\", \"module.backbone.blocks.7.fc1.weight\", \"module.backbone.blocks.7.fc1.bias\", \"module.backbone.blocks.7.fc2.weight\", \"module.backbone.blocks.7.fc2.bias\", \"module.backbone.blocks.7.adaptmlp.down_proj.weight\", \"module.backbone.blocks.7.adaptmlp.down_proj.bias\", \"module.backbone.blocks.7.adaptmlp.up_proj.weight\", \"module.backbone.blocks.7.adaptmlp.up_proj.bias\", \"module.backbone.blocks.8.norm1.weight\", \"module.backbone.blocks.8.norm1.bias\", \"module.backbone.blocks.8.attn.q_proj.weight\", \"module.backbone.blocks.8.attn.q_proj.bias\", \"module.backbone.blocks.8.attn.v_proj.weight\", \"module.backbone.blocks.8.attn.v_proj.bias\", \"module.backbone.blocks.8.attn.k_proj.weight\", \"module.backbone.blocks.8.attn.k_proj.bias\", \"module.backbone.blocks.8.attn.proj.weight\", \"module.backbone.blocks.8.attn.proj.bias\", \"module.backbone.blocks.8.norm2.weight\", \"module.backbone.blocks.8.norm2.bias\", \"module.backbone.blocks.8.fc1.weight\", \"module.backbone.blocks.8.fc1.bias\", \"module.backbone.blocks.8.fc2.weight\", \"module.backbone.blocks.8.fc2.bias\", \"module.backbone.blocks.8.adaptmlp.down_proj.weight\", \"module.backbone.blocks.8.adaptmlp.down_proj.bias\", \"module.backbone.blocks.8.adaptmlp.up_proj.weight\", \"module.backbone.blocks.8.adaptmlp.up_proj.bias\", \"module.backbone.blocks.9.norm1.weight\", \"module.backbone.blocks.9.norm1.bias\", \"module.backbone.blocks.9.attn.q_proj.weight\", \"module.backbone.blocks.9.attn.q_proj.bias\", \"module.backbone.blocks.9.attn.v_proj.weight\", \"module.backbone.blocks.9.attn.v_proj.bias\", \"module.backbone.blocks.9.attn.k_proj.weight\", \"module.backbone.blocks.9.attn.k_proj.bias\", \"module.backbone.blocks.9.attn.proj.weight\", \"module.backbone.blocks.9.attn.proj.bias\", \"module.backbone.blocks.9.norm2.weight\", \"module.backbone.blocks.9.norm2.bias\", \"module.backbone.blocks.9.fc1.weight\", \"module.backbone.blocks.9.fc1.bias\", \"module.backbone.blocks.9.fc2.weight\", \"module.backbone.blocks.9.fc2.bias\", \"module.backbone.blocks.9.adaptmlp.down_proj.weight\", \"module.backbone.blocks.9.adaptmlp.down_proj.bias\", \"module.backbone.blocks.9.adaptmlp.up_proj.weight\", \"module.backbone.blocks.9.adaptmlp.up_proj.bias\", \"module.backbone.blocks.10.norm1.weight\", \"module.backbone.blocks.10.norm1.bias\", \"module.backbone.blocks.10.attn.q_proj.weight\", \"module.backbone.blocks.10.attn.q_proj.bias\", \"module.backbone.blocks.10.attn.v_proj.weight\", \"module.backbone.blocks.10.attn.v_proj.bias\", \"module.backbone.blocks.10.attn.k_proj.weight\", \"module.backbone.blocks.10.attn.k_proj.bias\", \"module.backbone.blocks.10.attn.proj.weight\", \"module.backbone.blocks.10.attn.proj.bias\", \"module.backbone.blocks.10.norm2.weight\", \"module.backbone.blocks.10.norm2.bias\", \"module.backbone.blocks.10.fc1.weight\", \"module.backbone.blocks.10.fc1.bias\", \"module.backbone.blocks.10.fc2.weight\", \"module.backbone.blocks.10.fc2.bias\", \"module.backbone.blocks.10.adaptmlp.down_proj.weight\", \"module.backbone.blocks.10.adaptmlp.down_proj.bias\", \"module.backbone.blocks.10.adaptmlp.up_proj.weight\", \"module.backbone.blocks.10.adaptmlp.up_proj.bias\", \"module.backbone.blocks.11.norm1.weight\", \"module.backbone.blocks.11.norm1.bias\", \"module.backbone.blocks.11.attn.q_proj.weight\", \"module.backbone.blocks.11.attn.q_proj.bias\", \"module.backbone.blocks.11.attn.v_proj.weight\", \"module.backbone.blocks.11.attn.v_proj.bias\", \"module.backbone.blocks.11.attn.k_proj.weight\", \"module.backbone.blocks.11.attn.k_proj.bias\", \"module.backbone.blocks.11.attn.proj.weight\", \"module.backbone.blocks.11.attn.proj.bias\", \"module.backbone.blocks.11.norm2.weight\", \"module.backbone.blocks.11.norm2.bias\", \"module.backbone.blocks.11.fc1.weight\", \"module.backbone.blocks.11.fc1.bias\", \"module.backbone.blocks.11.fc2.weight\", \"module.backbone.blocks.11.fc2.bias\", \"module.backbone.blocks.11.adaptmlp.down_proj.weight\", \"module.backbone.blocks.11.adaptmlp.down_proj.bias\", \"module.backbone.blocks.11.adaptmlp.up_proj.weight\", \"module.backbone.blocks.11.adaptmlp.up_proj.bias\", \"module.backbone.norm.weight\", \"module.backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m m \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(model\u001b[38;5;241m.\u001b[39m_network)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.backbone.cls_token\", \"module.backbone.pos_embed\", \"module.backbone.patch_embed.proj.weight\", \"module.backbone.patch_embed.proj.bias\", \"module.backbone.blocks.0.norm1.weight\", \"module.backbone.blocks.0.norm1.bias\", \"module.backbone.blocks.0.attn.q_proj.weight\", \"module.backbone.blocks.0.attn.q_proj.bias\", \"module.backbone.blocks.0.attn.v_proj.weight\", \"module.backbone.blocks.0.attn.v_proj.bias\", \"module.backbone.blocks.0.attn.k_proj.weight\", \"module.backbone.blocks.0.attn.k_proj.bias\", \"module.backbone.blocks.0.attn.proj.weight\", \"module.backbone.blocks.0.attn.proj.bias\", \"module.backbone.blocks.0.norm2.weight\", \"module.backbone.blocks.0.norm2.bias\", \"module.backbone.blocks.0.fc1.weight\", \"module.backbone.blocks.0.fc1.bias\", \"module.backbone.blocks.0.fc2.weight\", \"module.backbone.blocks.0.fc2.bias\", \"module.backbone.blocks.0.adaptmlp.down_proj.weight\", \"module.backbone.blocks.0.adaptmlp.down_proj.bias\", \"module.backbone.blocks.0.adaptmlp.up_proj.weight\", \"module.backbone.blocks.0.adaptmlp.up_proj.bias\", \"module.backbone.blocks.1.norm1.weight\", \"module.backbone.blocks.1.norm1.bias\", \"module.backbone.blocks.1.attn.q_proj.weight\", \"module.backbone.blocks.1.attn.q_proj.bias\", \"module.backbone.blocks.1.attn.v_proj.weight\", \"module.backbone.blocks.1.attn.v_proj.bias\", \"module.backbone.blocks.1.attn.k_proj.weight\", \"module.backbone.blocks.1.attn.k_proj.bias\", \"module.backbone.blocks.1.attn.proj.weight\", \"module.backbone.blocks.1.attn.proj.bias\", \"module.backbone.blocks.1.norm2.weight\", \"module.backbone.blocks.1.norm2.bias\", \"module.backbone.blocks.1.fc1.weight\", \"module.backbone.blocks.1.fc1.bias\", \"module.backbone.blocks.1.fc2.weight\", \"module.backbone.blocks.1.fc2.bias\", \"module.backbone.blocks.1.adaptmlp.down_proj.weight\", \"module.backbone.blocks.1.adaptmlp.down_proj.bias\", \"module.backbone.blocks.1.adaptmlp.up_proj.weight\", \"module.backbone.blocks.1.adaptmlp.up_proj.bias\", \"module.backbone.blocks.2.norm1.weight\", \"module.backbone.blocks.2.norm1.bias\", \"module.backbone.blocks.2.attn.q_proj.weight\", \"module.backbone.blocks.2.attn.q_proj.bias\", \"module.backbone.blocks.2.attn.v_proj.weight\", \"module.backbone.blocks.2.attn.v_proj.bias\", \"module.backbone.blocks.2.attn.k_proj.weight\", \"module.backbone.blocks.2.attn.k_proj.bias\", \"module.backbone.blocks.2.attn.proj.weight\", \"module.backbone.blocks.2.attn.proj.bias\", \"module.backbone.blocks.2.norm2.weight\", \"module.backbone.blocks.2.norm2.bias\", \"module.backbone.blocks.2.fc1.weight\", \"module.backbone.blocks.2.fc1.bias\", \"module.backbone.blocks.2.fc2.weight\", \"module.backbone.blocks.2.fc2.bias\", \"module.backbone.blocks.2.adaptmlp.down_proj.weight\", \"module.backbone.blocks.2.adaptmlp.down_proj.bias\", \"module.backbone.blocks.2.adaptmlp.up_proj.weight\", \"module.backbone.blocks.2.adaptmlp.up_proj.bias\", \"module.backbone.blocks.3.norm1.weight\", \"module.backbone.blocks.3.norm1.bias\", \"module.backbone.blocks.3.attn.q_proj.weight\", \"module.backbone.blocks.3.attn.q_proj.bias\", \"module.backbone.blocks.3.attn.v_proj.weight\", \"module.backbone.blocks.3.attn.v_proj.bias\", \"module.backbone.blocks.3.attn.k_proj.weight\", \"module.backbone.blocks.3.attn.k_proj.bias\", \"module.backbone.blocks.3.attn.proj.weight\", \"module.backbone.blocks.3.attn.proj.bias\", \"module.backbone.blocks.3.norm2.weight\", \"module.backbone.blocks.3.norm2.bias\", \"module.backbone.blocks.3.fc1.weight\", \"module.backbone.blocks.3.fc1.bias\", \"module.backbone.blocks.3.fc2.weight\", \"module.backbone.blocks.3.fc2.bias\", \"module.backbone.blocks.3.adaptmlp.down_proj.weight\", \"module.backbone.blocks.3.adaptmlp.down_proj.bias\", \"module.backbone.blocks.3.adaptmlp.up_proj.weight\", \"module.backbone.blocks.3.adaptmlp.up_proj.bias\", \"module.backbone.blocks.4.norm1.weight\", \"module.backbone.blocks.4.norm1.bias\", \"module.backbone.blocks.4.attn.q_proj.weight\", \"module.backbone.blocks.4.attn.q_proj.bias\", \"module.backbone.blocks.4.attn.v_proj.weight\", \"module.backbone.blocks.4.attn.v_proj.bias\", \"module.backbone.blocks.4.attn.k_proj.weight\", \"module.backbone.blocks.4.attn.k_proj.bias\", \"module.backbone.blocks.4.attn.proj.weight\", \"module.backbone.blocks.4.attn.proj.bias\", \"module.backbone.blocks.4.norm2.weight\", \"module.backbone.blocks.4.norm2.bias\", \"module.backbone.blocks.4.fc1.weight\", \"module.backbone.blocks.4.fc1.bias\", \"module.backbone.blocks.4.fc2.weight\", \"module.backbone.blocks.4.fc2.bias\", \"module.backbone.blocks.4.adaptmlp.down_proj.weight\", \"module.backbone.blocks.4.adaptmlp.down_proj.bias\", \"module.backbone.blocks.4.adaptmlp.up_proj.weight\", \"module.backbone.blocks.4.adaptmlp.up_proj.bias\", \"module.backbone.blocks.5.norm1.weight\", \"module.backbone.blocks.5.norm1.bias\", \"module.backbone.blocks.5.attn.q_proj.weight\", \"module.backbone.blocks.5.attn.q_proj.bias\", \"module.backbone.blocks.5.attn.v_proj.weight\", \"module.backbone.blocks.5.attn.v_proj.bias\", \"module.backbone.blocks.5.attn.k_proj.weight\", \"module.backbone.blocks.5.attn.k_proj.bias\", \"module.backbone.blocks.5.attn.proj.weight\", \"module.backbone.blocks.5.attn.proj.bias\", \"module.backbone.blocks.5.norm2.weight\", \"module.backbone.blocks.5.norm2.bias\", \"module.backbone.blocks.5.fc1.weight\", \"module.backbone.blocks.5.fc1.bias\", \"module.backbone.blocks.5.fc2.weight\", \"module.backbone.blocks.5.fc2.bias\", \"module.backbone.blocks.5.adaptmlp.down_proj.weight\", \"module.backbone.blocks.5.adaptmlp.down_proj.bias\", \"module.backbone.blocks.5.adaptmlp.up_proj.weight\", \"module.backbone.blocks.5.adaptmlp.up_proj.bias\", \"module.backbone.blocks.6.norm1.weight\", \"module.backbone.blocks.6.norm1.bias\", \"module.backbone.blocks.6.attn.q_proj.weight\", \"module.backbone.blocks.6.attn.q_proj.bias\", \"module.backbone.blocks.6.attn.v_proj.weight\", \"module.backbone.blocks.6.attn.v_proj.bias\", \"module.backbone.blocks.6.attn.k_proj.weight\", \"module.backbone.blocks.6.attn.k_proj.bias\", \"module.backbone.blocks.6.attn.proj.weight\", \"module.backbone.blocks.6.attn.proj.bias\", \"module.backbone.blocks.6.norm2.weight\", \"module.backbone.blocks.6.norm2.bias\", \"module.backbone.blocks.6.fc1.weight\", \"module.backbone.blocks.6.fc1.bias\", \"module.backbone.blocks.6.fc2.weight\", \"module.backbone.blocks.6.fc2.bias\", \"module.backbone.blocks.6.adaptmlp.down_proj.weight\", \"module.backbone.blocks.6.adaptmlp.down_proj.bias\", \"module.backbone.blocks.6.adaptmlp.up_proj.weight\", \"module.backbone.blocks.6.adaptmlp.up_proj.bias\", \"module.backbone.blocks.7.norm1.weight\", \"module.backbone.blocks.7.norm1.bias\", \"module.backbone.blocks.7.attn.q_proj.weight\", \"module.backbone.blocks.7.attn.q_proj.bias\", \"module.backbone.blocks.7.attn.v_proj.weight\", \"module.backbone.blocks.7.attn.v_proj.bias\", \"module.backbone.blocks.7.attn.k_proj.weight\", \"module.backbone.blocks.7.attn.k_proj.bias\", \"module.backbone.blocks.7.attn.proj.weight\", \"module.backbone.blocks.7.attn.proj.bias\", \"module.backbone.blocks.7.norm2.weight\", \"module.backbone.blocks.7.norm2.bias\", \"module.backbone.blocks.7.fc1.weight\", \"module.backbone.blocks.7.fc1.bias\", \"module.backbone.blocks.7.fc2.weight\", \"module.backbone.blocks.7.fc2.bias\", \"module.backbone.blocks.7.adaptmlp.down_proj.weight\", \"module.backbone.blocks.7.adaptmlp.down_proj.bias\", \"module.backbone.blocks.7.adaptmlp.up_proj.weight\", \"module.backbone.blocks.7.adaptmlp.up_proj.bias\", \"module.backbone.blocks.8.norm1.weight\", \"module.backbone.blocks.8.norm1.bias\", \"module.backbone.blocks.8.attn.q_proj.weight\", \"module.backbone.blocks.8.attn.q_proj.bias\", \"module.backbone.blocks.8.attn.v_proj.weight\", \"module.backbone.blocks.8.attn.v_proj.bias\", \"module.backbone.blocks.8.attn.k_proj.weight\", \"module.backbone.blocks.8.attn.k_proj.bias\", \"module.backbone.blocks.8.attn.proj.weight\", \"module.backbone.blocks.8.attn.proj.bias\", \"module.backbone.blocks.8.norm2.weight\", \"module.backbone.blocks.8.norm2.bias\", \"module.backbone.blocks.8.fc1.weight\", \"module.backbone.blocks.8.fc1.bias\", \"module.backbone.blocks.8.fc2.weight\", \"module.backbone.blocks.8.fc2.bias\", \"module.backbone.blocks.8.adaptmlp.down_proj.weight\", \"module.backbone.blocks.8.adaptmlp.down_proj.bias\", \"module.backbone.blocks.8.adaptmlp.up_proj.weight\", \"module.backbone.blocks.8.adaptmlp.up_proj.bias\", \"module.backbone.blocks.9.norm1.weight\", \"module.backbone.blocks.9.norm1.bias\", \"module.backbone.blocks.9.attn.q_proj.weight\", \"module.backbone.blocks.9.attn.q_proj.bias\", \"module.backbone.blocks.9.attn.v_proj.weight\", \"module.backbone.blocks.9.attn.v_proj.bias\", \"module.backbone.blocks.9.attn.k_proj.weight\", \"module.backbone.blocks.9.attn.k_proj.bias\", \"module.backbone.blocks.9.attn.proj.weight\", \"module.backbone.blocks.9.attn.proj.bias\", \"module.backbone.blocks.9.norm2.weight\", \"module.backbone.blocks.9.norm2.bias\", \"module.backbone.blocks.9.fc1.weight\", \"module.backbone.blocks.9.fc1.bias\", \"module.backbone.blocks.9.fc2.weight\", \"module.backbone.blocks.9.fc2.bias\", \"module.backbone.blocks.9.adaptmlp.down_proj.weight\", \"module.backbone.blocks.9.adaptmlp.down_proj.bias\", \"module.backbone.blocks.9.adaptmlp.up_proj.weight\", \"module.backbone.blocks.9.adaptmlp.up_proj.bias\", \"module.backbone.blocks.10.norm1.weight\", \"module.backbone.blocks.10.norm1.bias\", \"module.backbone.blocks.10.attn.q_proj.weight\", \"module.backbone.blocks.10.attn.q_proj.bias\", \"module.backbone.blocks.10.attn.v_proj.weight\", \"module.backbone.blocks.10.attn.v_proj.bias\", \"module.backbone.blocks.10.attn.k_proj.weight\", \"module.backbone.blocks.10.attn.k_proj.bias\", \"module.backbone.blocks.10.attn.proj.weight\", \"module.backbone.blocks.10.attn.proj.bias\", \"module.backbone.blocks.10.norm2.weight\", \"module.backbone.blocks.10.norm2.bias\", \"module.backbone.blocks.10.fc1.weight\", \"module.backbone.blocks.10.fc1.bias\", \"module.backbone.blocks.10.fc2.weight\", \"module.backbone.blocks.10.fc2.bias\", \"module.backbone.blocks.10.adaptmlp.down_proj.weight\", \"module.backbone.blocks.10.adaptmlp.down_proj.bias\", \"module.backbone.blocks.10.adaptmlp.up_proj.weight\", \"module.backbone.blocks.10.adaptmlp.up_proj.bias\", \"module.backbone.blocks.11.norm1.weight\", \"module.backbone.blocks.11.norm1.bias\", \"module.backbone.blocks.11.attn.q_proj.weight\", \"module.backbone.blocks.11.attn.q_proj.bias\", \"module.backbone.blocks.11.attn.v_proj.weight\", \"module.backbone.blocks.11.attn.v_proj.bias\", \"module.backbone.blocks.11.attn.k_proj.weight\", \"module.backbone.blocks.11.attn.k_proj.bias\", \"module.backbone.blocks.11.attn.proj.weight\", \"module.backbone.blocks.11.attn.proj.bias\", \"module.backbone.blocks.11.norm2.weight\", \"module.backbone.blocks.11.norm2.bias\", \"module.backbone.blocks.11.fc1.weight\", \"module.backbone.blocks.11.fc1.bias\", \"module.backbone.blocks.11.fc2.weight\", \"module.backbone.blocks.11.fc2.bias\", \"module.backbone.blocks.11.adaptmlp.down_proj.weight\", \"module.backbone.blocks.11.adaptmlp.down_proj.bias\", \"module.backbone.blocks.11.adaptmlp.up_proj.weight\", \"module.backbone.blocks.11.adaptmlp.up_proj.bias\", \"module.backbone.norm.weight\", \"module.backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". "
     ]
    }
   ],
   "source": [
    "m = nn.DataParallel(model._network)\n",
    "m.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfb20f6e-90cb-42ac-bcff-12a4a2931b7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SimpleVitNet:\n\tMissing key(s) in state_dict: \"backbone.cls_token\", \"backbone.pos_embed\", \"backbone.patch_embed.proj.weight\", \"backbone.patch_embed.proj.bias\", \"backbone.blocks.0.norm1.weight\", \"backbone.blocks.0.norm1.bias\", \"backbone.blocks.0.attn.q_proj.weight\", \"backbone.blocks.0.attn.q_proj.bias\", \"backbone.blocks.0.attn.v_proj.weight\", \"backbone.blocks.0.attn.v_proj.bias\", \"backbone.blocks.0.attn.k_proj.weight\", \"backbone.blocks.0.attn.k_proj.bias\", \"backbone.blocks.0.attn.proj.weight\", \"backbone.blocks.0.attn.proj.bias\", \"backbone.blocks.0.norm2.weight\", \"backbone.blocks.0.norm2.bias\", \"backbone.blocks.0.fc1.weight\", \"backbone.blocks.0.fc1.bias\", \"backbone.blocks.0.fc2.weight\", \"backbone.blocks.0.fc2.bias\", \"backbone.blocks.0.adaptmlp.down_proj.weight\", \"backbone.blocks.0.adaptmlp.down_proj.bias\", \"backbone.blocks.0.adaptmlp.up_proj.weight\", \"backbone.blocks.0.adaptmlp.up_proj.bias\", \"backbone.blocks.1.norm1.weight\", \"backbone.blocks.1.norm1.bias\", \"backbone.blocks.1.attn.q_proj.weight\", \"backbone.blocks.1.attn.q_proj.bias\", \"backbone.blocks.1.attn.v_proj.weight\", \"backbone.blocks.1.attn.v_proj.bias\", \"backbone.blocks.1.attn.k_proj.weight\", \"backbone.blocks.1.attn.k_proj.bias\", \"backbone.blocks.1.attn.proj.weight\", \"backbone.blocks.1.attn.proj.bias\", \"backbone.blocks.1.norm2.weight\", \"backbone.blocks.1.norm2.bias\", \"backbone.blocks.1.fc1.weight\", \"backbone.blocks.1.fc1.bias\", \"backbone.blocks.1.fc2.weight\", \"backbone.blocks.1.fc2.bias\", \"backbone.blocks.1.adaptmlp.down_proj.weight\", \"backbone.blocks.1.adaptmlp.down_proj.bias\", \"backbone.blocks.1.adaptmlp.up_proj.weight\", \"backbone.blocks.1.adaptmlp.up_proj.bias\", \"backbone.blocks.2.norm1.weight\", \"backbone.blocks.2.norm1.bias\", \"backbone.blocks.2.attn.q_proj.weight\", \"backbone.blocks.2.attn.q_proj.bias\", \"backbone.blocks.2.attn.v_proj.weight\", \"backbone.blocks.2.attn.v_proj.bias\", \"backbone.blocks.2.attn.k_proj.weight\", \"backbone.blocks.2.attn.k_proj.bias\", \"backbone.blocks.2.attn.proj.weight\", \"backbone.blocks.2.attn.proj.bias\", \"backbone.blocks.2.norm2.weight\", \"backbone.blocks.2.norm2.bias\", \"backbone.blocks.2.fc1.weight\", \"backbone.blocks.2.fc1.bias\", \"backbone.blocks.2.fc2.weight\", \"backbone.blocks.2.fc2.bias\", \"backbone.blocks.2.adaptmlp.down_proj.weight\", \"backbone.blocks.2.adaptmlp.down_proj.bias\", \"backbone.blocks.2.adaptmlp.up_proj.weight\", \"backbone.blocks.2.adaptmlp.up_proj.bias\", \"backbone.blocks.3.norm1.weight\", \"backbone.blocks.3.norm1.bias\", \"backbone.blocks.3.attn.q_proj.weight\", \"backbone.blocks.3.attn.q_proj.bias\", \"backbone.blocks.3.attn.v_proj.weight\", \"backbone.blocks.3.attn.v_proj.bias\", \"backbone.blocks.3.attn.k_proj.weight\", \"backbone.blocks.3.attn.k_proj.bias\", \"backbone.blocks.3.attn.proj.weight\", \"backbone.blocks.3.attn.proj.bias\", \"backbone.blocks.3.norm2.weight\", \"backbone.blocks.3.norm2.bias\", \"backbone.blocks.3.fc1.weight\", \"backbone.blocks.3.fc1.bias\", \"backbone.blocks.3.fc2.weight\", \"backbone.blocks.3.fc2.bias\", \"backbone.blocks.3.adaptmlp.down_proj.weight\", \"backbone.blocks.3.adaptmlp.down_proj.bias\", \"backbone.blocks.3.adaptmlp.up_proj.weight\", \"backbone.blocks.3.adaptmlp.up_proj.bias\", \"backbone.blocks.4.norm1.weight\", \"backbone.blocks.4.norm1.bias\", \"backbone.blocks.4.attn.q_proj.weight\", \"backbone.blocks.4.attn.q_proj.bias\", \"backbone.blocks.4.attn.v_proj.weight\", \"backbone.blocks.4.attn.v_proj.bias\", \"backbone.blocks.4.attn.k_proj.weight\", \"backbone.blocks.4.attn.k_proj.bias\", \"backbone.blocks.4.attn.proj.weight\", \"backbone.blocks.4.attn.proj.bias\", \"backbone.blocks.4.norm2.weight\", \"backbone.blocks.4.norm2.bias\", \"backbone.blocks.4.fc1.weight\", \"backbone.blocks.4.fc1.bias\", \"backbone.blocks.4.fc2.weight\", \"backbone.blocks.4.fc2.bias\", \"backbone.blocks.4.adaptmlp.down_proj.weight\", \"backbone.blocks.4.adaptmlp.down_proj.bias\", \"backbone.blocks.4.adaptmlp.up_proj.weight\", \"backbone.blocks.4.adaptmlp.up_proj.bias\", \"backbone.blocks.5.norm1.weight\", \"backbone.blocks.5.norm1.bias\", \"backbone.blocks.5.attn.q_proj.weight\", \"backbone.blocks.5.attn.q_proj.bias\", \"backbone.blocks.5.attn.v_proj.weight\", \"backbone.blocks.5.attn.v_proj.bias\", \"backbone.blocks.5.attn.k_proj.weight\", \"backbone.blocks.5.attn.k_proj.bias\", \"backbone.blocks.5.attn.proj.weight\", \"backbone.blocks.5.attn.proj.bias\", \"backbone.blocks.5.norm2.weight\", \"backbone.blocks.5.norm2.bias\", \"backbone.blocks.5.fc1.weight\", \"backbone.blocks.5.fc1.bias\", \"backbone.blocks.5.fc2.weight\", \"backbone.blocks.5.fc2.bias\", \"backbone.blocks.5.adaptmlp.down_proj.weight\", \"backbone.blocks.5.adaptmlp.down_proj.bias\", \"backbone.blocks.5.adaptmlp.up_proj.weight\", \"backbone.blocks.5.adaptmlp.up_proj.bias\", \"backbone.blocks.6.norm1.weight\", \"backbone.blocks.6.norm1.bias\", \"backbone.blocks.6.attn.q_proj.weight\", \"backbone.blocks.6.attn.q_proj.bias\", \"backbone.blocks.6.attn.v_proj.weight\", \"backbone.blocks.6.attn.v_proj.bias\", \"backbone.blocks.6.attn.k_proj.weight\", \"backbone.blocks.6.attn.k_proj.bias\", \"backbone.blocks.6.attn.proj.weight\", \"backbone.blocks.6.attn.proj.bias\", \"backbone.blocks.6.norm2.weight\", \"backbone.blocks.6.norm2.bias\", \"backbone.blocks.6.fc1.weight\", \"backbone.blocks.6.fc1.bias\", \"backbone.blocks.6.fc2.weight\", \"backbone.blocks.6.fc2.bias\", \"backbone.blocks.6.adaptmlp.down_proj.weight\", \"backbone.blocks.6.adaptmlp.down_proj.bias\", \"backbone.blocks.6.adaptmlp.up_proj.weight\", \"backbone.blocks.6.adaptmlp.up_proj.bias\", \"backbone.blocks.7.norm1.weight\", \"backbone.blocks.7.norm1.bias\", \"backbone.blocks.7.attn.q_proj.weight\", \"backbone.blocks.7.attn.q_proj.bias\", \"backbone.blocks.7.attn.v_proj.weight\", \"backbone.blocks.7.attn.v_proj.bias\", \"backbone.blocks.7.attn.k_proj.weight\", \"backbone.blocks.7.attn.k_proj.bias\", \"backbone.blocks.7.attn.proj.weight\", \"backbone.blocks.7.attn.proj.bias\", \"backbone.blocks.7.norm2.weight\", \"backbone.blocks.7.norm2.bias\", \"backbone.blocks.7.fc1.weight\", \"backbone.blocks.7.fc1.bias\", \"backbone.blocks.7.fc2.weight\", \"backbone.blocks.7.fc2.bias\", \"backbone.blocks.7.adaptmlp.down_proj.weight\", \"backbone.blocks.7.adaptmlp.down_proj.bias\", \"backbone.blocks.7.adaptmlp.up_proj.weight\", \"backbone.blocks.7.adaptmlp.up_proj.bias\", \"backbone.blocks.8.norm1.weight\", \"backbone.blocks.8.norm1.bias\", \"backbone.blocks.8.attn.q_proj.weight\", \"backbone.blocks.8.attn.q_proj.bias\", \"backbone.blocks.8.attn.v_proj.weight\", \"backbone.blocks.8.attn.v_proj.bias\", \"backbone.blocks.8.attn.k_proj.weight\", \"backbone.blocks.8.attn.k_proj.bias\", \"backbone.blocks.8.attn.proj.weight\", \"backbone.blocks.8.attn.proj.bias\", \"backbone.blocks.8.norm2.weight\", \"backbone.blocks.8.norm2.bias\", \"backbone.blocks.8.fc1.weight\", \"backbone.blocks.8.fc1.bias\", \"backbone.blocks.8.fc2.weight\", \"backbone.blocks.8.fc2.bias\", \"backbone.blocks.8.adaptmlp.down_proj.weight\", \"backbone.blocks.8.adaptmlp.down_proj.bias\", \"backbone.blocks.8.adaptmlp.up_proj.weight\", \"backbone.blocks.8.adaptmlp.up_proj.bias\", \"backbone.blocks.9.norm1.weight\", \"backbone.blocks.9.norm1.bias\", \"backbone.blocks.9.attn.q_proj.weight\", \"backbone.blocks.9.attn.q_proj.bias\", \"backbone.blocks.9.attn.v_proj.weight\", \"backbone.blocks.9.attn.v_proj.bias\", \"backbone.blocks.9.attn.k_proj.weight\", \"backbone.blocks.9.attn.k_proj.bias\", \"backbone.blocks.9.attn.proj.weight\", \"backbone.blocks.9.attn.proj.bias\", \"backbone.blocks.9.norm2.weight\", \"backbone.blocks.9.norm2.bias\", \"backbone.blocks.9.fc1.weight\", \"backbone.blocks.9.fc1.bias\", \"backbone.blocks.9.fc2.weight\", \"backbone.blocks.9.fc2.bias\", \"backbone.blocks.9.adaptmlp.down_proj.weight\", \"backbone.blocks.9.adaptmlp.down_proj.bias\", \"backbone.blocks.9.adaptmlp.up_proj.weight\", \"backbone.blocks.9.adaptmlp.up_proj.bias\", \"backbone.blocks.10.norm1.weight\", \"backbone.blocks.10.norm1.bias\", \"backbone.blocks.10.attn.q_proj.weight\", \"backbone.blocks.10.attn.q_proj.bias\", \"backbone.blocks.10.attn.v_proj.weight\", \"backbone.blocks.10.attn.v_proj.bias\", \"backbone.blocks.10.attn.k_proj.weight\", \"backbone.blocks.10.attn.k_proj.bias\", \"backbone.blocks.10.attn.proj.weight\", \"backbone.blocks.10.attn.proj.bias\", \"backbone.blocks.10.norm2.weight\", \"backbone.blocks.10.norm2.bias\", \"backbone.blocks.10.fc1.weight\", \"backbone.blocks.10.fc1.bias\", \"backbone.blocks.10.fc2.weight\", \"backbone.blocks.10.fc2.bias\", \"backbone.blocks.10.adaptmlp.down_proj.weight\", \"backbone.blocks.10.adaptmlp.down_proj.bias\", \"backbone.blocks.10.adaptmlp.up_proj.weight\", \"backbone.blocks.10.adaptmlp.up_proj.bias\", \"backbone.blocks.11.norm1.weight\", \"backbone.blocks.11.norm1.bias\", \"backbone.blocks.11.attn.q_proj.weight\", \"backbone.blocks.11.attn.q_proj.bias\", \"backbone.blocks.11.attn.v_proj.weight\", \"backbone.blocks.11.attn.v_proj.bias\", \"backbone.blocks.11.attn.k_proj.weight\", \"backbone.blocks.11.attn.k_proj.bias\", \"backbone.blocks.11.attn.proj.weight\", \"backbone.blocks.11.attn.proj.bias\", \"backbone.blocks.11.norm2.weight\", \"backbone.blocks.11.norm2.bias\", \"backbone.blocks.11.fc1.weight\", \"backbone.blocks.11.fc1.bias\", \"backbone.blocks.11.fc2.weight\", \"backbone.blocks.11.fc2.bias\", \"backbone.blocks.11.adaptmlp.down_proj.weight\", \"backbone.blocks.11.adaptmlp.down_proj.bias\", \"backbone.blocks.11.adaptmlp.up_proj.weight\", \"backbone.blocks.11.adaptmlp.up_proj.bias\", \"backbone.norm.weight\", \"backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SimpleVitNet:\n\tMissing key(s) in state_dict: \"backbone.cls_token\", \"backbone.pos_embed\", \"backbone.patch_embed.proj.weight\", \"backbone.patch_embed.proj.bias\", \"backbone.blocks.0.norm1.weight\", \"backbone.blocks.0.norm1.bias\", \"backbone.blocks.0.attn.q_proj.weight\", \"backbone.blocks.0.attn.q_proj.bias\", \"backbone.blocks.0.attn.v_proj.weight\", \"backbone.blocks.0.attn.v_proj.bias\", \"backbone.blocks.0.attn.k_proj.weight\", \"backbone.blocks.0.attn.k_proj.bias\", \"backbone.blocks.0.attn.proj.weight\", \"backbone.blocks.0.attn.proj.bias\", \"backbone.blocks.0.norm2.weight\", \"backbone.blocks.0.norm2.bias\", \"backbone.blocks.0.fc1.weight\", \"backbone.blocks.0.fc1.bias\", \"backbone.blocks.0.fc2.weight\", \"backbone.blocks.0.fc2.bias\", \"backbone.blocks.0.adaptmlp.down_proj.weight\", \"backbone.blocks.0.adaptmlp.down_proj.bias\", \"backbone.blocks.0.adaptmlp.up_proj.weight\", \"backbone.blocks.0.adaptmlp.up_proj.bias\", \"backbone.blocks.1.norm1.weight\", \"backbone.blocks.1.norm1.bias\", \"backbone.blocks.1.attn.q_proj.weight\", \"backbone.blocks.1.attn.q_proj.bias\", \"backbone.blocks.1.attn.v_proj.weight\", \"backbone.blocks.1.attn.v_proj.bias\", \"backbone.blocks.1.attn.k_proj.weight\", \"backbone.blocks.1.attn.k_proj.bias\", \"backbone.blocks.1.attn.proj.weight\", \"backbone.blocks.1.attn.proj.bias\", \"backbone.blocks.1.norm2.weight\", \"backbone.blocks.1.norm2.bias\", \"backbone.blocks.1.fc1.weight\", \"backbone.blocks.1.fc1.bias\", \"backbone.blocks.1.fc2.weight\", \"backbone.blocks.1.fc2.bias\", \"backbone.blocks.1.adaptmlp.down_proj.weight\", \"backbone.blocks.1.adaptmlp.down_proj.bias\", \"backbone.blocks.1.adaptmlp.up_proj.weight\", \"backbone.blocks.1.adaptmlp.up_proj.bias\", \"backbone.blocks.2.norm1.weight\", \"backbone.blocks.2.norm1.bias\", \"backbone.blocks.2.attn.q_proj.weight\", \"backbone.blocks.2.attn.q_proj.bias\", \"backbone.blocks.2.attn.v_proj.weight\", \"backbone.blocks.2.attn.v_proj.bias\", \"backbone.blocks.2.attn.k_proj.weight\", \"backbone.blocks.2.attn.k_proj.bias\", \"backbone.blocks.2.attn.proj.weight\", \"backbone.blocks.2.attn.proj.bias\", \"backbone.blocks.2.norm2.weight\", \"backbone.blocks.2.norm2.bias\", \"backbone.blocks.2.fc1.weight\", \"backbone.blocks.2.fc1.bias\", \"backbone.blocks.2.fc2.weight\", \"backbone.blocks.2.fc2.bias\", \"backbone.blocks.2.adaptmlp.down_proj.weight\", \"backbone.blocks.2.adaptmlp.down_proj.bias\", \"backbone.blocks.2.adaptmlp.up_proj.weight\", \"backbone.blocks.2.adaptmlp.up_proj.bias\", \"backbone.blocks.3.norm1.weight\", \"backbone.blocks.3.norm1.bias\", \"backbone.blocks.3.attn.q_proj.weight\", \"backbone.blocks.3.attn.q_proj.bias\", \"backbone.blocks.3.attn.v_proj.weight\", \"backbone.blocks.3.attn.v_proj.bias\", \"backbone.blocks.3.attn.k_proj.weight\", \"backbone.blocks.3.attn.k_proj.bias\", \"backbone.blocks.3.attn.proj.weight\", \"backbone.blocks.3.attn.proj.bias\", \"backbone.blocks.3.norm2.weight\", \"backbone.blocks.3.norm2.bias\", \"backbone.blocks.3.fc1.weight\", \"backbone.blocks.3.fc1.bias\", \"backbone.blocks.3.fc2.weight\", \"backbone.blocks.3.fc2.bias\", \"backbone.blocks.3.adaptmlp.down_proj.weight\", \"backbone.blocks.3.adaptmlp.down_proj.bias\", \"backbone.blocks.3.adaptmlp.up_proj.weight\", \"backbone.blocks.3.adaptmlp.up_proj.bias\", \"backbone.blocks.4.norm1.weight\", \"backbone.blocks.4.norm1.bias\", \"backbone.blocks.4.attn.q_proj.weight\", \"backbone.blocks.4.attn.q_proj.bias\", \"backbone.blocks.4.attn.v_proj.weight\", \"backbone.blocks.4.attn.v_proj.bias\", \"backbone.blocks.4.attn.k_proj.weight\", \"backbone.blocks.4.attn.k_proj.bias\", \"backbone.blocks.4.attn.proj.weight\", \"backbone.blocks.4.attn.proj.bias\", \"backbone.blocks.4.norm2.weight\", \"backbone.blocks.4.norm2.bias\", \"backbone.blocks.4.fc1.weight\", \"backbone.blocks.4.fc1.bias\", \"backbone.blocks.4.fc2.weight\", \"backbone.blocks.4.fc2.bias\", \"backbone.blocks.4.adaptmlp.down_proj.weight\", \"backbone.blocks.4.adaptmlp.down_proj.bias\", \"backbone.blocks.4.adaptmlp.up_proj.weight\", \"backbone.blocks.4.adaptmlp.up_proj.bias\", \"backbone.blocks.5.norm1.weight\", \"backbone.blocks.5.norm1.bias\", \"backbone.blocks.5.attn.q_proj.weight\", \"backbone.blocks.5.attn.q_proj.bias\", \"backbone.blocks.5.attn.v_proj.weight\", \"backbone.blocks.5.attn.v_proj.bias\", \"backbone.blocks.5.attn.k_proj.weight\", \"backbone.blocks.5.attn.k_proj.bias\", \"backbone.blocks.5.attn.proj.weight\", \"backbone.blocks.5.attn.proj.bias\", \"backbone.blocks.5.norm2.weight\", \"backbone.blocks.5.norm2.bias\", \"backbone.blocks.5.fc1.weight\", \"backbone.blocks.5.fc1.bias\", \"backbone.blocks.5.fc2.weight\", \"backbone.blocks.5.fc2.bias\", \"backbone.blocks.5.adaptmlp.down_proj.weight\", \"backbone.blocks.5.adaptmlp.down_proj.bias\", \"backbone.blocks.5.adaptmlp.up_proj.weight\", \"backbone.blocks.5.adaptmlp.up_proj.bias\", \"backbone.blocks.6.norm1.weight\", \"backbone.blocks.6.norm1.bias\", \"backbone.blocks.6.attn.q_proj.weight\", \"backbone.blocks.6.attn.q_proj.bias\", \"backbone.blocks.6.attn.v_proj.weight\", \"backbone.blocks.6.attn.v_proj.bias\", \"backbone.blocks.6.attn.k_proj.weight\", \"backbone.blocks.6.attn.k_proj.bias\", \"backbone.blocks.6.attn.proj.weight\", \"backbone.blocks.6.attn.proj.bias\", \"backbone.blocks.6.norm2.weight\", \"backbone.blocks.6.norm2.bias\", \"backbone.blocks.6.fc1.weight\", \"backbone.blocks.6.fc1.bias\", \"backbone.blocks.6.fc2.weight\", \"backbone.blocks.6.fc2.bias\", \"backbone.blocks.6.adaptmlp.down_proj.weight\", \"backbone.blocks.6.adaptmlp.down_proj.bias\", \"backbone.blocks.6.adaptmlp.up_proj.weight\", \"backbone.blocks.6.adaptmlp.up_proj.bias\", \"backbone.blocks.7.norm1.weight\", \"backbone.blocks.7.norm1.bias\", \"backbone.blocks.7.attn.q_proj.weight\", \"backbone.blocks.7.attn.q_proj.bias\", \"backbone.blocks.7.attn.v_proj.weight\", \"backbone.blocks.7.attn.v_proj.bias\", \"backbone.blocks.7.attn.k_proj.weight\", \"backbone.blocks.7.attn.k_proj.bias\", \"backbone.blocks.7.attn.proj.weight\", \"backbone.blocks.7.attn.proj.bias\", \"backbone.blocks.7.norm2.weight\", \"backbone.blocks.7.norm2.bias\", \"backbone.blocks.7.fc1.weight\", \"backbone.blocks.7.fc1.bias\", \"backbone.blocks.7.fc2.weight\", \"backbone.blocks.7.fc2.bias\", \"backbone.blocks.7.adaptmlp.down_proj.weight\", \"backbone.blocks.7.adaptmlp.down_proj.bias\", \"backbone.blocks.7.adaptmlp.up_proj.weight\", \"backbone.blocks.7.adaptmlp.up_proj.bias\", \"backbone.blocks.8.norm1.weight\", \"backbone.blocks.8.norm1.bias\", \"backbone.blocks.8.attn.q_proj.weight\", \"backbone.blocks.8.attn.q_proj.bias\", \"backbone.blocks.8.attn.v_proj.weight\", \"backbone.blocks.8.attn.v_proj.bias\", \"backbone.blocks.8.attn.k_proj.weight\", \"backbone.blocks.8.attn.k_proj.bias\", \"backbone.blocks.8.attn.proj.weight\", \"backbone.blocks.8.attn.proj.bias\", \"backbone.blocks.8.norm2.weight\", \"backbone.blocks.8.norm2.bias\", \"backbone.blocks.8.fc1.weight\", \"backbone.blocks.8.fc1.bias\", \"backbone.blocks.8.fc2.weight\", \"backbone.blocks.8.fc2.bias\", \"backbone.blocks.8.adaptmlp.down_proj.weight\", \"backbone.blocks.8.adaptmlp.down_proj.bias\", \"backbone.blocks.8.adaptmlp.up_proj.weight\", \"backbone.blocks.8.adaptmlp.up_proj.bias\", \"backbone.blocks.9.norm1.weight\", \"backbone.blocks.9.norm1.bias\", \"backbone.blocks.9.attn.q_proj.weight\", \"backbone.blocks.9.attn.q_proj.bias\", \"backbone.blocks.9.attn.v_proj.weight\", \"backbone.blocks.9.attn.v_proj.bias\", \"backbone.blocks.9.attn.k_proj.weight\", \"backbone.blocks.9.attn.k_proj.bias\", \"backbone.blocks.9.attn.proj.weight\", \"backbone.blocks.9.attn.proj.bias\", \"backbone.blocks.9.norm2.weight\", \"backbone.blocks.9.norm2.bias\", \"backbone.blocks.9.fc1.weight\", \"backbone.blocks.9.fc1.bias\", \"backbone.blocks.9.fc2.weight\", \"backbone.blocks.9.fc2.bias\", \"backbone.blocks.9.adaptmlp.down_proj.weight\", \"backbone.blocks.9.adaptmlp.down_proj.bias\", \"backbone.blocks.9.adaptmlp.up_proj.weight\", \"backbone.blocks.9.adaptmlp.up_proj.bias\", \"backbone.blocks.10.norm1.weight\", \"backbone.blocks.10.norm1.bias\", \"backbone.blocks.10.attn.q_proj.weight\", \"backbone.blocks.10.attn.q_proj.bias\", \"backbone.blocks.10.attn.v_proj.weight\", \"backbone.blocks.10.attn.v_proj.bias\", \"backbone.blocks.10.attn.k_proj.weight\", \"backbone.blocks.10.attn.k_proj.bias\", \"backbone.blocks.10.attn.proj.weight\", \"backbone.blocks.10.attn.proj.bias\", \"backbone.blocks.10.norm2.weight\", \"backbone.blocks.10.norm2.bias\", \"backbone.blocks.10.fc1.weight\", \"backbone.blocks.10.fc1.bias\", \"backbone.blocks.10.fc2.weight\", \"backbone.blocks.10.fc2.bias\", \"backbone.blocks.10.adaptmlp.down_proj.weight\", \"backbone.blocks.10.adaptmlp.down_proj.bias\", \"backbone.blocks.10.adaptmlp.up_proj.weight\", \"backbone.blocks.10.adaptmlp.up_proj.bias\", \"backbone.blocks.11.norm1.weight\", \"backbone.blocks.11.norm1.bias\", \"backbone.blocks.11.attn.q_proj.weight\", \"backbone.blocks.11.attn.q_proj.bias\", \"backbone.blocks.11.attn.v_proj.weight\", \"backbone.blocks.11.attn.v_proj.bias\", \"backbone.blocks.11.attn.k_proj.weight\", \"backbone.blocks.11.attn.k_proj.bias\", \"backbone.blocks.11.attn.proj.weight\", \"backbone.blocks.11.attn.proj.bias\", \"backbone.blocks.11.norm2.weight\", \"backbone.blocks.11.norm2.bias\", \"backbone.blocks.11.fc1.weight\", \"backbone.blocks.11.fc1.bias\", \"backbone.blocks.11.fc2.weight\", \"backbone.blocks.11.fc2.bias\", \"backbone.blocks.11.adaptmlp.down_proj.weight\", \"backbone.blocks.11.adaptmlp.down_proj.bias\", \"backbone.blocks.11.adaptmlp.up_proj.weight\", \"backbone.blocks.11.adaptmlp.up_proj.bias\", \"backbone.norm.weight\", \"backbone.norm.bias\". \n\tUnexpected key(s) in state_dict: \"backbones.0.cls_token\", \"backbones.0.pos_embed\", \"backbones.0.patch_embed.proj.weight\", \"backbones.0.patch_embed.proj.bias\", \"backbones.0.blocks.0.norm1.weight\", \"backbones.0.blocks.0.norm1.bias\", \"backbones.0.blocks.0.attn.qkv.weight\", \"backbones.0.blocks.0.attn.qkv.bias\", \"backbones.0.blocks.0.attn.proj.weight\", \"backbones.0.blocks.0.attn.proj.bias\", \"backbones.0.blocks.0.norm2.weight\", \"backbones.0.blocks.0.norm2.bias\", \"backbones.0.blocks.0.mlp.fc1.weight\", \"backbones.0.blocks.0.mlp.fc1.bias\", \"backbones.0.blocks.0.mlp.fc2.weight\", \"backbones.0.blocks.0.mlp.fc2.bias\", \"backbones.0.blocks.1.norm1.weight\", \"backbones.0.blocks.1.norm1.bias\", \"backbones.0.blocks.1.attn.qkv.weight\", \"backbones.0.blocks.1.attn.qkv.bias\", \"backbones.0.blocks.1.attn.proj.weight\", \"backbones.0.blocks.1.attn.proj.bias\", \"backbones.0.blocks.1.norm2.weight\", \"backbones.0.blocks.1.norm2.bias\", \"backbones.0.blocks.1.mlp.fc1.weight\", \"backbones.0.blocks.1.mlp.fc1.bias\", \"backbones.0.blocks.1.mlp.fc2.weight\", \"backbones.0.blocks.1.mlp.fc2.bias\", \"backbones.0.blocks.2.norm1.weight\", \"backbones.0.blocks.2.norm1.bias\", \"backbones.0.blocks.2.attn.qkv.weight\", \"backbones.0.blocks.2.attn.qkv.bias\", \"backbones.0.blocks.2.attn.proj.weight\", \"backbones.0.blocks.2.attn.proj.bias\", \"backbones.0.blocks.2.norm2.weight\", \"backbones.0.blocks.2.norm2.bias\", \"backbones.0.blocks.2.mlp.fc1.weight\", \"backbones.0.blocks.2.mlp.fc1.bias\", \"backbones.0.blocks.2.mlp.fc2.weight\", \"backbones.0.blocks.2.mlp.fc2.bias\", \"backbones.0.blocks.3.norm1.weight\", \"backbones.0.blocks.3.norm1.bias\", \"backbones.0.blocks.3.attn.qkv.weight\", \"backbones.0.blocks.3.attn.qkv.bias\", \"backbones.0.blocks.3.attn.proj.weight\", \"backbones.0.blocks.3.attn.proj.bias\", \"backbones.0.blocks.3.norm2.weight\", \"backbones.0.blocks.3.norm2.bias\", \"backbones.0.blocks.3.mlp.fc1.weight\", \"backbones.0.blocks.3.mlp.fc1.bias\", \"backbones.0.blocks.3.mlp.fc2.weight\", \"backbones.0.blocks.3.mlp.fc2.bias\", \"backbones.0.blocks.4.norm1.weight\", \"backbones.0.blocks.4.norm1.bias\", \"backbones.0.blocks.4.attn.qkv.weight\", \"backbones.0.blocks.4.attn.qkv.bias\", \"backbones.0.blocks.4.attn.proj.weight\", \"backbones.0.blocks.4.attn.proj.bias\", \"backbones.0.blocks.4.norm2.weight\", \"backbones.0.blocks.4.norm2.bias\", \"backbones.0.blocks.4.mlp.fc1.weight\", \"backbones.0.blocks.4.mlp.fc1.bias\", \"backbones.0.blocks.4.mlp.fc2.weight\", \"backbones.0.blocks.4.mlp.fc2.bias\", \"backbones.0.blocks.5.norm1.weight\", \"backbones.0.blocks.5.norm1.bias\", \"backbones.0.blocks.5.attn.qkv.weight\", \"backbones.0.blocks.5.attn.qkv.bias\", \"backbones.0.blocks.5.attn.proj.weight\", \"backbones.0.blocks.5.attn.proj.bias\", \"backbones.0.blocks.5.norm2.weight\", \"backbones.0.blocks.5.norm2.bias\", \"backbones.0.blocks.5.mlp.fc1.weight\", \"backbones.0.blocks.5.mlp.fc1.bias\", \"backbones.0.blocks.5.mlp.fc2.weight\", \"backbones.0.blocks.5.mlp.fc2.bias\", \"backbones.0.blocks.6.norm1.weight\", \"backbones.0.blocks.6.norm1.bias\", \"backbones.0.blocks.6.attn.qkv.weight\", \"backbones.0.blocks.6.attn.qkv.bias\", \"backbones.0.blocks.6.attn.proj.weight\", \"backbones.0.blocks.6.attn.proj.bias\", \"backbones.0.blocks.6.norm2.weight\", \"backbones.0.blocks.6.norm2.bias\", \"backbones.0.blocks.6.mlp.fc1.weight\", \"backbones.0.blocks.6.mlp.fc1.bias\", \"backbones.0.blocks.6.mlp.fc2.weight\", \"backbones.0.blocks.6.mlp.fc2.bias\", \"backbones.0.blocks.7.norm1.weight\", \"backbones.0.blocks.7.norm1.bias\", \"backbones.0.blocks.7.attn.qkv.weight\", \"backbones.0.blocks.7.attn.qkv.bias\", \"backbones.0.blocks.7.attn.proj.weight\", \"backbones.0.blocks.7.attn.proj.bias\", \"backbones.0.blocks.7.norm2.weight\", \"backbones.0.blocks.7.norm2.bias\", \"backbones.0.blocks.7.mlp.fc1.weight\", \"backbones.0.blocks.7.mlp.fc1.bias\", \"backbones.0.blocks.7.mlp.fc2.weight\", \"backbones.0.blocks.7.mlp.fc2.bias\", \"backbones.0.blocks.8.norm1.weight\", \"backbones.0.blocks.8.norm1.bias\", \"backbones.0.blocks.8.attn.qkv.weight\", \"backbones.0.blocks.8.attn.qkv.bias\", \"backbones.0.blocks.8.attn.proj.weight\", \"backbones.0.blocks.8.attn.proj.bias\", \"backbones.0.blocks.8.norm2.weight\", \"backbones.0.blocks.8.norm2.bias\", \"backbones.0.blocks.8.mlp.fc1.weight\", \"backbones.0.blocks.8.mlp.fc1.bias\", \"backbones.0.blocks.8.mlp.fc2.weight\", \"backbones.0.blocks.8.mlp.fc2.bias\", \"backbones.0.blocks.9.norm1.weight\", \"backbones.0.blocks.9.norm1.bias\", \"backbones.0.blocks.9.attn.qkv.weight\", \"backbones.0.blocks.9.attn.qkv.bias\", \"backbones.0.blocks.9.attn.proj.weight\", \"backbones.0.blocks.9.attn.proj.bias\", \"backbones.0.blocks.9.norm2.weight\", \"backbones.0.blocks.9.norm2.bias\", \"backbones.0.blocks.9.mlp.fc1.weight\", \"backbones.0.blocks.9.mlp.fc1.bias\", \"backbones.0.blocks.9.mlp.fc2.weight\", \"backbones.0.blocks.9.mlp.fc2.bias\", \"backbones.0.blocks.10.norm1.weight\", \"backbones.0.blocks.10.norm1.bias\", \"backbones.0.blocks.10.attn.qkv.weight\", \"backbones.0.blocks.10.attn.qkv.bias\", \"backbones.0.blocks.10.attn.proj.weight\", \"backbones.0.blocks.10.attn.proj.bias\", \"backbones.0.blocks.10.norm2.weight\", \"backbones.0.blocks.10.norm2.bias\", \"backbones.0.blocks.10.mlp.fc1.weight\", \"backbones.0.blocks.10.mlp.fc1.bias\", \"backbones.0.blocks.10.mlp.fc2.weight\", \"backbones.0.blocks.10.mlp.fc2.bias\", \"backbones.0.blocks.11.norm1.weight\", \"backbones.0.blocks.11.norm1.bias\", \"backbones.0.blocks.11.attn.qkv.weight\", \"backbones.0.blocks.11.attn.qkv.bias\", \"backbones.0.blocks.11.attn.proj.weight\", \"backbones.0.blocks.11.attn.proj.bias\", \"backbones.0.blocks.11.norm2.weight\", \"backbones.0.blocks.11.norm2.bias\", \"backbones.0.blocks.11.mlp.fc1.weight\", \"backbones.0.blocks.11.mlp.fc1.bias\", \"backbones.0.blocks.11.mlp.fc2.weight\", \"backbones.0.blocks.11.mlp.fc2.bias\", \"backbones.0.norm.weight\", \"backbones.0.norm.bias\", \"backbones.1.cls_token\", \"backbones.1.pos_embed\", \"backbones.1.patch_embed.proj.weight\", \"backbones.1.patch_embed.proj.bias\", \"backbones.1.blocks.0.norm1.weight\", \"backbones.1.blocks.0.norm1.bias\", \"backbones.1.blocks.0.attn.q_proj.weight\", \"backbones.1.blocks.0.attn.q_proj.bias\", \"backbones.1.blocks.0.attn.v_proj.weight\", \"backbones.1.blocks.0.attn.v_proj.bias\", \"backbones.1.blocks.0.attn.k_proj.weight\", \"backbones.1.blocks.0.attn.k_proj.bias\", \"backbones.1.blocks.0.attn.proj.weight\", \"backbones.1.blocks.0.attn.proj.bias\", \"backbones.1.blocks.0.norm2.weight\", \"backbones.1.blocks.0.norm2.bias\", \"backbones.1.blocks.0.fc1.weight\", \"backbones.1.blocks.0.fc1.bias\", \"backbones.1.blocks.0.fc2.weight\", \"backbones.1.blocks.0.fc2.bias\", \"backbones.1.blocks.0.adaptmlp.down_proj.weight\", \"backbones.1.blocks.0.adaptmlp.down_proj.bias\", \"backbones.1.blocks.0.adaptmlp.up_proj.weight\", \"backbones.1.blocks.0.adaptmlp.up_proj.bias\", \"backbones.1.blocks.1.norm1.weight\", \"backbones.1.blocks.1.norm1.bias\", \"backbones.1.blocks.1.attn.q_proj.weight\", \"backbones.1.blocks.1.attn.q_proj.bias\", \"backbones.1.blocks.1.attn.v_proj.weight\", \"backbones.1.blocks.1.attn.v_proj.bias\", \"backbones.1.blocks.1.attn.k_proj.weight\", \"backbones.1.blocks.1.attn.k_proj.bias\", \"backbones.1.blocks.1.attn.proj.weight\", \"backbones.1.blocks.1.attn.proj.bias\", \"backbones.1.blocks.1.norm2.weight\", \"backbones.1.blocks.1.norm2.bias\", \"backbones.1.blocks.1.fc1.weight\", \"backbones.1.blocks.1.fc1.bias\", \"backbones.1.blocks.1.fc2.weight\", \"backbones.1.blocks.1.fc2.bias\", \"backbones.1.blocks.1.adaptmlp.down_proj.weight\", \"backbones.1.blocks.1.adaptmlp.down_proj.bias\", \"backbones.1.blocks.1.adaptmlp.up_proj.weight\", \"backbones.1.blocks.1.adaptmlp.up_proj.bias\", \"backbones.1.blocks.2.norm1.weight\", \"backbones.1.blocks.2.norm1.bias\", \"backbones.1.blocks.2.attn.q_proj.weight\", \"backbones.1.blocks.2.attn.q_proj.bias\", \"backbones.1.blocks.2.attn.v_proj.weight\", \"backbones.1.blocks.2.attn.v_proj.bias\", \"backbones.1.blocks.2.attn.k_proj.weight\", \"backbones.1.blocks.2.attn.k_proj.bias\", \"backbones.1.blocks.2.attn.proj.weight\", \"backbones.1.blocks.2.attn.proj.bias\", \"backbones.1.blocks.2.norm2.weight\", \"backbones.1.blocks.2.norm2.bias\", \"backbones.1.blocks.2.fc1.weight\", \"backbones.1.blocks.2.fc1.bias\", \"backbones.1.blocks.2.fc2.weight\", \"backbones.1.blocks.2.fc2.bias\", \"backbones.1.blocks.2.adaptmlp.down_proj.weight\", \"backbones.1.blocks.2.adaptmlp.down_proj.bias\", \"backbones.1.blocks.2.adaptmlp.up_proj.weight\", \"backbones.1.blocks.2.adaptmlp.up_proj.bias\", \"backbones.1.blocks.3.norm1.weight\", \"backbones.1.blocks.3.norm1.bias\", \"backbones.1.blocks.3.attn.q_proj.weight\", \"backbones.1.blocks.3.attn.q_proj.bias\", \"backbones.1.blocks.3.attn.v_proj.weight\", \"backbones.1.blocks.3.attn.v_proj.bias\", \"backbones.1.blocks.3.attn.k_proj.weight\", \"backbones.1.blocks.3.attn.k_proj.bias\", \"backbones.1.blocks.3.attn.proj.weight\", \"backbones.1.blocks.3.attn.proj.bias\", \"backbones.1.blocks.3.norm2.weight\", \"backbones.1.blocks.3.norm2.bias\", \"backbones.1.blocks.3.fc1.weight\", \"backbones.1.blocks.3.fc1.bias\", \"backbones.1.blocks.3.fc2.weight\", \"backbones.1.blocks.3.fc2.bias\", \"backbones.1.blocks.3.adaptmlp.down_proj.weight\", \"backbones.1.blocks.3.adaptmlp.down_proj.bias\", \"backbones.1.blocks.3.adaptmlp.up_proj.weight\", \"backbones.1.blocks.3.adaptmlp.up_proj.bias\", \"backbones.1.blocks.4.norm1.weight\", \"backbones.1.blocks.4.norm1.bias\", \"backbones.1.blocks.4.attn.q_proj.weight\", \"backbones.1.blocks.4.attn.q_proj.bias\", \"backbones.1.blocks.4.attn.v_proj.weight\", \"backbones.1.blocks.4.attn.v_proj.bias\", \"backbones.1.blocks.4.attn.k_proj.weight\", \"backbones.1.blocks.4.attn.k_proj.bias\", \"backbones.1.blocks.4.attn.proj.weight\", \"backbones.1.blocks.4.attn.proj.bias\", \"backbones.1.blocks.4.norm2.weight\", \"backbones.1.blocks.4.norm2.bias\", \"backbones.1.blocks.4.fc1.weight\", \"backbones.1.blocks.4.fc1.bias\", \"backbones.1.blocks.4.fc2.weight\", \"backbones.1.blocks.4.fc2.bias\", \"backbones.1.blocks.4.adaptmlp.down_proj.weight\", \"backbones.1.blocks.4.adaptmlp.down_proj.bias\", \"backbones.1.blocks.4.adaptmlp.up_proj.weight\", \"backbones.1.blocks.4.adaptmlp.up_proj.bias\", \"backbones.1.blocks.5.norm1.weight\", \"backbones.1.blocks.5.norm1.bias\", \"backbones.1.blocks.5.attn.q_proj.weight\", \"backbones.1.blocks.5.attn.q_proj.bias\", \"backbones.1.blocks.5.attn.v_proj.weight\", \"backbones.1.blocks.5.attn.v_proj.bias\", \"backbones.1.blocks.5.attn.k_proj.weight\", \"backbones.1.blocks.5.attn.k_proj.bias\", \"backbones.1.blocks.5.attn.proj.weight\", \"backbones.1.blocks.5.attn.proj.bias\", \"backbones.1.blocks.5.norm2.weight\", \"backbones.1.blocks.5.norm2.bias\", \"backbones.1.blocks.5.fc1.weight\", \"backbones.1.blocks.5.fc1.bias\", \"backbones.1.blocks.5.fc2.weight\", \"backbones.1.blocks.5.fc2.bias\", \"backbones.1.blocks.5.adaptmlp.down_proj.weight\", \"backbones.1.blocks.5.adaptmlp.down_proj.bias\", \"backbones.1.blocks.5.adaptmlp.up_proj.weight\", \"backbones.1.blocks.5.adaptmlp.up_proj.bias\", \"backbones.1.blocks.6.norm1.weight\", \"backbones.1.blocks.6.norm1.bias\", \"backbones.1.blocks.6.attn.q_proj.weight\", \"backbones.1.blocks.6.attn.q_proj.bias\", \"backbones.1.blocks.6.attn.v_proj.weight\", \"backbones.1.blocks.6.attn.v_proj.bias\", \"backbones.1.blocks.6.attn.k_proj.weight\", \"backbones.1.blocks.6.attn.k_proj.bias\", \"backbones.1.blocks.6.attn.proj.weight\", \"backbones.1.blocks.6.attn.proj.bias\", \"backbones.1.blocks.6.norm2.weight\", \"backbones.1.blocks.6.norm2.bias\", \"backbones.1.blocks.6.fc1.weight\", \"backbones.1.blocks.6.fc1.bias\", \"backbones.1.blocks.6.fc2.weight\", \"backbones.1.blocks.6.fc2.bias\", \"backbones.1.blocks.6.adaptmlp.down_proj.weight\", \"backbones.1.blocks.6.adaptmlp.down_proj.bias\", \"backbones.1.blocks.6.adaptmlp.up_proj.weight\", \"backbones.1.blocks.6.adaptmlp.up_proj.bias\", \"backbones.1.blocks.7.norm1.weight\", \"backbones.1.blocks.7.norm1.bias\", \"backbones.1.blocks.7.attn.q_proj.weight\", \"backbones.1.blocks.7.attn.q_proj.bias\", \"backbones.1.blocks.7.attn.v_proj.weight\", \"backbones.1.blocks.7.attn.v_proj.bias\", \"backbones.1.blocks.7.attn.k_proj.weight\", \"backbones.1.blocks.7.attn.k_proj.bias\", \"backbones.1.blocks.7.attn.proj.weight\", \"backbones.1.blocks.7.attn.proj.bias\", \"backbones.1.blocks.7.norm2.weight\", \"backbones.1.blocks.7.norm2.bias\", \"backbones.1.blocks.7.fc1.weight\", \"backbones.1.blocks.7.fc1.bias\", \"backbones.1.blocks.7.fc2.weight\", \"backbones.1.blocks.7.fc2.bias\", \"backbones.1.blocks.7.adaptmlp.down_proj.weight\", \"backbones.1.blocks.7.adaptmlp.down_proj.bias\", \"backbones.1.blocks.7.adaptmlp.up_proj.weight\", \"backbones.1.blocks.7.adaptmlp.up_proj.bias\", \"backbones.1.blocks.8.norm1.weight\", \"backbones.1.blocks.8.norm1.bias\", \"backbones.1.blocks.8.attn.q_proj.weight\", \"backbones.1.blocks.8.attn.q_proj.bias\", \"backbones.1.blocks.8.attn.v_proj.weight\", \"backbones.1.blocks.8.attn.v_proj.bias\", \"backbones.1.blocks.8.attn.k_proj.weight\", \"backbones.1.blocks.8.attn.k_proj.bias\", \"backbones.1.blocks.8.attn.proj.weight\", \"backbones.1.blocks.8.attn.proj.bias\", \"backbones.1.blocks.8.norm2.weight\", \"backbones.1.blocks.8.norm2.bias\", \"backbones.1.blocks.8.fc1.weight\", \"backbones.1.blocks.8.fc1.bias\", \"backbones.1.blocks.8.fc2.weight\", \"backbones.1.blocks.8.fc2.bias\", \"backbones.1.blocks.8.adaptmlp.down_proj.weight\", \"backbones.1.blocks.8.adaptmlp.down_proj.bias\", \"backbones.1.blocks.8.adaptmlp.up_proj.weight\", \"backbones.1.blocks.8.adaptmlp.up_proj.bias\", \"backbones.1.blocks.9.norm1.weight\", \"backbones.1.blocks.9.norm1.bias\", \"backbones.1.blocks.9.attn.q_proj.weight\", \"backbones.1.blocks.9.attn.q_proj.bias\", \"backbones.1.blocks.9.attn.v_proj.weight\", \"backbones.1.blocks.9.attn.v_proj.bias\", \"backbones.1.blocks.9.attn.k_proj.weight\", \"backbones.1.blocks.9.attn.k_proj.bias\", \"backbones.1.blocks.9.attn.proj.weight\", \"backbones.1.blocks.9.attn.proj.bias\", \"backbones.1.blocks.9.norm2.weight\", \"backbones.1.blocks.9.norm2.bias\", \"backbones.1.blocks.9.fc1.weight\", \"backbones.1.blocks.9.fc1.bias\", \"backbones.1.blocks.9.fc2.weight\", \"backbones.1.blocks.9.fc2.bias\", \"backbones.1.blocks.9.adaptmlp.down_proj.weight\", \"backbones.1.blocks.9.adaptmlp.down_proj.bias\", \"backbones.1.blocks.9.adaptmlp.up_proj.weight\", \"backbones.1.blocks.9.adaptmlp.up_proj.bias\", \"backbones.1.blocks.10.norm1.weight\", \"backbones.1.blocks.10.norm1.bias\", \"backbones.1.blocks.10.attn.q_proj.weight\", \"backbones.1.blocks.10.attn.q_proj.bias\", \"backbones.1.blocks.10.attn.v_proj.weight\", \"backbones.1.blocks.10.attn.v_proj.bias\", \"backbones.1.blocks.10.attn.k_proj.weight\", \"backbones.1.blocks.10.attn.k_proj.bias\", \"backbones.1.blocks.10.attn.proj.weight\", \"backbones.1.blocks.10.attn.proj.bias\", \"backbones.1.blocks.10.norm2.weight\", \"backbones.1.blocks.10.norm2.bias\", \"backbones.1.blocks.10.fc1.weight\", \"backbones.1.blocks.10.fc1.bias\", \"backbones.1.blocks.10.fc2.weight\", \"backbones.1.blocks.10.fc2.bias\", \"backbones.1.blocks.10.adaptmlp.down_proj.weight\", \"backbones.1.blocks.10.adaptmlp.down_proj.bias\", \"backbones.1.blocks.10.adaptmlp.up_proj.weight\", \"backbones.1.blocks.10.adaptmlp.up_proj.bias\", \"backbones.1.blocks.11.norm1.weight\", \"backbones.1.blocks.11.norm1.bias\", \"backbones.1.blocks.11.attn.q_proj.weight\", \"backbones.1.blocks.11.attn.q_proj.bias\", \"backbones.1.blocks.11.attn.v_proj.weight\", \"backbones.1.blocks.11.attn.v_proj.bias\", \"backbones.1.blocks.11.attn.k_proj.weight\", \"backbones.1.blocks.11.attn.k_proj.bias\", \"backbones.1.blocks.11.attn.proj.weight\", \"backbones.1.blocks.11.attn.proj.bias\", \"backbones.1.blocks.11.norm2.weight\", \"backbones.1.blocks.11.norm2.bias\", \"backbones.1.blocks.11.fc1.weight\", \"backbones.1.blocks.11.fc1.bias\", \"backbones.1.blocks.11.fc2.weight\", \"backbones.1.blocks.11.fc2.bias\", \"backbones.1.blocks.11.adaptmlp.down_proj.weight\", \"backbones.1.blocks.11.adaptmlp.down_proj.bias\", \"backbones.1.blocks.11.adaptmlp.up_proj.weight\", \"backbones.1.blocks.11.adaptmlp.up_proj.bias\", \"backbones.1.norm.weight\", \"backbones.1.norm.bias\", \"fc.weight\", \"fc.sigma\". "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f31b4fee-74e3-47ba-956f-eb9c4a93f90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2844ca58-212c-4fe8-9410-9692ffffc972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(\n",
    "        args[\"dataset\"],\n",
    "        args[\"shuffle\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"init_cls\"],\n",
    "        args[\"increment\"],\n",
    "        args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33b1e98e-9e92-4fc7-b76d-9f6d5b46dbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network.to(args[\"device\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17575af8-0623-4638-a00b-bbcff677515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._cur_task += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "993a71d4-28b4-4b28-a916-5dd71800fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e63fb169-b17f-4afb-be44-d410eefc6da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._cur_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bed53522-8cff-4039-b4e8-6de1afc4b1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c319bf69-55b4-49a5-8e6a-a1f3e1b4c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "#total_classes = args[\"init_cls\"] + args[\"increment\"] * task\n",
    "test_dataset = data_manager.get_dataset(np.arange(0, model._total_classes), source=\"test\", mode=\"test\" )\n",
    "model.test_loader = DataLoader(test_dataset, batch_size=args[\"batch_size\"], shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "train_dataset = data_manager.get_dataset(np.arange(0, model._total_classes),source=\"train\", mode=\"train\")\n",
    "model.train_dataset=train_dataset\n",
    "model.data_manager=data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "621e46b9-5b68-460a-8e52-798eace0b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_for_protonet=data_manager.get_dataset(np.arange(0,model._total_classes),source=\"train\", mode=\"test\")\n",
    "model.train_loader_for_protonet = DataLoader(train_dataset_for_protonet, batch_size=args[\"batch_size\"], shuffle=True, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd337d02-bf12-4666-b8e7-1f738f12b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._known_classes = model._total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7650314b-e953-45a0-b3a6-e0b227480aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n",
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "pretrained_vit_b16_224\n"
     ]
    }
   ],
   "source": [
    "model.construct_dual_branch_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3703d121-24e1-411f-bebe-d7b4d62f79b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBranchCosineIncrementalNet(\n",
       "  (backbone): Identity()\n",
       "  (backbones): ModuleList(\n",
       "    (0): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (1): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (mlp_drop): Dropout(p=0.0, inplace=False)\n",
       "          (adaptmlp): Adapter(\n",
       "            (down_proj): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (non_linear_func): ReLU()\n",
       "            (up_proj): Linear(in_features=64, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (pre_logits): Identity()\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (fc): CosineLinear()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.replace_fc(model.train_loader_for_protonet, model._network, None)\n",
    "# sind die weights die gleichen -> dann fishy \n",
    "#wenn nicht dann sollte es passen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbeae1de-6f8f-479d-9ec5-d51e3475c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "I'm using ViT with adapters.\n",
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n",
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "pretrained_vit_b16_224\n"
     ]
    }
   ],
   "source": [
    "optimizer=optim.AdamW(model._network.parameters(), lr=model.init_lr, weight_decay=model.weight_decay)\n",
    "scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args['tuned_epoch'], eta_min=model.min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75d6c8d-4347-4abb-9fb1-313377957c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._network.update_fc(model._total_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9893a92b-b2f7-4bfa-adcd-c7e567a6d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (_, inputs, targets) in enumerate(loader):\n",
    "        inputs = inputs.to(args[\"device\"][0])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)[\"logits\"]\n",
    "        predicts = torch.max(outputs, dim=1)[1]\n",
    "        correct += (predicts.cpu() == targets).sum()\n",
    "        total += len(targets)\n",
    "\n",
    "    return np.around(tensor2numpy(correct) * 100 / total, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fb4dd80-ec79-41de-9925-0dd3c8066eaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiBranchCosineIncrementalNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 1536]) from checkpoint, the shape in current model is torch.Size([10, 1536])."
     ]
    }
   ],
   "source": [
    "model._network.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "701790c1-d17f-43da-8d97-89706bda8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'top1': [98.2, 95.75, 94.17, 92.32, 90.5, 88.87, 88.59, 86.16, 85.64, 85.16], \n",
    "#'top5': [100.0, 99.35, 99.1, 98.95, 98.94, 98.5, 98.51, 98.21, 97.94, 97.62]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233c2e92-c1ed-48e2-8ca8-16fd881a6414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_compute_accuracy(model._network, model.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1abea2d8-1423-48d6-a49b-59d537efd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._cur_task = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa274928-d775-4cb9-bdee-0529eac6f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'grouped': {'total': 96.6, '00-09': 96.6, 'old': 0, 'new': 96.6},\n",
       "  'top1': 96.6,\n",
       "  'top5': 99.7},\n",
       " None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.test_loader = test_loader\n",
    "model.eval_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d55ed22a-a913-4ec5-ae81-49db4223b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import AutoAttack \n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from foolbox.attacks import LinfPGD\n",
    "import eagerpy as ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3da15eb-7235-457d-8e10-917ffae3fef6",
   "metadata": {},
   "outputs": [
    {
     "ename": "EstimatorError",
     "evalue": "AutoAttack requires an estimator derived from <class 'art.estimators.estimator.BaseEstimator'> and <class 'art.estimators.classification.classifier.ClassifierMixin'>, the provided classifier is an instance of <class 'utils.inc_net.MultiBranchCosineIncrementalNet'> and is derived from (<class 'utils.inc_net.BaseNet'>,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEstimatorError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m  \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(epsilons)):\n\u001b[1;32m     19\u001b[0m      \u001b[38;5;66;03m#attack = FastGradientMethod(estimator=model, eps=epsilons[j])\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m      attack \u001b[38;5;241m=\u001b[39m \u001b[43mAutoAttack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLinf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPDG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m      \u001b[38;5;66;03m#_images, _labels = ep.astensors(images, labels)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m      \u001b[38;5;66;03m#raw_advs, clipped_advs, success = attack(model._network, _images, _labels, epsilons=epsilons)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m      x_test_adv \u001b[38;5;241m=\u001b[39m attack\u001b[38;5;241m.\u001b[39mgenerate(x\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/attacks/evasion/auto_attack.py:96\u001b[0m, in \u001b[0;36mAutoAttack.__init__\u001b[0;34m(self, estimator, norm, eps, eps_step, attacks, batch_size, estimator_orig, targeted, parallel)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     71\u001b[0m     estimator: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLASSIFIER_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     parallel: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    Create a :class:`.AutoAttack` instance.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    :param parallel: If True run attacks in parallel.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attacks:\n\u001b[1;32m     99\u001b[0m         attacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/attacks/attack.py:212\u001b[0m, in \u001b[0;36mEvasionAttack.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_targeted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/attacks/attack.py:125\u001b[0m, in \u001b[0;36mAttack.__init__\u001b[0;34m(self, estimator, summary_writer)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator requirements have not been defined in `_estimator_requirements`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_estimator_valid(estimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator_requirements):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EstimatorError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_requirements, estimator)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator \u001b[38;5;241m=\u001b[39m estimator\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_writer_arg \u001b[38;5;241m=\u001b[39m summary_writer\n",
      "\u001b[0;31mEstimatorError\u001b[0m: AutoAttack requires an estimator derived from <class 'art.estimators.estimator.BaseEstimator'> and <class 'art.estimators.classification.classifier.ClassifierMixin'>, the provided classifier is an instance of <class 'utils.inc_net.MultiBranchCosineIncrementalNet'> and is derived from (<class 'utils.inc_net.BaseNet'>,)."
     ]
    }
   ],
   "source": [
    "epsilons = [0.01]#[0.001, 0.003, 0.005, 0.008, 0.01, 0.1]\n",
    "clean_acc = 0.0\n",
    "robust_acc = [0.0] * len(epsilons)\n",
    "attack_epochs = 5\n",
    "steps = [1, 5, 10, 30, 40, 50]\n",
    "attack = LinfPGD(steps=steps[0])\n",
    "\n",
    "for i, data in enumerate(model.test_loader, 0):\n",
    "\n",
    "    # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "    if i >= attack_epochs:\n",
    "        break\n",
    "\n",
    "    images, labels = data[0].to(args[\"device\"][0]), data[1].to(args[\"device\"][0])\n",
    "\n",
    "   # clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\n",
    "\n",
    "    for j in range(len(epsilons)):\n",
    "        #attack = FastGradientMethod(estimator=model._network, eps=epsilons[j])\n",
    "        attack = AutoAttack(model._network, norm='Linf', eps=epsilons[j], attacks=[\"PDG\"])\n",
    "        #_images, _labels = ep.astensors(images, labels)\n",
    "        #raw_advs, clipped_advs, success = attack(model._network, _images, _labels, epsilons=epsilons)\n",
    "    \n",
    "        x_test_adv = attack.generate(x=images)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(x_test_adv)[\"logits\"]\n",
    "        predicts = torch.max(outputs, dim=1)[1]\n",
    "        \n",
    "        \n",
    "        accuracy = np.sum(np.argmax(predicts.cpu(), axis=1) == np.argmax(labels, axis=1)) / len(labels)\n",
    "        \n",
    "        robust_acc[j] += accuracy / args.attack_epochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d924e1d-8e64-401b-a131-c7ad2f7a91bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiBranchCosineIncrementalNet' object has no attribute 'bounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\u001b[39;00m\n\u001b[1;32m     19\u001b[0m  _images, _labels \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensors(images, labels)\n\u001b[0;32m---> 20\u001b[0m  raw_advs, clipped_advs, success \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m  \u001b[38;5;66;03m#x_test_adv = attack.generate(x=images)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m  \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:257\u001b[0m, in \u001b[0;36mFixedEpsilonAttack.__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    254\u001b[0m x, restore_type \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensor_(inputs)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m--> 257\u001b[0m \u001b[43mverify_input_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m criterion \u001b[38;5;241m=\u001b[39m get_criterion(criterion)\n\u001b[1;32m    260\u001b[0m is_adversarial \u001b[38;5;241m=\u001b[39m get_is_adversarial(criterion, model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:498\u001b[0m, in \u001b[0;36mverify_input_bounds\u001b[0;34m(input, model)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_input_bounds\u001b[39m(\u001b[38;5;28minput\u001b[39m: ep\u001b[38;5;241m.\u001b[39mTensor, model: Model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# verify that input to the attack lies within model's input bounds\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[38;5;241m.\u001b[39mlower\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mupper\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiBranchCosineIncrementalNet' object has no attribute 'bounds'"
     ]
    }
   ],
   "source": [
    "epsilons = [0.01]#[0.001, 0.003, 0.005, 0.008, 0.01, 0.1]\n",
    "clean_acc = 0.0\n",
    "robust_acc = [0.0] * len(epsilons)\n",
    "attack_epochs = 5\n",
    "steps = [1, 5, 10, 30, 40, 50]\n",
    "attack = LinfPGD(steps=steps[0])\n",
    "\n",
    "for i, data in enumerate(model.test_loader, 0):\n",
    "\n",
    "    # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "    if i >= attack_epochs:\n",
    "        break\n",
    "\n",
    "    images, labels = data[0].to(args[\"device\"][0]), data[1].to(args[\"device\"][0])\n",
    "\n",
    "   # clean_acc += (get_acc(model, images, labels)) / args.attack_epochs\n",
    "\n",
    "    \n",
    "    _images, _labels = ep.astensors(images, labels)\n",
    "    raw_advs, clipped_advs, success = attack(model._network, _images, _labels, epsilons=epsilons)\n",
    "\n",
    "    #x_test_adv = attack.generate(x=images)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(raw_advs)[\"logits\"]\n",
    "    predicts = torch.max(outputs, dim=1)[1]\n",
    "    \n",
    "    \n",
    "    accuracy = np.sum(np.argmax(predicts.cpu(), axis=1) == np.argmax(labels, axis=1)) / len(labels)\n",
    "    \n",
    "    robust_acc[j] += accuracy / args.attack_epochs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab5af9-8ca0-4fe8-80de-0af5025cc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "345df6a2-f4fe-458c-93ea-94f73f43ef91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc8493e-33d2-440f-8fea-f54890efffd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:7'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"device\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596b5c27-e9e1-4e43-8c64-4f0ef58e5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.estimators.estimator import BaseEstimator\n",
    "from art.estimators.classification.classifier import ClassifierMixin\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from foolbox.utils import accuracy \n",
    "\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "from foolbox.attacks import LinfPGD, FGSM, L2CarliniWagnerAttack\n",
    "from autoattack import AutoAttack\n",
    "import eagerpy as ep\n",
    "from timm.models import load_checkpoint, create_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a7df3d-487c-4367-a10a-6526b0c37793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#model = Learner(args)\n",
    "#model._network.eval().to(args[\"device\"][0])\n",
    "data_manager = DataManager(\n",
    "    args[\"dataset\"],\n",
    "    args[\"shuffle\"],\n",
    "    args[\"seed\"],\n",
    "    args[\"init_cls\"],\n",
    "    args[\"increment\"],\n",
    "    args,\n",
    ")\n",
    "args[\"nb_classes\"] = data_manager.nb_classes # update args\n",
    "args[\"nb_tasks\"] = data_manager.nb_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68462730-686e-4b19-a03f-5a50af0be013",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "472e5ffc-39ae-4c9f-aae1-e0d0373aed3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39m_cur_task \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39m_total_classes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_known_classes \u001b[38;5;241m+\u001b[39m data_manager\u001b[38;5;241m.\u001b[39mget_task_size(model\u001b[38;5;241m.\u001b[39m_cur_task)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39m_network\u001b[38;5;241m.\u001b[39mupdate_fc(model\u001b[38;5;241m.\u001b[39m_total_classes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model._cur_task += 1\n",
    "model._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)\n",
    "model._network.update_fc(model._total_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b34ed7f-b18b-40fd-853a-3ebab152f80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._cur_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f965ba-80c7-493e-8924-44c23a966d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0f8ea-ce5b-42bb-8d93-f0fde43fdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.after_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c25ea0b-ef85-488c-9401-5f187976e8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): CosineLinear()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = data_manager.get_dataset(np.arange(model._known_classes, model._total_classes),source=\"train\", mode=\"train\", )\n",
    "model.train_dataset = train_dataset\n",
    "model.data_manager = data_manager\n",
    "model.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_dataset = data_manager.get_dataset(np.arange(0, model._total_classes), source=\"test\", mode=\"test\" )\n",
    "model.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "train_dataset_for_protonet = data_manager.get_dataset(np.arange(model._known_classes, model._total_classes),source=\"train\", mode=\"test\", )\n",
    "model.train_loader_for_protonet = DataLoader(train_dataset_for_protonet, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "#model._network.to(model._device)\n",
    "model.replace_fc(model.train_loader_for_protonet, model._network, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f080ee25-bb1d-483d-9f9a-590afd5fdc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, y_true = model._eval_cnn(model.test_loader)\n",
    "cnn_accy = model._evaluate(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce18abed-a171-4e3a-bcd2-72f53b2290a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grouped': {'total': 93.8, '00-09': 93.8, 'old': 0, 'new': 93.8},\n",
       " 'top1': 93.8,\n",
       " 'top5': 99.8}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_accy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "23f44e8f-2f38-4fd2-891d-bd88a6079a46",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign module before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 35\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 3\u001b[0m, in \u001b[0;36mPretrainedClassifier.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_network\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Ensure the model is in evaluation mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_total_classes\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Module):\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign module before Module.__init__() call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1718\u001b[0m     remove_from(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_persistent_buffers_set)\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m _global_module_registration_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "\n",
    "class PretrainedClassifier(torch.nn.Module, BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model):\n",
    "        self._model = model._network\n",
    "        self._model.eval()  # Ensure the model is in evaluation mode\n",
    "        self.num_classes = model._total_classes\n",
    "        self.input_size = (3, 224, 224)\n",
    "        self._model.to(args[\"device\"][0])\n",
    "        self._model.eval()\n",
    "    \n",
    "    def model(self, model):\n",
    "        self._model = model._network\n",
    "        \n",
    "    def input_shape(self):\n",
    "        return self.input_size\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        raise NotImplementedError(\"fit method not supported for pretrained models\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        x_tensor = torch.tensor(x)\n",
    "        inputs = x_tensor.to(model._device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)[\"logits\"]\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        x_tensor = torch.tensor(x)\n",
    "        inputs = x_tensor.to(model._device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)[\"logits\"]\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        return probabilities.detach().numpy()\n",
    "\n",
    "classifier = PretrainedClassifier(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d53921e-26ff-47b4-a335-cde301e8b759",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The input model must inherit from `nn.Module`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Wrap the classifier in a PyTorchClassifier\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This step is necessary to use ART's attack functionalities\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pytorch_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assuming input shape for your model\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnb_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_total_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust according to your model's input range\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/estimators/classification/pytorch.py:145\u001b[0m, in \u001b[0;36mPyTorchClassifier.__init__\u001b[0;34m(self, model, loss, input_shape, nb_classes, optimizer, use_amp, opt_level, loss_scale, channels_first, clip_values, preprocessing_defences, postprocessing_defences, preprocessing, device_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_rnn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m((\u001b[38;5;28misinstance\u001b[39m(m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mRNNBase) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodules()))\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Get the internal layers\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_names: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layers\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Index of layer at which the class gradients should be calculated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/art/estimators/classification/pytorch.py:1202\u001b[0m, in \u001b[0;36mPyTorchClassifier._make_model_wrapper.<locals>.ModelWrapper.get_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m-> 1202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input model must inherit from `nn.Module`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1203\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferred \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m hidden layers on PyTorch classifier.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28mlen\u001b[39m(result),\n\u001b[1;32m   1206\u001b[0m )\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: The input model must inherit from `nn.Module`."
     ]
    }
   ],
   "source": [
    "# Wrap the classifier in a PyTorchClassifier\n",
    "# This step is necessary to use ART's attack functionalities\n",
    "pytorch_classifier = PyTorchClassifier(\n",
    "    model=classifier,\n",
    "    input_shape=(3, 224, 224),  # Assuming input shape for your model\n",
    "    nb_classes=model._total_classes,\n",
    "    clip_values=(0, 1),  # Adjust according to your model's input range\n",
    "    loss = nn.CrossEntropyLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51dddde3-bb1d-49ac-833a-a1c941472443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.inc_net import IncrementalNet,SimpleCosineIncrementalNet,SimpleVitNet\n",
    "import foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2afcf7a6-f51b-4375-bdf8-3a45fc4caa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel(SimpleVitNet):\n",
    "    def __init__(self,args):\n",
    "        super().__init__(args,True)\n",
    "        \n",
    "        #self._cur_task += 1\n",
    "        #self._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)\n",
    "        self.update_fc(10)\n",
    "        self.args = args\n",
    "        self._device = args[\"device\"][0]\n",
    "    def __call__(self, inputs):\n",
    "        return super().__call__(inputs)[\"logits\"]\n",
    "    def replace_fc(self, trainloader, train_dataset):\n",
    "        model = super().eval()\n",
    "        model.to(self._device)\n",
    "        embedding_list = []\n",
    "        label_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(trainloader):\n",
    "                (_,data, label) = batch\n",
    "                data = data.to(self._device)\n",
    "                label = label.to(self._device)\n",
    "                embedding = model.backbone(data)\n",
    "                embedding_list.append(embedding.cpu())\n",
    "                label_list.append(label.cpu())\n",
    "        embedding_list = torch.cat(embedding_list, dim=0)\n",
    "        label_list = torch.cat(label_list, dim=0)\n",
    "\n",
    "        class_list = np.unique(train_dataset.labels)\n",
    "        proto_list = []\n",
    "        for class_index in class_list:\n",
    "            # print('Replacing...',class_index)\n",
    "            data_index = (label_list == class_index).nonzero().squeeze(-1)\n",
    "            embedding = embedding_list[data_index]\n",
    "            proto = embedding.mean(0)\n",
    "            self.fc.weight.data[class_index] = proto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf7817fb-355f-4474-92ba-ae042034acf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n",
      "After BaseNet initialization.\n"
     ]
    }
   ],
   "source": [
    "_total_classes = 10\n",
    "train_dataset = data_manager.get_dataset(np.arange(0, _total_classes),source=\"train\", mode=\"train\", )\n",
    "\n",
    "model = WrapperModel(args)\n",
    "_total_classes = 0 + data_manager.get_task_size(0)\n",
    "\n",
    "train_dataset_for_protonet = data_manager.get_dataset(np.arange(0, _total_classes),source=\"train\", mode=\"test\", )\n",
    "train_loader_for_protonet = DataLoader(train_dataset_for_protonet, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "#model._network.to(model._device)\n",
    "model.replace_fc(train_loader_for_protonet, train_dataset)\n",
    "\n",
    "model.update_fc(_total_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afed50-fd01-46b7-acad-990ee6f1cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19fd3cea-043d-46b0-bb23-3355dfa0f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.device(args[\"device\"][0])\n",
    "model.to(args[\"device\"][0])\n",
    "model.eval()\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fmodel = foolbox.models.PyTorchModel(model, bounds=(0,1), device=args[\"device\"][0], preprocessing=preprocessing)\n",
    "#fmodel.to(args[\"device\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ae0af9e-b55c-4783-bb64-77a23ae270a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:7'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8a7432-f170-439b-9e04-63bf799a21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (_, inputs, targets) in enumerate(loader):\n",
    "        inputs = inputs.to(model._device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        predicts = torch.max(outputs, dim=1)[1]\n",
    "        correct += (predicts.cpu() == targets).sum()\n",
    "        total += len(targets)\n",
    "\n",
    "    return np.around(tensor2numpy(correct) * 100 / total, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79856a2a-ec42-48db-a396-d02416067f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.8\n"
     ]
    }
   ],
   "source": [
    "test_dataset = data_manager.get_dataset(np.arange(0, _total_classes), source=\"test\", mode=\"test\" )\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "model.to(args[\"device\"][0])\n",
    "print(_compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "278f933e-ab70-4f53-a56b-4841de2cd5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9204"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.device(args[\"device\"][0])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ba54f832-c49c-4760-8fd9-8d581e08317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del _images, _labels, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e06781b-4d79-433a-8e8e-159a7d613c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_loader, train_loader, acc, fmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3e2ca4da-06cd-437f-8e67-ef5ffae90ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_model', '_bounds', '_dummy', '_preprocess_args', 'data_format', 'device'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab6128aa-62a8-4846-ab1c-46a70e7bede4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(args[\"device\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7837af7f-9c86-416e-a822-46b882f4aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "    (_,data, label) = batch\n",
    "    \n",
    "    images = data.to(args[\"device\"][0])\n",
    "    labels = label.to(args[\"device\"][0])\n",
    "    acc.append(accuracy(fmodel, images, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f073ff37-9fd0-412b-bdf2-2cc88fabe910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "192868c7-09d8-413a-8974-fe6634ef9736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8953124992549419"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c5ba5c86-9f83-4f35-8f38-da4157c102d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fmodel.transform_bounds((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3fc6d8d7-fd6f-4ccd-b95f-327ce4b3b1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=7)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmodel.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3e35aaa-d8d8-4757-854e-f8530b7b56c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 82.8 %\n",
      "  Linf norm  0.001 : 79.7 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 78.1 %\n",
      "  Linf norm  0.001 : 78.1 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 85.9 %\n",
      "  Linf norm  0.001 : 78.1 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 93.8 %\n",
      "  Linf norm  0.001 : 93.8 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 95.3 %\n",
      "  Linf norm  0.001 : 95.3 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 96.9 %\n",
      "  Linf norm  0.001 : 96.9 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 95.3 %\n",
      "  Linf norm  0.001 : 93.8 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 98.4 %\n",
      "  Linf norm  0.001 : 95.3 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 92.2 %\n",
      "  Linf norm  0.001 : 92.2 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 85.9 %\n",
      "  Linf norm  0.001 : 84.4 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 90.6 %\n",
      "  Linf norm  0.001 : 90.6 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 93.8 %\n",
      "  Linf norm  0.001 : 92.2 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 95.3 %\n",
      "  Linf norm  0.001 : 95.3 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 87.5 %\n",
      "  Linf norm  0.001 : 87.5 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 78.1 %\n",
      "  Linf norm  0.001 : 78.1 %\n",
      "robust accuracy for perturbations with\n",
      "  Linf norm  0.0   : 82.5 %\n",
      "  Linf norm  0.001 : 82.5 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_manager.get_dataset(np.arange(0, 10),source=\"train\", mode=\"train\")\n",
    "train_dataset = train_dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "attack = LinfPGD(steps=1)\n",
    "epsilons = [0.0,\n",
    "        0.001,\n",
    "        #0.003,\n",
    "        #0.005,\n",
    "        #0.008,\n",
    "        #0.01,\n",
    "        #1.0\n",
    "           ]\n",
    "arr = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "\n",
    "    (_,data, label) = batch\n",
    "    images = data.to(args[\"device\"][0])\n",
    "    labels = label.to(args[\"device\"][0])\n",
    "    _images, _labels = ep.astensors(images, labels)\n",
    "    raw_advs, clipped_advs, success = attack(fmodel, _images, _labels, epsilons=epsilons)\n",
    "    robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "    arr.append(robust_accuracy)\n",
    "    print(\"robust accuracy for perturbations with\")\n",
    "    for eps, acc in zip(epsilons, robust_accuracy):\n",
    "        print(f\"  Linf norm  {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
    "\n",
    "    del images, labels, _images, _labels\n",
    "    #plt.plot(epsilons, robust_accuracy.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac22d75e-fe53-408e-8f7f-cdee1faec66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PyTorchTensor(tensor([0.8281, 0.7969], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.7812, 0.7812], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.8594, 0.7812], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9375, 0.9375], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9531, 0.9531], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9688, 0.9688], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9531, 0.9375], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9844, 0.9531], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9219, 0.9219], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.8594, 0.8438], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9062, 0.9062], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9375, 0.9219], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.9531, 0.9531], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.8750, 0.8750], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.7812, 0.7812], device='cuda:7')),\n",
       " PyTorchTensor(tensor([0.8250, 0.8250], device='cuda:7'))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a8c8b49e-112a-4b04-b4d6-9c39b75e4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc4ebd3b-5959-4db4-a73c-ee8a789f04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.0,\n",
    "        0.001,\n",
    "        0.003,\n",
    "        0.005,\n",
    "        0.008,\n",
    "        0.01,\n",
    "        1.0\n",
    "           ]\n",
    "steps = [1, 5, 10, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5921393e-aed7-4c91-939d-e07c43b5a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "robacc_per_step = []\n",
    "attack_epochs = 10\n",
    "for step in steps:\n",
    "    print(f\"Step {step}\")\n",
    "    attack = LinfPGD(steps=step)\n",
    "    \n",
    "    clean_acc = 0.0\n",
    "    robust_acc = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        (_,data, label) = batch\n",
    "        images = data.to(args[\"device\"][0])\n",
    "        labels = label.to(args[\"device\"][0])\n",
    "        \n",
    "        # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "        if i >= attack_epochs:\n",
    "            break\n",
    "    \n",
    "        #images, labels = data[0].to(device), data[1].to(device)\n",
    "        #if step == steps[0]:\n",
    "        #    clean_acc += (get_acc(fmodel, images, labels)) / args.attack_epochs  # accumulate for attack epochs.\n",
    "    \n",
    "        \n",
    "        _images, _labels = ep.astensors(images, labels)\n",
    "        raw_advs, clipped_advs, success = attack(fmodel, _images, _labels, epsilons=epsilons)\n",
    "    \n",
    "        robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "        #print(robust_accuracy)\n",
    "        robust_acc.append(robust_accuracy)# / attack_epochs\n",
    "    \n",
    "        #for eps, acc in zip(epsilons, robust_acc):\n",
    "        #    print(f\"  Step {step}, Linf norm  {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
    "        #print('  -------------------')\n",
    "    racc_step = 0\n",
    "    for i in range(len(robust_acc)):\n",
    "        racc_step += robust_acc[i].numpy()\n",
    "    racc_step = racc_step/len(robust_acc)\n",
    "    robacc_per_step.append(racc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c30d4fa6-5a9b-46ca-98b1-d789a720a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('robustaccperstep.npy', robacc_per_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa82652f-379c-4962-a78e-a853fdc31383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9046875 0.8875    0.875     0.846875  0.8234375 0.8       0.       ]\n",
      " [0.9046875 0.8390625 0.725     0.5828125 0.3984375 0.3234375 0.       ]\n",
      " [0.9046875 0.7953125 0.528125  0.325     0.1578125 0.0953125 0.       ]\n",
      " [0.9046875 0.58125   0.171875  0.040625  0.00625   0.003125  0.       ]\n",
      " [0.9046875 0.4984375 0.1109375 0.0203125 0.0015625 0.        0.       ]\n",
      " [0.9046875 0.4515625 0.0765625 0.0109375 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "arr_loaded = np.load('robustaccperstep.npy')\n",
    "print(arr_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46e0565-c634-4287-8c73-1ccd4c8d5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c114385-dcb0-4f8f-9eac-8ab86ce11f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_loaded, index = steps, columns = epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ed5b08a-8a43-40ec-a3b4-f5a5f5751edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.003</th>\n",
       "      <th>0.005</th>\n",
       "      <th>0.008</th>\n",
       "      <th>0.010</th>\n",
       "      <th>1.000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.904688</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.846875</td>\n",
       "      <td>0.823438</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.904688</td>\n",
       "      <td>0.839063</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.582812</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>0.323438</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.904688</td>\n",
       "      <td>0.795313</td>\n",
       "      <td>0.528125</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.157813</td>\n",
       "      <td>0.095312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.904688</td>\n",
       "      <td>0.581250</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.904688</td>\n",
       "      <td>0.498437</td>\n",
       "      <td>0.110937</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.904688</td>\n",
       "      <td>0.451562</td>\n",
       "      <td>0.076563</td>\n",
       "      <td>0.010937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0.000     0.001     0.003     0.005     0.008     0.010  1.000\n",
       "1   0.904688  0.887500  0.875000  0.846875  0.823438  0.800000    0.0\n",
       "5   0.904688  0.839063  0.725000  0.582812  0.398438  0.323438    0.0\n",
       "10  0.904688  0.795313  0.528125  0.325000  0.157813  0.095312    0.0\n",
       "30  0.904688  0.581250  0.171875  0.040625  0.006250  0.003125    0.0\n",
       "40  0.904688  0.498437  0.110937  0.020312  0.001563  0.000000    0.0\n",
       "50  0.904688  0.451562  0.076563  0.010937  0.000000  0.000000    0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b9f68-52bc-4769-9888-4b8fa243b89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
