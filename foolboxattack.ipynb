{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7380433a-ba48-49fd-b0d1-95c73dcc846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.inc_net import IncrementalNet,SimpleCosineIncrementalNet,SimpleVitNet\n",
    "import foolbox\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from utils.data_manager import DataManager\n",
    "from utils.toolkit import tensor2numpy, accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "from art.attacks.evasion import AutoAttack \n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "import eagerpy as ep\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "from foolbox.attacks import LinfPGD, FGSM, L2CarliniWagnerAttack\n",
    "from autoattack import AutoAttack\n",
    "from timm.models import load_checkpoint, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67703b84-3d05-476c-b445-1114b0ee1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_random(seed=1):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "def _set_device(args):\n",
    "    device_type = args[\"device\"]\n",
    "    gpus = []\n",
    "\n",
    "    for device in device_type:\n",
    "        if device == -1:\n",
    "            device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(device))\n",
    "\n",
    "        gpus.append(device)\n",
    "\n",
    "    args[\"device\"] = gpus\n",
    "def load_json(setting_path):\n",
    "    import json\n",
    "    with open(setting_path) as data_file:\n",
    "        param = json.load(data_file)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e6d6f7-bcf3-4c79-94e6-abc10954cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simplecil import Learner\n",
    "json = \"./exps/simplecil.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a17d6-f3ba-40bf-ba88-633e8f836008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.adam_adapter import Learner\n",
    "json = \"./exps/adam_adapter.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8bf2d3-bfeb-4009-b94d-d6de461964b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = load_json(json)\n",
    "args[\"seed\"] = args[\"seed\"][0]\n",
    "#args[\"device\"] = args[\"device\"][0]\n",
    "_set_random(args[\"seed\"])\n",
    "#_set_device(args)\n",
    "args[\"device\"][0] = \"cuda:4\"\n",
    "torch.device(args[\"device\"][0])\n",
    "\n",
    "args[\"batch_size\"] =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "62eb8ed3-ba5e-4d8b-8f3e-148efc652d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0961563b-5be9-48a2-b0e6-9f8b7754504a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel\u001b[49m, fmodel, images, labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model, fmodel, images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ada93f1-322a-4d2f-9eea-8c9ba220aff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "#torch.device(args[\"device\"][0])\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd17380b-501e-478e-b163-dbbc13a55685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (_, inputs, targets) in enumerate(loader):\n",
    "        inputs = inputs.to(model._device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        predicts = torch.max(outputs, dim=1)[1]\n",
    "        correct += (predicts.cpu() == targets).sum()\n",
    "        total += len(targets)\n",
    "        del inputs\n",
    "    return np.around(tensor2numpy(correct) * 100 / total, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057c5f5-2c16-4658-9610-97ef504a2bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel(SimpleVitNet):\n",
    "    def __init__(self,args):\n",
    "        super().__init__(args,True)\n",
    "        \n",
    "        #self._cur_task += 1\n",
    "        #self._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)\n",
    "        self.update_fc(10)\n",
    "        self.args = args\n",
    "        self._device = args[\"device\"][0]\n",
    "    def __call__(self, inputs):\n",
    "        return super().__call__(inputs)[\"logits\"]\n",
    "    def replace_fc(self, trainloader, train_dataset):\n",
    "        model = super().eval()\n",
    "        model.to(self._device)\n",
    "        embedding_list = []\n",
    "        label_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(trainloader):\n",
    "                (_,data, label) = batch\n",
    "                data = data.to(self._device)\n",
    "                label = label.to(self._device)\n",
    "                embedding = model.backbone(data)\n",
    "                embedding_list.append(embedding.cpu())\n",
    "                label_list.append(label.cpu())\n",
    "        embedding_list = torch.cat(embedding_list, dim=0)\n",
    "        label_list = torch.cat(label_list, dim=0)\n",
    "\n",
    "        class_list = np.unique(train_dataset.labels)\n",
    "        proto_list = []\n",
    "        for class_index in class_list:\n",
    "            # print('Replacing...',class_index)\n",
    "            data_index = (label_list == class_index).nonzero().squeeze(-1)\n",
    "            embedding = embedding_list[data_index]\n",
    "            proto = embedding.mean(0)\n",
    "            self.fc.weight.data[class_index] = proto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8053d850-107c-4c86-a3f8-9cda7046e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel(Module, Learner):\n",
    "    def __init__(self,args):\n",
    "        Module.__init__(self)\n",
    "        #super(Learner, self).__init__(args)\n",
    "        Learner.__init__(self,args)\n",
    "        \n",
    "        #self._cur_task += 1\n",
    "        #self._total_classes = model._known_classes + data_manager.get_task_size(model._cur_task)\n",
    "        #self._network.update_fc(10)\n",
    "        self.args = args\n",
    "        self._device = args[\"device\"][0]\n",
    "    def eval(self):\n",
    "        self._network.eval()\n",
    "    def __call__(self, inputs):\n",
    "        return self._network(inputs)[\"logits\"]\n",
    "    def forward(self, inputs):\n",
    "        #return Learner.__call__(x)[\"logits\"]\n",
    "        return self._network(inputs)[\"logits\"]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "26e00e15-0815-404c-a30e-de05fcfda904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac70ac54-f2e0-4b7c-9c89-00214762715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args[\"batch_size\"]\n",
    "\n",
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfcbb6df-2ffe-4467-8147-5fb833fcad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "This is for the BaseNet initialization.\n",
      "After BaseNet initialization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleVitNet(\n",
       "  (backbone): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc): CosineLinear()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_total_classes = 10\n",
    "data_manager = DataManager(\n",
    "        args[\"dataset\"],\n",
    "        args[\"shuffle\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"init_cls\"],\n",
    "        args[\"increment\"],\n",
    "        args,\n",
    "    )\n",
    "train_dataset = data_manager.get_dataset(np.arange(0, _total_classes),source=\"train\", mode=\"train\", )\n",
    "\n",
    "model = WrapperModel(args)\n",
    "model.train_dataset=train_dataset\n",
    "_total_classes = 0 + data_manager.get_task_size(0)\n",
    "\n",
    "train_dataset_for_protonet = data_manager.get_dataset(np.arange(0, _total_classes),source=\"train\", mode=\"test\", )\n",
    "train_loader_for_protonet = DataLoader(train_dataset_for_protonet, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "model._network.to(model._device)\n",
    "model._network.update_fc(_total_classes)\n",
    "model.replace_fc(train_loader_for_protonet, model._network, args)\n",
    "\n",
    "#model._network.update_fc(_total_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9747d281-4460-4f45-a8b1-8b895deaf610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/han22002/miniconda3/lib/python3.11/site-packages/foolbox/models/pytorch.py:36: UserWarning: The PyTorch model is in training mode and therefore might not be deterministic. Call the eval() method to set it in evaluation mode if this is not intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model.to(args[\"device\"][0])\n",
    "model.eval()\n",
    "model._network.eval()\n",
    "preprocessing = dict(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], axis=-3)\n",
    "fmodel = foolbox.models.PyTorchModel(model, bounds=(0,1), device=args[\"device\"][0], preprocessing=preprocessing)\n",
    "#fmodel.to(args[\"device\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126495b-2a69-4042-94d4-4c0ed4ba4f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74890c3-9a39-4268-9246-ed5e189351cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.8\n"
     ]
    }
   ],
   "source": [
    "test_dataset = data_manager.get_dataset(np.arange(0, _total_classes), source=\"test\", mode=\"test\" )\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "#model.to(args[\"device\"][0])\n",
    "print(_compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016bc829-23c9-4e3a-8451-8bbbe31c5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i, batch in enumerate(test_loader):\n",
    "    (_,data, label) = batch\n",
    "    \n",
    "    images = data.to(args[\"device\"][0])\n",
    "    labels = label.to(args[\"device\"][0])\n",
    "    acc.append(accuracy(fmodel, images, labels))\n",
    "    del images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2310e5c-f6ef-41c1-ad8a-1415350dfb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8953124992549419"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515f565b-e38c-4918-9bed-0005473f24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0.0,\n",
    "        0.001,\n",
    "        0.003,\n",
    "        0.005,\n",
    "        0.008,\n",
    "        0.01,\n",
    "        1.0\n",
    "           ]\n",
    "steps = [1, 5, 10]#,30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b4dce9e-3a19-4609-9acf-dee9e22e4166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#images, labels = data[0].to(device), data[1].to(device)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#if step == steps[0]:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#    clean_acc += (get_acc(fmodel, images, labels)) / args.attack_epochs  # accumulate for attack epochs.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m ep\u001b[38;5;241m.\u001b[39mastensors(images, labels)\n\u001b[0;32m---> 24\u001b[0m raw_advs, clipped_advs, success \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m robust_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m success\u001b[38;5;241m.\u001b[39mfloat32()\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#print(robust_accuracy)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:283\u001b[0m, in \u001b[0;36mFixedEpsilonAttack.__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    281\u001b[0m success \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon \u001b[38;5;129;01min\u001b[39;00m real_epsilons:\n\u001b[0;32m--> 283\u001b[0m     xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# clip to epsilon because we don't really know what the attack returns;\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# alternatively, we could check if the perturbation is at most epsilon,\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# but then we would need to handle numerical violations;\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     xpc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mclip_perturbation(x, xp, epsilon)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/gradient_descent_base.py:127\u001b[0m, in \u001b[0;36mBaseGradientDescent.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    124\u001b[0m criterion_ \u001b[38;5;241m=\u001b[39m get_criterion(criterion)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs, criterion, kwargs\n\u001b[0;32m--> 127\u001b[0m \u001b[43mverify_input_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# perform a gradient ascent (targeted attack) or descent (untargeted attack)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(criterion_, Misclassification):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/foolbox/attacks/base.py:498\u001b[0m, in \u001b[0;36mverify_input_bounds\u001b[0;34m(input, model)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_input_bounds\u001b[39m(\u001b[38;5;28minput\u001b[39m: ep\u001b[38;5;241m.\u001b[39mTensor, model: Model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# verify that input to the attack lies within model's input bounds\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mlower\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;241m.\u001b[39mupper\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/eagerpy/tensor/pytorch.py:73\u001b[0m, in \u001b[0;36mPyTorchTensor.item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mitem\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "robacc_per_step = []\n",
    "attack_epochs = 10\n",
    "for step in steps:\n",
    "    print(f\"Step {step}\")\n",
    "    attack = LinfPGD(steps=step)\n",
    "    \n",
    "    clean_acc = 0.0\n",
    "    robust_acc = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        (_,data, label) = batch\n",
    "        images = data.to(args[\"device\"][0])\n",
    "        labels = label.to(args[\"device\"][0])\n",
    "        \n",
    "        # Samples (attack_batch_size * attack_epochs) images for adversarial attack.\n",
    "        if i >= attack_epochs:\n",
    "            break\n",
    "    \n",
    "        #images, labels = data[0].to(device), data[1].to(device)\n",
    "        #if step == steps[0]:\n",
    "        #    clean_acc += (get_acc(fmodel, images, labels)) / args.attack_epochs  # accumulate for attack epochs.\n",
    "    \n",
    "        \n",
    "        images, labels = ep.astensors(images, labels)\n",
    "        raw_advs, clipped_advs, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
    "    \n",
    "        robust_accuracy = 1 - success.float32().mean(axis=-1)\n",
    "        #print(robust_accuracy)\n",
    "        robust_acc.append(robust_accuracy)# / attack_epochs\n",
    "    \n",
    "        #for eps, acc in zip(epsilons, robust_acc):\n",
    "        #    print(f\"  Step {step}, Linf norm ≤ {eps:<6}: {acc.item() * 100:4.1f} %\")\n",
    "        #print('  -------------------')\n",
    "        del images, labels\n",
    "    racc_step = 0\n",
    "    for i in range(len(robust_acc)):\n",
    "        racc_step += robust_acc[i].numpy()\n",
    "    racc_step = racc_step/len(robust_acc)\n",
    "    robacc_per_step.append(racc_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ef368-1536-4fd1-b85c-76aa2c5616f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
